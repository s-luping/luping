<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Spark学习记录 | Lu Ping's blog</title><meta name="keywords" content="spark"><meta name="author" content="luping"><meta name="copyright" content="luping"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="spark学习记录">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习记录">
<meta property="og:url" content="https://s-luping.github.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="Lu Ping&#39;s blog">
<meta property="og:description" content="spark学习记录">
<meta property="og:locale">
<meta property="og:image" content="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1">
<meta property="article:published_time" content="2021-09-14T03:38:03.000Z">
<meta property="article:modified_time" content="2022-03-29T11:14:06.527Z">
<meta property="article:author" content="luping">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1"><link rel="shortcut icon" href="/luping/img/favicon.png"><link rel="canonical" href="https://s-luping.github.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/luping/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/luping/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark学习记录',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-03-29 03:14:06'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/luping/atom.xml" title="Lu Ping's blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://tse1-mm.cn.bing.net/th/id/R-C.50a11e325200050a0094a0bc5e302cf8?rik=%2bxttpKGOrVrr5w&amp;riu=http%3a%2f%2fwww.gx8899.com%2fuploads%2fallimg%2f2017110610%2fogkgrubpb00.jpg&amp;ehk=GRkc8RLcFB9TWPLsMsgxie8Z%2fuHG8VS1eT%2f2%2bPUR8Nc%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/luping/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a><a href="/luping/tags/"><div class="headline">Tags</div><div class="length-num">10</div></a><a href="/luping/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/luping/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/luping/archives/"><i class="fa-fw fas fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://blog.csdn.net/kun666666?spm=1010.2135.3001.5343"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&amp;riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&amp;ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/luping/">Lu Ping's blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/luping/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/luping/archives/"><i class="fa-fw fas fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://blog.csdn.net/kun666666?spm=1010.2135.3001.5343"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark学习记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-09-14T03:38:03.000Z" title="Created 2021-09-13 19:38:03">2021-09-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-03-29T11:14:06.527Z" title="Updated 2022-03-29 03:14:06">2022-03-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/luping/categories/bigdata/">bigdata</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark学习记录"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="spark介绍"><a href="#spark介绍" class="headerlink" title="spark介绍"></a>spark介绍</h1><p>Spark是加州大学伯克利分校AMP实验室（Algorithms, Machines, and People Lab）开发的通用内存并行计算框架<br>Spark使用Scala语言进行实现，它是一种面向对象、函数式编程语言，能够像操作本地集合对象一样轻松地操作分布式数据集，具有以下特点:<br>1.运行速度快：Spark拥有DAG执行引擎，支持在内存中对数据进行迭代计算。官方提供的数据表明，如果数据由磁盘读取，速度是Hadoop MapReduce的10倍以上，如果数据从内存中读取，速度可以高达100多倍。<br>2.易用性好：Spark不仅支持Scala编写应用程序，而且支持Java和Python等语言进行编写，特别是Scala是一种高效、可拓展的语言，能够用简洁的代码处理较为复杂的处理工作。<br>3.通用性强：Spark生态圈即BDAS（伯克利数据分析栈）包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX等组件，这些组件分别处理Spark Core提供内存计算框架、SparkStreaming的实时处理应用、Spark SQL的即席查询、MLlib或MLbase的机器学习和GraphX的图处理。<br>4.随处运行：Spark具有很强的适应性，能够读取HDFS、Cassandra、HBase、S3和Techyon为持久层读写原生数据，能够以Mesos、YARN和自身携带的Standalone作为资源管理器调度job，来完成Spark应用程序的计算</p>
<h2 id="Spark与Hadoop差异"><a href="#Spark与Hadoop差异" class="headerlink" title="Spark与Hadoop差异"></a>Spark与Hadoop差异</h2><p>Spark是在借鉴了MapReduce之上发展而来的，继承了其分布式并行计算的优点并改进了MapReduce明显的缺陷，具体如下:<br>首先，Spark把中间数据放到内存中，迭代运算效率高。MapReduce中计算结果需要落地，保存到磁盘上，这样势必会影响整体速度，而Spark支持DAG图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，提高了处理效率。<br>其次，Spark容错性高。Spark引进了弹性分布式数据集RDD (Resilient Distributed Dataset) 的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，如果数据集一部分丢失，则可以根据“血统”（即充许基于数据衍生过程）对它们进行重建。另外在RDD计算时可以通过CheckPoint来实现容错，而CheckPoint有两种方式：CheckPoint Data，和Logging The Updates，用户可以控制采用哪种方式来实现容错。<br>最后，Spark更加通用。不像Hadoop只提供了Map和Reduce两种操作，Spark提供的数据集操作类型有很多种，大致分为：Transformations和Actions两大类。Transformations包括Map、Filter、FlatMap、Sample、GroupByKey、ReduceByKey、Union、Join、Cogroup、MapValues、Sort和PartionBy等多种操作类型，同时还提供Count, Actions包括Collect、Reduce、Lookup和Save等操作。另外各个处理节点之间的通信模型不再像Hadoop只有Shuffle一种模式，用户可以命名、物化，控制中间结果的存储、分区等。<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/kxiaozhuk/article/details/82699175">原文链接</a></p>
<h1 id="spark安装"><a href="#spark安装" class="headerlink" title="spark安装"></a>spark安装</h1><h2 id="下载解压"><a href="#下载解压" class="headerlink" title="下载解压"></a>下载解压</h2><p>下载安装包 解压到本地软件安装目录<br><a target="_blank" rel="noopener" href="https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8.tgz">spark-2.4.8.tgz</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers</span><br><span class="line">wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8.tgz</span><br><span class="line">tar xvf spark-2.4.8.tgz .</span><br></pre></td></tr></table></figure>
<h2 id="添加系统环境变量"><a href="#添加系统环境变量" class="headerlink" title="添加系统环境变量"></a>添加系统环境变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/servers/spark-2.4.8</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<h2 id="spark-shell"><a href="#spark-shell" class="headerlink" title="spark-shell"></a>spark-shell</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure>
<p><img src="/luping/img_6.png" alt="img_5.png"></p>
<h1 id="spark任务提交执行"><a href="#spark任务提交执行" class="headerlink" title="spark任务提交执行"></a>spark任务提交执行</h1><h2 id="standalone-spark自主管理的集群模式"><a href="#standalone-spark自主管理的集群模式" class="headerlink" title="standalone spark自主管理的集群模式"></a>standalone spark自主管理的集群模式</h2><p><strong>要配置spark安装目录下的slaves文件</strong>添加本地注意域名映射<br><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_7.png" alt="img_7.png"><br>通过spark-submit提交任务时，在任务提交节点或Client启动driver，<br>在driver创建并初始化sparkContext对象包含DAGScheduler和TaskScheduler，<br>与master通信申请资源，master指派worker为其启动executor<br>生成job阶段，遇到行动算子生成一个job<br>DAGScheduler负责把Sparkjob转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；<br>TaskScheduler与Master节点通讯申请注册Application，Master节点接收到Application的注册请求后，通过资源调度算法，在自己的集群的worker上启动Executor进程；启动的Executor也会反向注册到TaskScheduler上<br>所有task运行完成后，SparkContext向Master注销，释放资源；<br>Stage阶段划分<br>根据宽依赖窄依赖划分阶段，判断宽依赖和窄依赖的依据是是否进行shuffle操作，不需要shuffle的窄依赖分到一个阶段中间的RDD转换操作无需落地，而宽依赖需要shuffle的过程数据需要落地磁盘</p>
<h2 id="spark-on-yarn-提交到hadoop的yarn集群执行"><a href="#spark-on-yarn-提交到hadoop的yarn集群执行" class="headerlink" title="spark on yarn 提交到hadoop的yarn集群执行"></a>spark on yarn 提交到hadoop的yarn集群执行</h2><p><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_8.png" alt="img_8.png"><br>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler<br>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster<br>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源<br>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver<br>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task<br>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p>
<h1 id="spark的模块"><a href="#spark的模块" class="headerlink" title="spark的模块"></a>spark的模块</h1><h2 id="spark-Core"><a href="#spark-Core" class="headerlink" title="spark Core"></a>spark Core</h2><p>###RDD<br>Spark提供的主要抽象是弹性分布式数据集(RDD),它是跨集群节点分区的元素集合,可以并行操作.<br>RDD特点:</p>
<ul>
<li>1.它是在集群节点上的不可变的、已分区的集合对象;</li>
<li>2.通过并行转换的方式来创建(如 Map、 filter、join 等);</li>
<li>3.失败自动重建;</li>
<li>4.可以控制存储级别(内存、磁盘等)来进行重用;</li>
<li>5.必须是可序列化的;</li>
<li>6.是静态类型的(只读)。</li>
</ul>
<h3 id="RDD操作函数"><a href="#RDD操作函数" class="headerlink" title="RDD操作函数"></a>RDD操作函数</h3><p>RDD的操作函数主要分为2种类型行动算子(Transformation)和转换算子(Action).<br>可以对RDD进行函数操作,当你对一个RDD进行了操作,那么结果将会是一个新的RDD<br><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_9.png" alt="img_9.png"><br>Transformation操作不是马上提交Spark集群执行,Spark在遇到 Transformation操作时只会记录需要这样的操作,并不会去执行,需要等到有Action 操作的时候才会真正启动计算过程进行计算.<br>针对每个 Action,Spark 会生成一个Job, 从数据的创建开始,经过 Transformation, 结尾是 Action 操作.<br>这些操作对应形成一个有向无环图(DAG),形成 DAG 的先决条件是最后的函数操作是一个Action.</p>
<h3 id="DAG-stage-划分依据"><a href="#DAG-stage-划分依据" class="headerlink" title="DAG stage 划分依据"></a>DAG stage 划分依据</h3><p><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_10.png" alt="img_10.png"><br>spark dagscheduler将任务划分stage,shuffle是划分DAG中stage 的标识,同时影响Spark执行速度的关键步骤.<br>RDD 的 Transformation 函数中,又分为窄依赖(narrow dependency)和宽依赖(wide dependency)的操作.<br>窄依赖跟宽依赖的区别是是否发生 shuffle(洗牌) 操作.宽依赖会发生 shuffle 操作.<br>窄依赖是子 RDD的各个分片(partition)不依赖于其他分片,能够独立计算得到结果,<br>宽依赖指子 RDD 的各个分片会依赖于父RDD 的多个分片,所以会造成父 RDD 的各个分片在集群中重新分片</p>
<h3 id="shuffle优化"><a href="#shuffle优化" class="headerlink" title="shuffle优化"></a>shuffle优化</h3><p>shuffle涉及网络传输和磁盘io,非常消耗资源 因此需要对shuffle优化<br><strong>一是如果可以避免shuffle则不选择涉及shuffle的算子</strong><br>rdd.groupByKey().mapValues(_ .sum) 与 rdd.reduceByKey(_ + _) 执行的结果是一样的，但是前者需要把全部的数据通过网络传递一遍，而后者只需要根据每个 key 局部的 partition 累积结果，在 shuffle 的之后把局部的累积值相加后得到结果.<br><strong>缓存机制 cache persist</strong><br>Spark中对于一个RDD执行多次算子(函数操作)的默认原理是这样的:每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。<br>对于这种情况,可对多次使用的RDD进行持久化。<br>cache 是使用的默认缓存选项,一般默认为Memoryonly(内存中缓存),<br>persist 则可以在缓存的时候选择任意一种缓存类型.事实上,cache内部调用的是默认的persist.persist可选择的方式很多缓存到磁盘或是内存磁盘组合缓存等</p>
<h1 id="spark常用算子"><a href="#spark常用算子" class="headerlink" title="spark常用算子"></a>spark常用算子</h1><h2 id="转换算子-Transformations"><a href="#转换算子-Transformations" class="headerlink" title="转换算子(Transformations)"></a>转换算子(Transformations)</h2><table>
<thead>
<tr>
<th>Transformations</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>map(func)</td>
<td>通过函数func传递源的每个元素，返回一个新的分布式数据集。</td>
</tr>
<tr>
<td>filter(func)</td>
<td>过滤数据，通过选择func返回true的源元素返回一个新的数据集。</td>
</tr>
<tr>
<td>flatMap(func)</td>
<td>与map类似，但是每个输入项都可以映射到0个或更多的输出项(因此func应该返回一个Seq而不是单个项)。展平 多个集合 汇总成一个集合</td>
</tr>
<tr>
<td>mapPartitions(func)</td>
<td>与map类似，但在RDD的每个分区(块)上分别运行，因此在类型为T的RDD上运行时，func必须是Iterator &#x3D;&gt; Iterator</td>
</tr>
<tr>
<td>mapPartitionsWithIndex(func)</td>
<td>与mapPartitions类似，但也为func提供了一个表示分区索引的整数值，因此func必须是类型(Int, Iterator) &#x3D;&gt; Iterator时，类型为T的RDD。</td>
</tr>
<tr>
<td>sample(withReplacement, fraction, seed)</td>
<td>使用给定的随机数生成器种子，对数据的一小部分进行抽样，无论是否进行替换。</td>
</tr>
<tr>
<td>union(otherDataset)</td>
<td>合并，返回一个新数据集，其中包含源数据集中的元素和参数的并集。</td>
</tr>
<tr>
<td>intersection(otherDataset)</td>
<td>交集，返回一个新的RDD，其中包含源数据集中的元素和参数的交集。</td>
</tr>
<tr>
<td>distinct([numPartitions]))</td>
<td>去重，返回包含源数据集的不同元素的新数据集。</td>
</tr>
<tr>
<td>groupByKey([numPartitions])</td>
<td>当对一个(K, V)对的数据集调用时，返回一个(K，可迭代)对的数据集。注意:如果您要对每个键进行分组以执行聚合(比如求和或平均)，那么使用reduceByKey或aggregateByKey将产生更好的性能。注意:默认情况下，输出中的并行级别取决于父RDD的分区数量。您可以传递一个可选的numPartitions参数来设置不同数量的任务。</td>
</tr>
<tr>
<td>reduceByKey(func, [numPartitions])</td>
<td>在（K，V）对的数据集上调用时，返回一个（K，V）对的数据集，其中每个键的值使用给定的reduce函数func进行聚合，该函数的类型必须是（V，V）&#x3D;&gt;V。与groupByKey一样，reduce任务的数量可以通过可选的第二个参数进行配置。</td>
</tr>
<tr>
<td>aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])</td>
<td>当对一个(K, V)对的数据集调用时，返回一个(K, U)对的数据集，其中每个键的值使用给定的combine函数和一个中立的“零”值进行聚合。允许不同于输入值类型的聚合值类型，同时避免不必要的分配。与groupByKey类似，reduce任务的数量可以通过第二个可选参数进行配置。</td>
</tr>
<tr>
<td>sortByKey([ascending], [numPartitions])</td>
<td>当对一个(K, V)对的数据集(K, V)调用时，K实现有序，返回一个(K, V)对的数据集，按键序升序或降序排序，如布尔升序参数中指定的那样。</td>
</tr>
<tr>
<td>join(otherDataset, [numPartitions])</td>
<td>当对类型(K, V)和(K, W)的数据集调用时，返回一个(K， (V, W))对的数据集，其中包含每个键的所有元素对。通过leftOuterJoin、right touterjoin和fullOuterJoin来支持外部连接。</td>
</tr>
<tr>
<td>cogroup(otherDataset, [numPartitions])</td>
<td>当对类型(K, V)和(K, W)的数据集调用时，返回一个元组(K， (Iterable， Iterable))的数据集。这个操作也称为groupWith。</td>
</tr>
<tr>
<td>cartesian(otherDataset)</td>
<td>当对T和U类型的数据集调用时，返回一个(T, U)对的数据集(所有元素对)。</td>
</tr>
<tr>
<td>pipe(command, [envVars])</td>
<td>通过shell命令(例如Perl或bash脚本)管道传输RDD的每个分区。RDD元素被写入到进程的stdin中，并以字符串的RDD形式返回到它的stdout中的行输出。</td>
</tr>
<tr>
<td>coalesce(numPartitions)</td>
<td>将RDD中的分区数减少到numPartitions。用于筛选大型数据集后更有效地运行操作。</td>
</tr>
<tr>
<td>repartition(numPartitions)</td>
<td>随机重组RDD中的数据，创建更多或更少的分区，并在这些分区之间进行平衡。这总是在网络上对所有数据进行无序处理。</td>
</tr>
<tr>
<td>repartitionAndSortWithinPartitions(partitioner)</td>
<td>根据给定的分区器重新分区RDD，并在每个结果分区中按关键字对记录进行排序。这比在每个分区内调用重新分区然后进行排序更有效，因为它可以将排序向下推到无序处理机制中。</td>
</tr>
</tbody></table>
<h2 id="行动算子-Actions"><a href="#行动算子-Actions" class="headerlink" title="行动算子(Actions)"></a>行动算子(Actions)</h2><p>行动算子从功能上来说作为一个触发器，会触发提交整个作业并开始执行。从代码上来说，它与转换算子的最大不同之处在于：转换算子返回的还是 RDD，行动算子返回的是非 RDD 类型的值，如整数，或者根本没有返回值。</p>
<table>
<thead>
<tr>
<th>Actions</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>reduce(func)</td>
<td>使用函数func（接受两个参数并返回一个）聚合数据集的元素。函数应该是可交换的和相联的，从而可以并行计算</td>
</tr>
<tr>
<td>collect()</td>
<td>在驱动程序中将数据集的所有元素作为数组返回。这通常在过滤器或其他返回足够小的数据子集的操作之后有用。</td>
</tr>
<tr>
<td>count()</td>
<td>返回数据集中元素的数量。</td>
</tr>
<tr>
<td>first()</td>
<td>返回数据集的第一个元素(类似于take(1))。</td>
</tr>
<tr>
<td>take(n)</td>
<td>返回一个包含数据集前n个元素的数组。</td>
</tr>
<tr>
<td>takeSample(withReplacement, num, [seed])</td>
<td>返回数据集num元素的随机样本数组，可选地预先指定随机数生成器种子，是否进行替换。</td>
</tr>
<tr>
<td>takeOrdered(n, [ordering])</td>
<td>使用自然顺序或自定义比较器返回RDD的前n个元素。</td>
</tr>
<tr>
<td>saveAsTextFile(path)</td>
<td>将数据集的元素作为文本文件(或一组文本文件)写入本地文件系统、HDFS或任何其他hadoop支持的文件系统的给定目录中。Spark将对每个元素调用toString，将其转换为文件中的一行文本。</td>
</tr>
<tr>
<td>saveAsSequenceFile(path)(Java and Scala)</td>
<td>在本地文件系统、HDFS或任何其他Hadoop支持的文件系统的给定路径中，将数据集的元素作为Hadoop序列文件编写。这在实现Hadoop可写接口的键值对RDDs上可用。在Scala中，它还可以用于隐式转换为可写的类型(Spark包括基本类型的转换，如Int、Double、String等)。</td>
</tr>
<tr>
<td>saveAsObjectFile(path)(Java and Scala)</td>
<td>使用Java序列化以简单的格式编写数据集的元素，然后可以使用SparkContext.objectFile()加载这些元素。</td>
</tr>
<tr>
<td>countByKey()</td>
<td>只在类型(K, V)的RDDs上可用。返回一个(K, Int)对的hashmap，并记录每个键的计数。</td>
</tr>
<tr>
<td>foreach(func)</td>
<td>对数据集的每个元素运行函数func。这通常是为了避免副作用，如更新累加器或与外部存储系统交互。注意：在foreach（）之外修改除累加器以外的变量可能会导致未定义的行为。有关更多详细信息，请参见理解闭包。</td>
</tr>
</tbody></table>
<h2 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h2><p>缓存是迭代算法和快速交互式使用的关键工具。‎第一次在操作中计算它时，它将保存在节点上的内存中。Spark的缓存是容错的 - 如果RDD的任何分区丢失，它将使用最初创建它的转换自动重新计算。‎persist() cache()</p>
<table>
<thead>
<tr>
<th align="left">Storage Level</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MEMORY_ONLY</td>
<td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</td>
</tr>
<tr>
<td align="left">MEMORY_AND_DISK</td>
<td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed.</td>
</tr>
<tr>
<td align="left">MEMORY_ONLY_SER (Java and Scala)</td>
<td align="left">Store RDD as <em>serialized</em> Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/2.4.5/tuning.html">fast serializer</a>, but more CPU-intensive to read.</td>
</tr>
<tr>
<td align="left">MEMORY_AND_DISK_SER (Java and Scala)</td>
<td align="left">Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of recomputing them on the fly each time they’re needed.</td>
</tr>
<tr>
<td align="left">DISK_ONLY</td>
<td align="left">Store the RDD partitions only on disk.</td>
</tr>
<tr>
<td align="left">MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td>
<td align="left">Same as the levels above, but replicate each partition on two cluster nodes.</td>
</tr>
<tr>
<td align="left">OFF_HEAP (experimental)</td>
<td align="left">Similar to MEMORY_ONLY_SER, but store the data in <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/2.4.5/configuration.html#memory-management">off-heap memory</a>. This requires off-heap memory to be enabled.</td>
</tr>
</tbody></table>
<p>unpersist()</p>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3)</span><br></pre></td></tr></table></figure>

<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val accum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))</span><br><span class="line">10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</span><br><span class="line"></span><br><span class="line">scala&gt; accum.value</span><br><span class="line">res2: Long = 10</span><br></pre></td></tr></table></figure>

<h1 id="spark-SQL"><a href="#spark-SQL" class="headerlink" title="spark SQL"></a>spark SQL</h1><p>SparkSession中所有功能的入口点是SparkSession类.使用:SparkSessionSparkSession.builder()创建SparkSession对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Spark SQL example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>

<p>创建dataframes</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<h2 id="UDF和UDAF"><a href="#UDF和UDAF" class="headerlink" title="UDF和UDAF"></a>UDF和UDAF</h2><p>UDF用户自定义非聚合函数</p>
<p>UDAF用户自定义聚合操作函数</p>
<h2 id="DataSources"><a href="#DataSources" class="headerlink" title="DataSources"></a>DataSources</h2><h3 id="load-x2F-save函数"><a href="#load-x2F-save函数" class="headerlink" title="load &#x2F;save函数"></a>load &#x2F;save函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.load(&quot;examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;namesAndFavColors.parquet&quot;)</span><br></pre></td></tr></table></figure>

<p>format</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val peopleDFCsv = spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)</span><br><span class="line">  .load(&quot;examples/src/main/resources/people.csv&quot;)</span><br><span class="line">  </span><br><span class="line">  usersDF.write.format(&quot;orc&quot;)</span><br><span class="line">  .option(&quot;orc.bloom.filter.columns&quot;, &quot;favorite_color&quot;)</span><br><span class="line">  .option(&quot;orc.dictionary.key.threshold&quot;, &quot;1.0&quot;)</span><br><span class="line">  .save(&quot;users_with_options.orc&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="Save-Modes"><a href="#Save-Modes" class="headerlink" title="Save Modes"></a>Save Modes</h3><table>
<thead>
<tr>
<th align="left">Scala&#x2F;Java</th>
<th align="left">Any Language</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists</code> (default)</td>
<td align="left">erroror errorifexists(default)</td>
<td align="left">When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td align="left">append</td>
<td align="left">When saving a DataFrame to a data source, if data&#x2F;table already exists, contents of the DataFrame are expected to be appended to existing data.</td>
</tr>
<tr>
<td align="left">SaveMode.Overwrite</td>
<td align="left">overwrite</td>
<td align="left">Overwrite mode means that when saving a DataFrame to a data source, if data&#x2F;table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>
</tr>
<tr>
<td align="left">SaveMode.Ignore</td>
<td align="left">ignore</td>
<td align="left">Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected not to save the contents of the DataFrame and not to change the existing data. This is similar to a <code>CREATE TABLE IF NOT EXISTS</code> in SQL.</td>
</tr>
</tbody></table>
<h3 id="JSON-file"><a href="#JSON-file" class="headerlink" title="JSON file"></a>JSON file</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span><br><span class="line">// supported by importing this when creating a Dataset.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// A JSON dataset is pointed to by path.</span><br><span class="line">// The path can be either a single text file or a directory storing text files</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line">// The inferred schema can be visualized using the printSchema() method</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: long (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// Creates a temporary view using the DataFrame</span><br><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL statements can be run by using the sql methods provided by spark</span><br><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">// +------+</span><br><span class="line">// |  name|</span><br><span class="line">// +------+</span><br><span class="line">// |Justin|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><br><span class="line">// a Dataset[String] storing one JSON object per string</span><br><span class="line">val otherPeopleDataset = spark.createDataset(</span><br><span class="line">  &quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |        address|name|</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |[Columbus,Ohio]| Yin|</span><br><span class="line">// +---------------+----+</span><br></pre></td></tr></table></figure>

<h3 id="hive-表"><a href="#hive-表" class="headerlink" title="hive 表"></a>hive 表</h3><p>通过将 中的 （对于安全配置）和（对于 HDFS 配置）文件，可以完成 Hive 的配置。‎<code>hive-site.xml``core-site.xml``hdfs-site.xml``conf/</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &#x27;examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">  case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span><br><span class="line">// `USING hive`</span><br><span class="line">sql(&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;)</span><br><span class="line">// Save DataFrame to the Hive managed table</span><br><span class="line">val df = spark.table(&quot;src&quot;)</span><br><span class="line">df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;)</span><br><span class="line">// After insertion, the Hive managed table has data now</span><br><span class="line">sql(&quot;SELECT * FROM hive_records&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Prepare a Parquet data directory</span><br><span class="line">val dataDir = &quot;/tmp/parquet_data&quot;</span><br><span class="line">spark.range(10).write.parquet(dataDir)</span><br><span class="line">// Create a Hive external Parquet table</span><br><span class="line">sql(s&quot;CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION &#x27;$dataDir&#x27;&quot;)</span><br><span class="line">// The Hive external table should already have data</span><br><span class="line">sql(&quot;SELECT * FROM hive_bigints&quot;).show()</span><br><span class="line">// +---+</span><br><span class="line">// | id|</span><br><span class="line">// +---+</span><br><span class="line">// |  0|</span><br><span class="line">// |  1|</span><br><span class="line">// |  2|</span><br><span class="line">// ... Order may vary, as spark processes the partitions in parallel.</span><br><span class="line"></span><br><span class="line">// Turn on flag for Hive Dynamic Partitioning</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)</span><br><span class="line">// Create a Hive partitioned table using DataFrame API</span><br><span class="line">df.write.partitionBy(&quot;key&quot;).format(&quot;hive&quot;).saveAsTable(&quot;hive_part_tbl&quot;)</span><br><span class="line">// Partitioned column `key` will be moved to the end of the schema.</span><br><span class="line">sql(&quot;SELECT * FROM hive_part_tbl&quot;).show()</span><br><span class="line">// +-------+---+</span><br><span class="line">// |  value|key|</span><br><span class="line">// +-------+---+</span><br><span class="line">// |val_238|238|</span><br><span class="line">// | val_86| 86|</span><br><span class="line">// |val_311|311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<h3 id="JDBC-TO-OTHER-Databases"><a href="#JDBC-TO-OTHER-Databases" class="headerlink" title="JDBC TO OTHER Databases"></a>JDBC TO OTHER Databases</h3><table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>url</code></td>
<td align="left">The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., <code>jdbc:postgresql://localhost/test?user=fred&amp;password=secret</code></td>
</tr>
<tr>
<td align="left"><code>dbtable</code></td>
<td align="left">The JDBC table that should be read from or written into. Note that when using it in the read path anything that is valid in a <code>FROM</code> clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses. It is not allowed to specify <code>dbtable</code> and <code>query</code> options at the same time.</td>
</tr>
<tr>
<td align="left"><code>query</code></td>
<td align="left">A query that will be used to read data into Spark. The specified query will be parenthesized and used as a subquery in the <code>FROM</code> clause. Spark will also assign an alias to the subquery clause. As an example, spark will issue a query of the following form to the JDBC Source.  <code>SELECT &lt;columns&gt; FROM (&lt;user_specified_query&gt;) spark_gen_alias</code>  Below are couple of restrictions while using this option. It is not allowed to specify <code>dbtable</code> and <code>query</code> options at the same time.It is not allowed to specify <code>query</code> and <code>partitionColumn</code> options at the same time. When specifying <code>partitionColumn</code> option is required, the subquery can be specified using <code>dbtable</code> option instead and partition columns can be qualified using the subquery alias provided as part of <code>dbtable</code>. Example: <code>spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, jdbcUrl).option(&quot;query&quot;, &quot;select c1, c2 from t1&quot;).load()</code></td>
</tr>
<tr>
<td align="left"><code>driver</code></td>
<td align="left">The class name of the JDBC driver to use to connect to this URL.</td>
</tr>
<tr>
<td align="left"><code>partitionColumn, lowerBound, upperBound</code></td>
<td align="left">These options must all be specified if any of them is specified. In addition, <code>numPartitions</code> must be specified. They describe how to partition the table when reading in parallel from multiple workers. <code>partitionColumn</code> must be a numeric, date, or timestamp column from the table in question. Notice that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.</td>
</tr>
<tr>
<td align="left"><code>numPartitions</code></td>
<td align="left">The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling <code>coalesce(numPartitions)</code> before writing.</td>
</tr>
<tr>
<td align="left"><code>queryTimeout</code></td>
<td align="left">The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on how JDBC drivers implement the API <code>setQueryTimeout</code>, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch. It defaults to <code>0</code>.</td>
</tr>
<tr>
<td align="left"><code>fetchsize</code></td>
<td align="left">The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading.</td>
</tr>
<tr>
<td align="left"><code>batchsize</code></td>
<td align="left">The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to <code>1000</code>.</td>
</tr>
<tr>
<td align="left"><code>isolationLevel</code></td>
<td align="left">The transaction isolation level, which applies to current connection. It can be one of <code>NONE</code>, <code>READ_COMMITTED</code>, <code>READ_UNCOMMITTED</code>, <code>REPEATABLE_READ</code>, or <code>SERIALIZABLE</code>, corresponding to standard transaction isolation levels defined by JDBC’s Connection object, with default of <code>READ_UNCOMMITTED</code>. This option applies only to writing. Please refer the documentation in <code>java.sql.Connection</code>.</td>
</tr>
<tr>
<td align="left"><code>sessionInitStatement</code></td>
<td align="left">After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL&#x2F;SQL block). Use this to implement session initialization code. Example: <code>option(&quot;sessionInitStatement&quot;, &quot;&quot;&quot;BEGIN execute immediate &#39;alter session set &quot;_serial_direct_read&quot;=true&#39;; END;&quot;&quot;&quot;)</code></td>
</tr>
<tr>
<td align="left"><code>truncate</code></td>
<td align="left">This is a JDBC writer related option. When <code>SaveMode.Overwrite</code> is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to <code>false</code>. This option applies only to writing.</td>
</tr>
<tr>
<td align="left"><code>cascadeTruncate</code></td>
<td align="left">This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a <code>TRUNCATE TABLE t CASCADE</code> (in the case of PostgreSQL a <code>TRUNCATE TABLE ONLY t CASCADE</code> is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care. This option applies only to writing. It defaults to the default cascading truncate behaviour of the JDBC database in question, specified in the <code>isCascadeTruncate</code> in each JDBCDialect.</td>
</tr>
<tr>
<td align="left"><code>createTableOptions</code></td>
<td align="left">This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., <code>CREATE TABLE t (name string) ENGINE=InnoDB.</code>). This option applies only to writing.</td>
</tr>
<tr>
<td align="left"><code>createTableColumnTypes</code></td>
<td align="left">The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: <code>&quot;name CHAR(64), comments VARCHAR(1024)&quot;)</code>. The specified types should be valid spark sql data types. This option applies only to writing.</td>
</tr>
<tr>
<td align="left"><code>customSchema</code></td>
<td align="left">The custom schema to use for reading data from JDBC connectors. For example, <code>&quot;id DECIMAL(38, 0), name STRING&quot;</code>. You can also specify partial fields, and the others use the default type mapping. For example, <code>&quot;id DECIMAL(38, 0)&quot;</code>. The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults. This option applies only to reading.</td>
</tr>
<tr>
<td align="left"><code>pushDownPredicate</code></td>
<td align="left">The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><br><span class="line">// Loading data from a JDBC source</span><br><span class="line">val jdbcDF = spark.read</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">connectionProperties.put(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line">// Specifying the custom data types of the read schema</span><br><span class="line">connectionProperties.put(&quot;customSchema&quot;, &quot;id DECIMAL(38, 0), name STRING&quot;)</span><br><span class="line">val jdbcDF3 = spark.read</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">  .save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br></pre></td></tr></table></figure>

<h2 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h2><h2 id="缓存内存中的数据‎"><a href="#缓存内存中的数据‎" class="headerlink" title="缓存内存中的数据‎"></a>缓存内存中的数据‎</h2><p>‎Spark SQL 可以通过调用 或 使用 内存中列式格式 来缓存表。然后，Spark SQL 将仅扫描所需的列，并将自动调整压缩，以最大程度地减少内存使用量和 GC 压力。您可以调用以从内存中删除该表。‎</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.catalog.cacheTable(&quot;tableName&quot;)</span><br><span class="line">dataFrame.cache()</span><br><span class="line">spark.catalog.uncacheTable(&quot;tableName&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="‎其他配置选项‎"><a href="#‎其他配置选项‎" class="headerlink" title="‎其他配置选项‎"></a>‎其他配置选项‎</h2><p>‎以下选项还可用于优化查询执行的性能。这些选项可能会在将来的版本中弃用，因为会自动执行更多优化。‎</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.sql.files.maxPartitionBytes</code></td>
<td align="left">134217728 (128 MB)</td>
<td align="left">The maximum number of bytes to pack into a single partition when reading files.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.files.openCostInBytes</code></td>
<td align="left">4194304 (4 MB)</td>
<td align="left">The estimated cost to open a file, measured by the number of bytes could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over-estimated, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</td>
</tr>
<tr>
<td align="left"><code>spark.sql.broadcastTimeout</code></td>
<td align="left">300</td>
<td align="left">Timeout in seconds for the broadcast wait time in broadcast joins</td>
</tr>
<tr>
<td align="left"><code>spark.sql.autoBroadcastJoinThreshold</code></td>
<td align="left">10485760 (10 MB)</td>
<td align="left">Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code> has been run.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.shuffle.partitions</code></td>
<td align="left">200</td>
<td align="left">Configures the number of partitions to use when shuffling data for joins or aggregations.</td>
</tr>
</tbody></table>
<h2 id="SQL-查询的使用广播变量"><a href="#SQL-查询的使用广播变量" class="headerlink" title="SQL 查询的使用广播变量"></a>SQL 查询的使用广播变量</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.functions.broadcast</span><br><span class="line">broadcast(spark.table(&quot;src&quot;)).join(spark.table(&quot;records&quot;), &quot;key&quot;).show()</span><br></pre></td></tr></table></figure>

<h2 id="与-Apache-Hive-的兼容性"><a href="#与-Apache-Hive-的兼容性" class="headerlink" title="与 Apache Hive 的兼容性"></a>与 Apache Hive 的兼容性</h2><h3 id="支持的配置单元功能"><a href="#支持的配置单元功能" class="headerlink" title="支持的配置单元功能"></a>支持的配置单元功能</h3><p>Spark SQL 支持绝大多数 Hive 功能，例如：</p>
<ul>
<li>Hive 查询语句，包括：<ul>
<li><code>SELECT</code></li>
<li><code>GROUP BY</code></li>
<li><code>ORDER BY</code></li>
<li><code>CLUSTER BY</code></li>
<li><code>SORT BY</code></li>
</ul>
</li>
<li>所有 Hive 运算符，包括：<ul>
<li>关系运算符 （， ， ， ， ， ， ， ， 等）<code>=``⇔``==``&lt;&gt;``&lt;``&gt;``&gt;=``&lt;=</code></li>
<li>算术运算符（、、、、等）<code>+``-``*``/``%</code></li>
<li>逻辑运算符（、、、、等）<code>AND``&amp;&amp;``OR``||</code></li>
<li>复杂类型构造函数</li>
<li>数学函数（、、、等）<code>sign``ln``cos</code></li>
<li>字符串函数（、、、等）<code>instr``length``printf</code></li>
</ul>
</li>
<li>用户定义函数 （UDF）</li>
<li>用户定义的聚合函数 （UDAF）</li>
<li>用户定义的序列化格式 （SerDes）</li>
<li>窗口函数</li>
<li>加入<ul>
<li><code>JOIN</code></li>
<li><code>&#123;LEFT|RIGHT|FULL&#125; OUTER JOIN</code></li>
<li><code>LEFT SEMI JOIN</code></li>
<li><code>CROSS JOIN</code></li>
</ul>
</li>
<li>工会</li>
<li>子查询<ul>
<li><code>SELECT col FROM ( SELECT a + b AS col from t1) t2</code></li>
</ul>
</li>
<li>采样</li>
<li>解释</li>
<li>分区表，包括动态分区插入</li>
<li>视图</li>
<li>所有 Hive DDL 函数，包括：<ul>
<li><code>CREATE TABLE</code></li>
<li><code>CREATE TABLE AS SELECT</code></li>
<li><code>ALTER TABLE</code></li>
</ul>
</li>
<li>大多数 Hive 数据类型，包括：<ul>
<li><code>TINYINT</code></li>
<li><code>SMALLINT</code></li>
<li><code>INT</code></li>
<li><code>BIGINT</code></li>
<li><code>BOOLEAN</code></li>
<li><code>FLOAT</code></li>
<li><code>DOUBLE</code></li>
<li><code>STRING</code></li>
<li><code>BINARY</code></li>
<li><code>TIMESTAMP</code></li>
<li><code>DATE</code></li>
<li><code>ARRAY&lt;&gt;</code></li>
<li><code>MAP&lt;&gt;</code></li>
<li><code>STRUCT&lt;&gt;</code></li>
</ul>
</li>
</ul>
<h2 id="structure-treaming"><a href="#structure-treaming" class="headerlink" title="structure treaming"></a>structure treaming</h2><p><strong>example</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Create DataFrame representing the stream of input lines from connection to localhost:9999</span><br><span class="line">val lines = spark.readStream</span><br><span class="line">  .format(&quot;socket&quot;)</span><br><span class="line">  .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">  .option(&quot;port&quot;, 9999)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">// Split the lines into words</span><br><span class="line">val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">// Generate running word count</span><br><span class="line">val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">// Start running the query that prints the running counts to the console</span><br><span class="line">val query = wordCounts.writeStream</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>数据源</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Source</td>
<td align="left">Options</td>
<td align="left">Fault-tolerant</td>
<td align="left">Notes</td>
</tr>
<tr>
<td align="left"><strong>File source</strong></td>
<td align="left"><code>path</code>: path to the input directory, and common to all file formats. <code>maxFilesPerTrigger</code>: maximum number of new files to be considered in every trigger (default: no max) <code>latestFirst</code>: whether to process the latest new files first, useful when there is a large backlog of files (default: false) <code>fileNameOnly</code>: whether to check new files based on only the filename instead of on the full path (default: false). With this set to <code>true</code>, the following files would be considered as the same file, because their filenames, “dataset.txt”, are the same: “file:&#x2F;&#x2F;&#x2F;dataset.txt” “s3:&#x2F;&#x2F;a&#x2F;dataset.txt” “s3n:&#x2F;&#x2F;a&#x2F;b&#x2F;dataset.txt” “s3a:&#x2F;&#x2F;a&#x2F;b&#x2F;c&#x2F;dataset.txt” <code>maxFileAge</code>: Maximum age of a file that can be found in this directory, before it is ignored. For the first batch all files will be considered valid. If <code>latestFirst</code> is set to <code>true</code> and <code>maxFilesPerTrigger</code> is set, then this parameter will be ignored, because old files that are valid, and should be processed, may be ignored. The max age is specified with respect to the timestamp of the latest file, and not the timestamp of the current system.(default: 1 week) <code>cleanSource</code>: option to clean up completed files after processing. Available options are “archive”, “delete”, “off”. If the option is not provided, the default value is “off”. When “archive” is provided, additional option <code>sourceArchiveDir</code> must be provided as well. The value of “sourceArchiveDir” must not match with source pattern in depth (the number of directories from the root directory), where the depth is minimum of depth on both paths. This will ensure archived files are never included as new source files. For example, suppose you provide ‘&#x2F;hello?&#x2F;spark&#x2F;<em>‘ as source pattern, ‘&#x2F;hello1&#x2F;spark&#x2F;archive&#x2F;dir’ cannot be used as the value of “sourceArchiveDir”, as ‘&#x2F;hello?&#x2F;spark&#x2F;</em>‘ and ‘&#x2F;hello1&#x2F;spark&#x2F;archive’ will be matched. ‘&#x2F;hello1&#x2F;spark’ cannot be also used as the value of “sourceArchiveDir”, as ‘&#x2F;hello?&#x2F;spark’ and ‘&#x2F;hello1&#x2F;spark’ will be matched. ‘&#x2F;archived&#x2F;here’ would be OK as it doesn’t match. Spark will move source files respecting their own path. For example, if the path of source file is <code>/a/b/dataset.txt</code> and the path of archive directory is <code>/archived/here</code>, file will be moved to <code>/archived/here/a/b/dataset.txt</code>. NOTE: Both archiving (via moving) or deleting completed files will introduce overhead (slow down, even if it’s happening in separate thread) in each micro-batch, so you need to understand the cost for each operation in your file system before enabling this option. On the other hand, enabling this option will reduce the cost to list source files which can be an expensive operation. Number of threads used in completed file cleaner can be configured with<code>spark.sql.streaming.fileSource.cleaner.numThreads</code> (default: 1). NOTE 2: The source path should not be used from multiple sources or queries when enabling this option. Similarly, you must ensure the source path doesn’t match to any files in output directory of file stream sink. NOTE 3: Both delete and move actions are best effort. Failing to delete or move files will not fail the streaming query. Spark may not clean up some source files in some circumstances - e.g. the application doesn’t shut down gracefully, too many files are queued to clean up.  For file-format-specific options, see the related methods in <code>DataStreamReader</code> (<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html">Scala</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/java/org/apache/spark/sql/streaming/DataStreamReader.html">Java</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader">Python</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/R/read.stream.html">R</a>). E.g. for “parquet” format options see <code>DataStreamReader.parquet()</code>.  In addition, there are session configurations that affect certain file-formats. See the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/sql-programming-guide.html">SQL Programming Guide</a> for more details. E.g., for “parquet”, see <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/sql-data-sources-parquet.html#configuration">Parquet configuration</a> section.</td>
<td align="left">Yes</td>
<td align="left">Supports glob paths, but does not support multiple comma-separated paths&#x2F;globs.</td>
</tr>
<tr>
<td align="left"><strong>Socket Source</strong></td>
<td align="left"><code>host</code>: host to connect to, must be specified <code>port</code>: port to connect to, must be specified</td>
<td align="left">No</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>Rate Source</strong></td>
<td align="left"><code>rowsPerSecond</code> (e.g. 100, default: 1): How many rows should be generated per second.  <code>rampUpTime</code> (e.g. 5s, default: 0s): How long to ramp up before the generating speed becomes <code>rowsPerSecond</code>. Using finer granularities than seconds will be truncated to integer seconds.  <code>numPartitions</code> (e.g. 10, default: Spark’s default parallelism): The partition number for the generated rows.  The source will try its best to reach <code>rowsPerSecond</code>, but the query may be resource constrained, and <code>numPartitions</code> can be tweaked to help reach the desired speed.</td>
<td align="left">Yes</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>Kafka Source</strong></td>
<td align="left">See the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a>.</td>
<td align="left">Yes</td>
<td align="left"></td>
</tr>
</tbody></table>
<h3 id="Output-Sinks"><a href="#Output-Sinks" class="headerlink" title="Output Sinks"></a>Output Sinks</h3><p>There are a few types of built-in output sinks.</p>
<ul>
<li><strong>File sink</strong> - Stores the output to a directory.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;parquet&quot;</span>)        <span class="comment">// can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span></span><br><span class="line">    .option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/destination/dir&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Kafka sink</strong> - Stores the output to one or more topics in Kafka.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;host1:port1,host2:port2&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;topic&quot;</span>, <span class="string">&quot;updates&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Foreach sink</strong> - Runs arbitrary computation on the records in the output. See later in the section for more details.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Console sink (for debugging)</strong> - Prints the output to the console&#x2F;stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Memory sink (for debugging)</strong> - The output is stored in memory as an in-memory table. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory. Hence, use it with caution.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;memory&quot;</span>)</span><br><span class="line">    .queryName(<span class="string">&quot;tableName&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<p>Some sinks are not fault-tolerant because they do not guarantee persistence of the output and are meant for debugging purposes only. See the earlier section on <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#fault-tolerance-semantics">fault-tolerance semantics</a>. Here are the details of all the sinks in Spark.</p>
<table>
<thead>
<tr>
<th align="left">Sink</th>
<th align="left">Supported Output Modes</th>
<th align="left">Options</th>
<th align="left">Fault-tolerant</th>
<th align="left">Notes</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>File Sink</strong></td>
<td align="left">Append</td>
<td align="left"><code>path</code>: path to the output directory, must be specified. <code>retention</code>: time to live (TTL) for output files. Output files which batches were committed older than TTL will be eventually excluded in metadata log. This means reader queries which read the sink’s output directory may not process them. You can provide the value as string format of the time. (like “12h”, “7d”, etc.) By default it’s disabled.  For file-format-specific options, see the related methods in DataFrameWriter (<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/sql/DataFrameWriter.html">Scala</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/java/org/apache/spark/sql/DataFrameWriter.html">Java</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter">Python</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/R/write.stream.html">R</a>). E.g. for “parquet” format options see <code>DataFrameWriter.parquet()</code></td>
<td align="left">Yes (exactly-once)</td>
<td align="left">Supports writes to partitioned tables. Partitioning by time may be useful.</td>
</tr>
<tr>
<td align="left"><strong>Kafka Sink</strong></td>
<td align="left">Append, Update, Complete</td>
<td align="left">See the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a></td>
<td align="left">Yes (at-least-once)</td>
<td align="left">More details in the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a></td>
</tr>
<tr>
<td align="left"><strong>Foreach Sink</strong></td>
<td align="left">Append, Update, Complete</td>
<td align="left">None</td>
<td align="left">Yes (at-least-once)</td>
<td align="left">More details in the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch">next section</a></td>
</tr>
<tr>
<td align="left"><strong>ForeachBatch Sink</strong></td>
<td align="left">Append, Update, Complete</td>
<td align="left">None</td>
<td align="left">Depends on the implementation</td>
<td align="left">More details in the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch">next section</a></td>
</tr>
<tr>
<td align="left"><strong>Console Sink</strong></td>
<td align="left">Append, Update, Complete</td>
<td align="left"><code>numRows</code>: Number of rows to print every trigger (default: 20) <code>truncate</code>: Whether to truncate the output if too long (default: true)</td>
<td align="left">No</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>Memory Sink</strong></td>
<td align="left">Append, Complete</td>
<td align="left">None</td>
<td align="left">No. But in Complete Mode, restarted query will recreate the full table.</td>
<td align="left">Table name is the query name.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h3 id="Triggers-触发器"><a href="#Triggers-触发器" class="headerlink" title="Triggers 触发器"></a>Triggers 触发器</h3><table>
<thead>
<tr>
<th><strong>Fixed interval micro-batches</strong></th>
<th>‎查询将以微批处理模式执行，其中微批处理将按用户指定的时间间隔启动。‎‎如果前一个微批处理在间隔内完成，则引擎将等到间隔结束，然后再启动下一个微批次。‎‎如果前一个微批处理花费的时间超过完成间隔的时间（即，如果错过了间隔边界），则下一个微批处理将在前一个微批处理完成后立即启动（即，它不会等待下一个间隔边界）。‎‎如果没有新数据可用，则不会启动任何微批处理。‎</th>
</tr>
</thead>
<tbody><tr>
<td><strong>One-time micro-batch</strong></td>
<td>‎查询将仅执行‎<strong>‎一个‎</strong>‎微批处理来处理所有可用数据，然后自行停止。这在您希望定期启动群集、处理自上一个周期以来可用的所有内容，然后关闭群集的情况下非常有用。在某些情况下，这可能会节省大量成本。‎</td>
</tr>
<tr>
<td><strong>Continuous with fixed checkpoint interval</strong> <em>(experimental)</em></td>
<td>‎查询将在新的低延迟连续处理模式下执行。在下面的‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#continuous-processing">‎”连续处理”部分中‎</a>‎阅读有关此内容的更多信息。‎</td>
</tr>
</tbody></table>
<h1 id="sparkstreaming"><a href="#sparkstreaming" class="headerlink" title="sparkstreaming"></a>sparkstreaming</h1><p><img src="http://spark.incubator.apache.org/docs/3.1.2/img/streaming-arch.png" alt="火花流"></p>
<p>在内部，它的工作方式如下。Spark 流接收实时输入数据流并将数据划分为批次，然后由 Spark 引擎处理这些批处理，以批量生成最终的结果流。‎</p>
<p><img src="http://spark.incubator.apache.org/docs/3.1.2/img/streaming-flow.png" alt="火花流"></p>
<h1 id="A-Quick-Example"><a href="#A-Quick-Example" class="headerlink" title="A Quick Example"></a>A Quick Example</h1><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._ <span class="comment">// not necessary since Spark 1.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span></span><br><span class="line"><span class="comment">// The master requires 2 cores to prevent a starvation scenario.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[2]&quot;</span>).setAppName(<span class="string">&quot;NetworkWordCount&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="comment">// Split each line into words</span></span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._ <span class="comment">// not necessary since Spark 1.3</span></span><br><span class="line"><span class="comment">// Count each word in each batch</span></span><br><span class="line"><span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">wordCounts.print()</span><br><span class="line">ssc.start()             <span class="comment">// Start the computation</span></span><br><span class="line">ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># TERMINAL 1: # Running Netcat $ nc -lk 9999 hello world</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TERMINAL 2: RUNNING NetworkWordCount</span></span><br><span class="line"></span><br><span class="line">$ ./bin/run-example streaming.NetworkWordCount localhost 9999</span><br><span class="line">...</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1357008430000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,1)</span><br><span class="line">(world,1)</span><br></pre></td></tr></table></figure>

<h2 id="基本概念‎"><a href="#基本概念‎" class="headerlink" title="基本概念‎"></a>基本概念‎</h2><p>引入sparkstreaming</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>要从 Spark Streaming 核心 API 中不存在的 Kafka 和 Kinesis 等源引入数据，必须将相应的项目添加到依赖项中。例如，一些常见的如下。‎<code>spark-streaming-xyz_2.12</code></p>
<table>
<thead>
<tr>
<th align="left">Source</th>
<th align="left">Artifact</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Kafka</td>
<td align="left">spark-streaming-kafka-0-10_2.12</td>
</tr>
<tr>
<td align="left">Kinesis</td>
<td align="left">spark-streaming-kinesis-asl_2.12 [Amazon Software License]</td>
</tr>
</tbody></table>
<h2 id="‎初始化spark上下文"><a href="#‎初始化spark上下文" class="headerlink" title="‎初始化spark上下文"></a>‎初始化spark上下文</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>‎‎还可以从现有对象创建对象。‎<code>StreamingContext``SparkContext</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = ...                <span class="comment">// existing SparkContext</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<p>‎定义上下文后，必须执行以下操作。‎</p>
<ol>
<li>‎通过创建输入 DStream 来定义输入源。‎</li>
<li>‎通过将转换和输出操作应用于 DStream 来定义流式计算。‎</li>
<li>‎开始接收数据并使用 进行处理。‎<code>streamingContext.start()</code></li>
<li>‎使用 等待停止处理（手动或由于任何错误）。‎<code>streamingContext.awaitTermination()</code></li>
<li>‎可以使用 手动停止处理。‎<code>streamingContext.stop()</code></li>
</ol>
<h5 id="‎要记住的要点：‎"><a href="#‎要记住的要点：‎" class="headerlink" title="‎要记住的要点：‎"></a>‎要记住的要点：‎</h5><ul>
<li>‎启动上下文后，无法设置或向其添加新的流式计算。‎</li>
<li>‎一旦上下文停止，就无法重新启动。‎</li>
<li>‎一个 JVM 中只能同时激活一个流式流上下文。‎</li>
<li>‎StreamingContext 上的 stop（） 也会停止 SparkContext。若要仅停止流式处理上下文，请将 called 的可选参数设置为 false。‎<code>stop()``stopSparkContext</code></li>
<li>‎SparkContext 可以重新用于创建多个 StreamingContext，只要在创建下一个 StreamingContext 之前停止（不停止 SparkContext）即可。‎</li>
</ul>
<h2 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h2><p><strong>‎DStream‎</strong>‎ 是 Spark 流提供的基本抽象。它表示连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。在内部，DStream由一系列连续的RDD表示，这是Spark对不可变的分布式数据集的抽象（有关详细信息，请参阅‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/rdd-programming-guide.html#resilient-distributed-datasets-rdds">‎Spark编程指南‎</a>‎）。DStream 中的每个 RDD 都包含特定时间间隔的数据</p>
<p><img src="http://spark.incubator.apache.org/docs/3.1.2/img/streaming-dstream-ops.png" alt="Spark Streaming"></p>
<h3 id="文件流"><a href="#文件流" class="headerlink" title="文件流"></a>文件流</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory)</span><br><span class="line">streamingContext.textFileStream(dataDirectory)</span><br></pre></td></tr></table></figure>

<h4 id="‎如何监视目录‎"><a href="#‎如何监视目录‎" class="headerlink" title="‎如何监视目录‎"></a>‎如何监视目录‎</h4><p>‎Spark 流式处理将监视该目录并处理在该目录中创建的任何文件。‎<code>dataDirectory</code></p>
<ul>
<li>‎可以监视一个简单的目录，例如 。直接位于此类路径下的所有文件都将在被发现时进行处理。‎<code>&quot;hdfs://namenode:8040/logs/&quot;</code></li>
<li>‎可以提供 ‎<a target="_blank" rel="noopener" href="http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_13_02">‎POSIX glob 模式‎</a>‎，例如 。在这里，DStream 将包含目录中与模式匹配的所有文件。也就是说：它是目录的模式，而不是目录中的文件的模式。‎<code>&quot;hdfs://namenode:8040/logs/2017/*&quot;</code></li>
<li>‎所有文件必须采用相同的数据格式。‎</li>
<li>‎文件根据其修改时间（而不是其创建时间）被视为时间段的一部分。‎</li>
<li>‎处理后，在当前窗口中对文件所做的更改将不会导致重新读取该文件。也就是说：‎<em>‎更新将被忽略‎</em>‎。‎</li>
<li>‎目录下的文件越多，扫描更改所需的时间就越长，即使没有修改任何文件也是如此。‎</li>
<li>‎如果使用通配符来标识目录（例如 ），则重命名整个目录以匹配路径会将该目录添加到受监视目录列表中。只有目录中修改时间在当前窗口内的文件才会包含在流中。‎<code>&quot;hdfs://namenode:8040/logs/2016-*&quot;</code></li>
<li>‎调用 ‎<a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html#setTimes-org.apache.hadoop.fs.Path-long-long-"><code>‎FileSystem.setTimes（）‎</code></a>‎ 来修复时间戳是一种在以后的窗口中选取文件的方法，即使其内容没有更改。‎</li>
</ul>
<h4 id="‎文件写入时不要监控"><a href="#‎文件写入时不要监控" class="headerlink" title="‎文件写入时不要监控"></a>‎文件写入时不要监控</h4><p>‎”完整”文件系统（如 HDFS）倾向于在创建输出流后立即设置其文件的修改时间。当文件被打开时，甚至在数据完全写入之前，它也可能包含在 - 中，之后将忽略同一窗口中对文件的更新。也就是说：可能会错过更改，并且从流中省略数据。‎</p>
<p>‎要确保在窗口中选取更改，<strong>请将文件写入不受监视的目录</strong>，然后在输出流关闭后立即将其重命名为目标目录。如果重命名的文件在创建过程中出现在扫描的目标目录中，则将选取新数据。‎</p>
<h2 id="‎DStream-上的转换‎"><a href="#‎DStream-上的转换‎" class="headerlink" title="‎DStream 上的转换‎"></a>‎DStream 上的转换‎</h2><table>
<thead>
<tr>
<th align="left">Transformation</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>map</strong>(<em>func</em>)</td>
<td align="left">Return a new DStream by passing each element of the source DStream through a function <em>func</em>.</td>
</tr>
<tr>
<td align="left"><strong>flatMap</strong>(<em>func</em>)</td>
<td align="left">Similar to map, but each input item can be mapped to 0 or more output items.</td>
</tr>
<tr>
<td align="left"><strong>filter</strong>(<em>func</em>)</td>
<td align="left">Return a new DStream by selecting only the records of the source DStream on which <em>func</em> returns true.</td>
</tr>
<tr>
<td align="left"><strong>repartition</strong>(<em>numPartitions</em>)</td>
<td align="left">Changes the level of parallelism in this DStream by creating more or fewer partitions.</td>
</tr>
<tr>
<td align="left"><strong>union</strong>(<em>otherStream</em>)</td>
<td align="left">Return a new DStream that contains the union of the elements in the source DStream and <em>otherDStream</em>.</td>
</tr>
<tr>
<td align="left"><strong>count</strong>()</td>
<td align="left">Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</td>
</tr>
<tr>
<td align="left"><strong>reduce</strong>(<em>func</em>)</td>
<td align="left">Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function <em>func</em> (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</td>
</tr>
<tr>
<td align="left"><strong>countByValue</strong>()</td>
<td align="left">When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</td>
</tr>
<tr>
<td align="left"><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>
<td align="left">When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. <strong>Note:</strong> By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property <code>spark.default.parallelism</code>) to do the grouping. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td>
</tr>
<tr>
<td align="left"><strong>join</strong>(<em>otherStream</em>, [<em>numTasks</em>])</td>
<td align="left">When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</td>
</tr>
<tr>
<td align="left"><strong>cogroup</strong>(<em>otherStream</em>, [<em>numTasks</em>])</td>
<td align="left">When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</td>
</tr>
<tr>
<td align="left"><strong>transform</strong>(<em>func</em>)</td>
<td align="left">Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.</td>
</tr>
<tr>
<td align="left"><strong>updateStateByKey</strong>(<em>func</em>)</td>
<td align="left">Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.</td>
</tr>
</tbody></table>
<h4 id="‎UpdateStateByKey-操作‎"><a href="#‎UpdateStateByKey-操作‎" class="headerlink" title="‎UpdateStateByKey 操作‎"></a>‎UpdateStateByKey 操作‎</h4><p>‎该操作允许您保持任意状态，同时不断使用新信息对其进行更新。要使用它，您必须执行两个步骤。‎<code>updateStateByKey</code></p>
<ol>
<li>‎定义状态 - 状态可以是任意数据类型。‎</li>
<li>‎定义状态更新函数 - 使用函数指定如何使用以前的状态和输入流中的新值来更新状态。‎</li>
</ol>
<p>‎在每个批次中，Spark 都会对所有现有密钥应用状态更新功能，无论它们是否在批处理中具有新数据。如果更新函数返回，则键值对将被淘汰。‎<code>None</code></p>
<h4 id="‎转换操作‎"><a href="#‎转换操作‎" class="headerlink" title="‎转换操作‎"></a>‎转换操作‎</h4><p>‎该操作（及其变体，如）允许在DStream上应用任意RDD到RDD函数。它可用于应用 DStream API 中未公开的任何 RDD 操作。例如，将数据流中的每个批与另一个数据集联接的功能不会直接在 DStream API 中公开。但是，您可以轻松地使用它来执行此操作。这带来了非常强大的可能性。例如，可以通过将输入数据流与预先计算的垃圾邮件信息（也可能是由Spark生成的）连接起来，然后基于它进行过滤来执行实时数据清理。‎</p>
<h4 id="‎窗口操作‎"><a href="#‎窗口操作‎" class="headerlink" title="‎窗口操作‎"></a>‎窗口操作‎</h4><p>‎Spark 流式处理还提供‎<em>‎窗口化计算‎</em>‎，允许您在滑动的数据窗口上应用转换。下图说明了此滑动窗口。‎</p>
<p>‎如图所示，每次窗口在源 DStream 上‎<em>‎滑动‎</em>‎时，落在窗口内的源 RDD 都会被组合并操作，以生成窗口 DStream 的 RDD。在此特定情况下，该操作应用于数据的最近 3 个时间单位，并按 2 个时间单位滑动。这表明任何窗口操作都需要指定两个参数。‎</p>
<ul>
<li><em>‎窗口长度‎</em>‎ - 窗口的持续时间（图中为 3）。‎</li>
<li><em>‎滑动间隔‎</em>‎ - 执行窗口操作的时间间隔（图中为 2）。‎</li>
</ul>
<p>‎这两个参数必须是源 DStream 的批处理间隔的倍数（图中为 1）。‎</p>
<p>‎让我们通过一个示例来说明窗口操作。假设，您希望通过每 10 秒生成一次数据的最后 30 秒的字数统计来扩展‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#a-quick-example">‎前面的示例‎</a>‎。为此，我们必须在数据的最后 30 秒内对 DStream 上应用该操作。这是使用 操作完成的。‎<code>reduceByKey``pairs``(word, 1)``reduceByKeyAndWindow</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reduce last 30 seconds of data, every 10 seconds</span></span><br><span class="line"><span class="keyword">val</span> windowedWordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b), <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th align="left">Transformation</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>window</strong>(<em>windowLength</em>, <em>slideInterval</em>)</td>
<td align="left">Return a new DStream which is computed based on windowed batches of the source DStream.</td>
</tr>
<tr>
<td align="left"><strong>countByWindow</strong>(<em>windowLength</em>, <em>slideInterval</em>)</td>
<td align="left">Return a sliding window count of elements in the stream.</td>
</tr>
<tr>
<td align="left"><strong>reduceByWindow</strong>(<em>func</em>, <em>windowLength</em>, <em>slideInterval</em>)</td>
<td align="left">Return a new single-element stream, created by aggregating elements in the stream over a sliding interval using <em>func</em>. The function should be associative and commutative so that it can be computed correctly in parallel.</td>
</tr>
<tr>
<td align="left"><strong>reduceByKeyAndWindow</strong>(<em>func</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td>
<td align="left">When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function <em>func</em> over batches in a sliding window. <strong>Note:</strong> By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property <code>spark.default.parallelism</code>) to do the grouping. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td>
</tr>
<tr>
<td align="left"><strong>reduceByKeyAndWindow</strong>(<em>func</em>, <em>invFunc</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td>
<td align="left">A more efficient version of the above <code>reduceByKeyAndWindow()</code> where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter <em>invFunc</em>). Like in <code>reduceByKeyAndWindow</code>, the number of reduce tasks is configurable through an optional argument. Note that <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#checkpointing">checkpointing</a> must be enabled for using this operation.</td>
</tr>
<tr>
<td align="left"><strong>countByValueAndWindow</strong>(<em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td>
<td align="left">When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in <code>reduceByKeyAndWindow</code>, the number of reduce tasks is configurable through an optional argument.</td>
</tr>
</tbody></table>
<h4 id="‎联接操作‎"><a href="#‎联接操作‎" class="headerlink" title="‎联接操作‎"></a>‎联接操作‎</h4><p>‎最后，值得强调的是，您可以在Spark Streaming中轻松执行不同类型的联接。‎</p>
<h5 id="‎流-流联接‎"><a href="#‎流-流联接‎" class="headerlink" title="‎流-流联接‎"></a>‎流-流联接‎</h5><p>‎流可以很容易地与其他流连接。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream1: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> stream2: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> joinedStream = stream1.join(stream2)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>‎在每个批处理间隔中，生成的 RDD 将与 生成的 RDD 合并。也可以在流的窗口上进行联接通常非常有用。这也很容易。‎</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> windowedStream1 = stream1.window(<span class="type">Seconds</span>(<span class="number">20</span>))</span><br><span class="line"><span class="keyword">val</span> windowedStream2 = stream2.window(<span class="type">Minutes</span>(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> joinedStream = windowedStream1.join(windowedStream2)</span><br></pre></td></tr></table></figure>

<h2 id="‎DStream-上的输出操作‎"><a href="#‎DStream-上的输出操作‎" class="headerlink" title="‎DStream 上的输出操作‎"></a>‎DStream 上的输出操作‎</h2><p>‎输出操作允许将 DStream 的数据推送到外部系统，如数据库或文件系统。由于输出操作实际上允许外部系统使用转换后的数据，因此它们会触发所有 DStream 转换的实际执行（类似于 RDD 的操作）。目前，定义了以下输出操作：‎</p>
<table>
<thead>
<tr>
<th align="left">Output Operation</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>print</strong>()</td>
<td align="left">Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. <strong>Python API</strong> This is called <strong>pprint()</strong> in the Python API.</td>
</tr>
<tr>
<td align="left"><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td align="left">Save this DStream’s contents as text files. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: <em>“prefix-TIME_IN_MS[.suffix]”</em>.</td>
</tr>
<tr>
<td align="left"><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td align="left">Save this DStream’s contents as <code>SequenceFiles</code> of serialized Java objects. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: <em>“prefix-TIME_IN_MS[.suffix]”</em>. <strong>Python API</strong> This is not available in the Python API.</td>
</tr>
<tr>
<td align="left"><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td>
<td align="left">Save this DStream’s contents as Hadoop files. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: <em>“prefix-TIME_IN_MS[.suffix]”</em>. <strong>Python API</strong> This is not available in the Python API.</td>
</tr>
<tr>
<td align="left"><strong>foreachRDD</strong>(<em>func</em>)</td>
<td align="left">The most generic output operator that applies a function, <em>func</em>, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function <em>func</em> is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs.</td>
</tr>
</tbody></table>
<h2 id="‎缓存-x2F-持久性‎"><a href="#‎缓存-x2F-持久性‎" class="headerlink" title="‎缓存&#x2F;持久性‎"></a>‎缓存&#x2F;持久性‎</h2><p>‎与RDD类似，DStreams还允许开发人员将流的数据保存在内存中。也就是说，在 DStream 上使用该方法将自动在内存中保留该 DStream 的每个 RDD。如果 DStream 中的数据将被多次计算（例如，对同一数据执行多个操作），这将非常有用。对于基于窗口的操作（如 和）和基于状态的操作（如 ），这是隐式正确的。因此，由基于窗口的操作生成的 DStream 会自动保留在内存中，而无需开发人员调用 。‎<code>persist()``reduceByWindow``reduceByKeyAndWindow``updateStateByKey``persist()</code></p>
<p>‎对于通过网络接收数据的输入流（如 Kafka、套接字等），默认持久性级别设置为将数据复制到两个节点以实现容错。‎</p>
<p>‎请注意，与 RDD 不同，DStreams 的默认持久性级别将数据序列化在内存中。‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#memory-tuning">‎这将在性能调整‎</a>‎部分中进一步讨论。有关不同持久性级别的详细信息，请参阅 ‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/rdd-programming-guide.html#rdd-persistence">‎Spark 编程指南‎</a>‎。‎</p>
<hr>
<h2 id="‎检查点‎"><a href="#‎检查点‎" class="headerlink" title="‎检查点‎"></a>‎检查点‎</h2><p>‎流式处理应用程序必须全天候运行，因此必须能够灵活应对与应用程序逻辑无关的故障（例如，系统故障、JVM 崩溃等）。为了实现这一点，Spark 流需要将足够的信息‎<em>‎检查点‎</em>‎到容错存储系统，以便它可以从故障中恢复。有两种类型的数据需要检查点。‎</p>
<ul>
<li><em>‎元数据检查点‎</em>‎ - 将定义流式计算的信息保存到容错存储（如 HDFS）中。这用于从运行流式处理应用程序驱动程序的节点的故障中恢复（稍后将详细讨论）。元数据包括：‎<ul>
<li><em>‎配置‎</em>‎ - 用于创建流式处理应用程序的配置。‎</li>
<li><em>‎DStream 操作‎</em>‎ - 定义流式处理应用程序的 DStream 操作集。‎</li>
<li><em>‎不完整批‎</em>‎处理 - 作业已排队但尚未完成的批处理。‎</li>
</ul>
</li>
<li><em>‎数据检查点‎</em>‎ - 将生成的RDD保存到可靠的存储中。这在某些跨多个批处理组合数据的‎<em>‎有状态‎</em>‎转换中是必需的。在此类转换中，生成的 RDD 依赖于先前批次的 RDD，这会导致依赖关系链的长度随时间不断增加。为了避免恢复时间的这种无限增加（与依赖关系链成正比），有状态转换的中间RDD定期‎<em>‎通过检查点‎</em>‎连接到可靠存储（例如HDFS）以切断依赖关系链。‎</li>
</ul>
<p>‎总而言之，元数据检查点主要用于从驱动程序故障中恢复，而数据或 RDD 检查点对于使用有状态转换时，即使对于基本功能也是必需的。‎</p>
<h4 id="‎何时启用检查点‎"><a href="#‎何时启用检查点‎" class="headerlink" title="‎何时启用检查点‎"></a>‎何时启用检查点‎</h4><p>‎必须为具有以下任一要求的应用程序启用检查点：‎</p>
<ul>
<li><em>‎有状态转换的使用‎</em>‎ - 如果在应用程序中使用或（具有反向函数），则必须提供检查点目录以允许定期 RDD 检查点。‎<code>updateStateByKey``reduceByKeyAndWindow</code></li>
<li><em>‎从运行应用程序的驱动程序的故障中恢复‎</em>‎ - 元数据检查点用于使用进度信息进行恢复。‎</li>
</ul>
<p>‎请注意，无需启用检查点即可运行没有上述有状态转换的简单流式处理应用程序。在这种情况下，从驱动程序故障中恢复也将是部分的（一些已接收但未处理的数据可能会丢失）。这通常是可以接受的，许多以这种方式运行 Spark 流式处理应用程序。对非Hadoop环境的支持预计将来会得到改善。‎</p>
<h4 id="‎如何配置检查点‎"><a href="#‎如何配置检查点‎" class="headerlink" title="‎如何配置检查点‎"></a>‎如何配置检查点‎</h4><p>‎可以通过在容错、可靠的文件系统（例如 HDFS、S3 等）中设置一个目录来启用检查点，检查点信息将保存到该文件系统中。这是通过使用 来完成的。这将允许您使用上述有状态转换。此外，如果要使应用程序从驱动程序故障中恢复，则应重写流式处理应用程序以具有以下行为。‎<code>streamingContext.checkpoint(checkpointDirectory)</code></p>
<ul>
<li>‎当程序首次启动时，它将创建一个新的StreamingContext，设置所有流，然后调用start（）。‎</li>
<li>‎当程序在失败后重新启动时，它将从检查点目录中的检查点数据重新创建流式处理上下文。‎</li>
</ul>
<h2 id="‎部署应用程序‎"><a href="#‎部署应用程序‎" class="headerlink" title="‎部署应用程序‎"></a>‎部署应用程序‎</h2><p>‎本节讨论部署 Spark 流式处理应用程序的步骤。‎</p>
<h3 id="‎要求‎"><a href="#‎要求‎" class="headerlink" title="‎要求‎"></a>‎要求‎</h3><p>‎若要运行 Spark 流式处理应用程序，需要具备以下条件。‎</p>
<ul>
<li><em>‎具有集群管理器的集群‎</em>‎ - 这是任何 Spark 应用程序的一般要求，在‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/cluster-overview.html">‎部署指南‎</a>‎中进行了详细讨论。‎</li>
<li><em>‎打包应用程序 JAR‎</em>‎ - 您必须将流式处理应用程序编译为 JAR。如果使用 ‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/submitting-applications.html"><code>‎spark-submit‎</code></a>‎ 启动应用程序，则无需在 JAR 中提供 Spark 和 Spark 流式处理。但是，如果您的应用程序使用‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#advanced-sources">‎高级源‎</a>‎（例如 Kafka），则必须将它们链接到的额外工件及其依赖项打包到用于部署应用程序的 JAR 中。例如，使用的应用程序必须在应用程序 JAR 中包含其所有可传递依赖项及其所有依赖项。‎<code>KafkaUtils``spark-streaming-kafka-0-10_2.12</code></li>
<li><em>‎为执行程序配置足够的内存‎</em>‎ - 由于接收的数据必须存储在内存中，因此必须为执行程序配置足够的内存来保存接收的数据。请注意，如果要执行 10 分钟的窗口操作，系统必须在内存中保留至少最后 10 分钟的数据。因此，应用程序的内存要求取决于其中使用的操作。‎</li>
<li><em>‎配置检查点‎</em>‎ - 如果流应用程序需要它，则必须将Hadoop API兼容容错存储中的目录（例如HDFS，S3等）配置为检查点目录和流应用程序，其编写方式是检查点信息可用于故障恢复。有关更多详细信息，请参阅‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#checkpointing">‎检查点‎</a>‎部分。‎</li>
<li><em>‎配置应用程序驱动程序的自动重新启动‎</em>‎ - 若要从驱动程序故障中自动恢复，用于运行流式处理应用程序的部署基础结构必须监视驱动程序进程，并在驱动程序失败时重新启动驱动程序。不同的‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/cluster-overview.html#cluster-manager-types">‎集群管理器‎</a>‎有不同的工具来实现这一点。‎<ul>
<li><em>‎Spark 独立‎</em>‎ - 可以提交 Spark 应用程序驱动程序以在 Spark 独立群集中运行（请参阅‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/spark-standalone.html#launching-spark-applications">‎群集部署模式‎</a>‎），即应用程序驱动程序本身在其中一个工作节点上运行。此外，可以指示独立集群管理器‎<em>‎监督‎</em>‎驱动程序，并在驱动程序由于非零退出代码或由于运行驱动程序的节点故障而失败时重新启动驱动程序。有关更多详细信息，请参阅 ‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/spark-standalone.html">‎Spark 独立指南‎</a>‎中的‎<em>‎群集模式‎</em>‎和‎<em>‎监督‎</em>‎。‎</li>
<li><em>‎YARN‎</em>‎ - Yarn 支持用于自动重新启动应用程序的类似机制。有关更多详细信息，请参阅 YARN 文档。‎</li>
<li><em>‎Mesos‎</em>‎ - ‎<a target="_blank" rel="noopener" href="https://github.com/mesosphere/marathon">‎Marathon‎</a>‎ 已被用于通过 Mesos 实现这一目标。‎</li>
</ul>
</li>
<li><em>‎配置预写日志‎</em>‎ - 自 Spark 1.2 起，我们引入了‎<em>‎预写日志‎</em>‎以实现强大的容错保证。如果启用，则从接收方接收的所有数据都将写入配置检查点目录中的预写日志中。这可以防止驱动程序恢复时丢失数据，从而确保零数据丢失（在‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#fault-tolerance-semantics">‎容错语义‎</a>‎部分中进行了详细讨论）。可以通过将‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">‎配置参数‎</a>‎设置为 来启用此功能。但是，这些更强的语义可能会以单个接收方的接收吞吐量为代价。这可以通过‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#level-of-parallelism-in-data-receiving">‎并行运行更多接收器‎</a>‎来纠正，以提高聚合吞吐量。此外，建议在启用预写日志时禁用 Spark 中接收的数据的复制，因为日志已存储在复制的存储系统中。这可以通过将输入流的存储级别设置为 来完成。使用 S3（或任何不支持刷新的文件系统）进行‎<em>‎预写日志‎</em>‎时，请记住启用 和 。有关更多详细信息‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">‎，请参阅 Spark 流式处理配置‎</a>‎。请注意，启用 I&#x2F;O 加密时，Spark 不会对写入预写日志的数据进行加密。如果需要对预写日志数据进行加密，则应将其存储在本机支持加密的文件系统中。‎<code>spark.streaming.receiver.writeAheadLog.enable``true``StorageLevel.MEMORY_AND_DISK_SER``spark.streaming.driver.writeAheadLog.closeFileAfterWrite``spark.streaming.receiver.writeAheadLog.closeFileAfterWrite</code></li>
<li><em>‎设置最大接收速率‎</em>‎ - 如果群集资源不够大，使流式处理应用程序无法像接收数据一样快地处理数据，则可以通过设置记录&#x2F;秒的最大速率限制来限制接收器的速率。请参阅接收器和直接 Kafka 方法的‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">‎配置参数‎</a>‎。在 Spark 1.5 中，我们引入了一个名为‎<em>‎”背压‎</em>‎”的功能，无需设置此速率限制，因为 Spark 流会自动计算出速率限制，并在处理条件发生变化时动态调整它们。可以通过将‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">‎配置参数‎</a>‎设置为 来启用此背压。‎</li>
</ul>
<h3 id="升级应用程序代码"><a href="#升级应用程序代码" class="headerlink" title="升级应用程序代码"></a>升级应用程序代码</h3><p>如果正在运行的 Spark 流式处理应用程序需要使用新的应用程序代码进行升级，则有两种可能的机制。</p>
<ul>
<li>升级后的 Spark 流式处理应用程序将启动并与现有应用程序并行运行。一旦新一个（接收与旧数据相同的数据）已经预热并准备好进入黄金时段，旧一个就可以被关闭。请注意，对于支持将数据发送到两个目标（即早期和升级的应用程序）的数据源，可以执行此操作。</li>
<li>现有应用程序正常关闭（有关正常关闭选项，请参阅 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/streaming/StreamingContext.html"><code>StreamingContext.stop（...）</code></a> 或 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html"><code>JavaStreamingContext.stop（...），</code></a>这可确保在关闭之前完全处理已接收的数据。然后可以启动升级的应用程序，该应用程序将从先前应用程序中断的同一点开始处理。请注意，这只能使用支持源端缓冲的输入源（如 Kafka）来完成，因为在以前的应用程序关闭且升级后的应用程序尚未启动时，需要缓冲数据。并且无法从升级前代码的早期检查点信息重新启动。检查点信息实质上包含序列化的 Scala&#x2F;Java&#x2F;Python 对象，尝试使用新的、修改过的类反序列化对象可能会导致错误。在这种情况下，请使用不同的检查点目录启动升级后的应用，或删除以前的检查点目录。</li>
</ul>
<hr>
<h2 id="监控应用程序"><a href="#监控应用程序" class="headerlink" title="监控应用程序"></a>监控应用程序</h2><p>除了 Spark 的<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/monitoring.html">监控功能</a>之外，还有特定于 Spark Streaming 的其他功能。使用StreamingContext时，<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/monitoring.html#web-interfaces">Spark Web UI</a>会显示一个附加选项卡，其中显示有关正在运行的接收器（接收器是否处于活动状态，收到的记录数，接收器错误等）和已完成批处理（批处理时间，排队延迟等）的统计信息。这可用于监视流式处理应用程序的进度。<code>Streaming</code></p>
<p>Web UI 中的以下两个指标尤其重要：</p>
<ul>
<li><em>处理时间</em> - 处理每批数据的时间。</li>
<li><em>计划延迟</em> - 批处理在队列中等待处理先前批处理完成的时间。</li>
</ul>
<p>如果批处理时间始终大于批处理间隔和&#x2F;或排队延迟不断增加，则表明系统无法像生成批处理批处理时那样快速处理批处理，并且正在落后。在这种情况下，请考虑<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#reducing-the-batch-processing-times">减少</a>批处理时间。</p>
<p>Spark 流式处理程序的进度也可以使用 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/streaming/scheduler/StreamingListener.html">StreamingListener</a> 接口进行监视，该接口允许您获取接收方状态和处理时间。请注意，这是一个开发人员API，将来可能会对其进行改进（即，报告的更多信息）。</p>
<hr>
<hr>
<h1 id="性能调优-1"><a href="#性能调优-1" class="headerlink" title="性能调优"></a>性能调优</h1><p>从群集上的 Spark 流式处理应用程序获得最佳性能需要一些调整。本节介绍一些参数和配置，可以调整这些参数和配置以提高应用程序的性能。在高层次上，您需要考虑两件事：</p>
<ol>
<li>通过有效使用群集资源，缩短每批数据的处理时间。</li>
<li>设置正确的批大小，以便可以像接收数据一样快地处理数据批次（即，数据处理跟上数据引入的步伐）。</li>
</ol>
<h2 id="减少批处理时间"><a href="#减少批处理时间" class="headerlink" title="减少批处理时间"></a>减少批处理时间</h2><p>Spark 中可以进行许多优化，以最大限度地减少每个批次的处理时间。这些已在<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/tuning.html">《调优指南</a>》中详细讨论过。本节重点介绍一些最重要的问题。</p>
<h3 id="数据接收的并行度级别"><a href="#数据接收的并行度级别" class="headerlink" title="数据接收的并行度级别"></a>数据接收的并行度级别</h3><p>通过网络接收数据（如 Kafka、套接字等）需要反序列化数据并将其存储在 Spark 中。如果数据接收成为系统中的瓶颈，则考虑并行化数据接收。请注意，每个输入 DStream 都会创建一个接收单个数据流的接收器（在工作计算机上运行）。因此，可以通过创建多个输入 DStream 并将其配置为从源接收数据流的不同分区来实现接收多个数据流。例如，接收两个数据主题的单个 Kafka 输入 DStream 可以拆分为两个 Kafka 输入流，每个输入流仅接收一个主题。这将运行两个接收器，允许并行接收数据，从而提高整体吞吐量。这些多个 DStream 可以合并在一起以创建单个 DStream。然后，应用于单个输入 DStream 的转换可以应用于统一流。这按如下方式完成。</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#tab_scala_19"><strong>斯卡拉</strong></a></li>
<li><a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#tab_java_19"><strong>爪哇岛</strong></a></li>
<li><a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#tab_python_19"><strong>蟒</strong></a></li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val numStreams = 5</span><br><span class="line">val kafkaStreams = (1 to numStreams).map &#123; i =&gt; KafkaUtils.createStream(...) &#125;</span><br><span class="line">val unifiedStream = streamingContext.union(kafkaStreams)</span><br><span class="line">unifiedStream.print()</span><br></pre></td></tr></table></figure>

<p>另一个应该考虑的参数是接收器的块间隔，它由<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">配置参数</a>确定。对于大多数接收器，接收到的数据在存储在Spark的内存中之前被合并成数据块。每个批处理中的块数决定了将在类似地图的转换中用于处理接收到的数据的任务数。每个接收方每个批次的任务数约为（批次间隔&#x2F;块间隔）。例如，200 毫秒的块间隔将每 2 个批次创建 10 个任务。如果任务数太少（即少于每台计算机的核心数），则效率低下，因为不会使用所有可用核心来处理数据。若要增加给定批处理间隔的任务数，请减少块间隔。但是，建议的最小块间隔值约为 50 毫秒，低于此值，任务启动开销可能是一个问题。<code>spark.streaming.blockInterval</code></p>
<p>使用多个输入流&#x2F;接收器接收数据的替代方法是显式地对输入数据流进行重新分区（使用 ）。这会在进一步处理之前，将接收到的数据批次分布到群集中指定数量的计算机上。<code>inputStream.repartition(&lt;number of partitions&gt;)</code></p>
<p>有关直接流式传输，请参阅 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-kafka-0-10-integration.html">Spark Streaming + Kafka 集成指南</a></p>
<h3 id="数据处理中的并行度级别"><a href="#数据处理中的并行度级别" class="headerlink" title="数据处理中的并行度级别"></a>数据处理中的并行度级别</h3><p>如果在计算的任何阶段使用的并行任务数不够高，则群集资源可能未得到充分利用。例如，对于分布式 reduce 操作（如 和 ），并行任务的默认数目由<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-properties">配置属性控制</a>。您可以将并行度级别作为参数传递（请参阅 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.html"><code>PairDStreamFunctions</code></a> 文档），或设置<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-properties">配置属性</a>以更改默认值。<code>reduceByKey``reduceByKeyAndWindow``spark.default.parallelism``spark.default.parallelism</code></p>
<h3 id="数据序列化"><a href="#数据序列化" class="headerlink" title="数据序列化"></a>数据序列化</h3><p>通过调整序列化格式，可以减少数据序列化的开销。对于流式处理，有两种类型的数据正在序列化。</p>
<ul>
<li><strong>输入数据</strong>：默认情况下，通过接收器接收的输入数据存储在执行器的内存中<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/storage/StorageLevel$.html">，StorageLevel.MEMORY_AND_DISK_SER_2</a>。也就是说，数据被序列化为字节以减少GC开销，并复制以容忍执行程序故障。此外，数据首先保留在内存中，并且仅当内存不足以保存流式计算所需的所有输入数据时才溢出到磁盘。这种序列化显然会产生开销 - 接收方必须反序列化接收的数据，并使用 Spark 的序列化格式对其进行重新序列化。</li>
<li><strong>流式处理操作生成的持久化 RDD：流</strong>式处理计算生成的 RDD 可以持久保存在内存中。例如，窗口操作将数据保留在内存中，因为它们将被多次处理。但是，与 Spark Core 默认的 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/storage/StorageLevel$.html">StorageLevel.MEMORY_ONLY</a> 不同，流式处理计算生成的持久化 RDD 默认使用<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/storage/StorageLevel.html$">StorageLevel.MEMORY_ONLY_SER</a>（即序列化）进行持久化，以最大程度地减少 GC 开销。</li>
</ul>
<p>在这两种情况下，使用 Kryo 序列化都可以减少 CPU 和内存开销。有关更多详细信息<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/tuning.html#data-serialization">，请参阅 Spark 调优指南</a>。对于 Kryo，请考虑注册自定义类并禁用对象引用跟踪（请参阅<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#compression-and-serialization">配置指南</a>中的 Kryo 相关配置）。</p>
<p>在需要为流式处理应用程序保留的数据量不大的特定情况下，将数据（两种类型）保留为反序列化对象可能会很可行，而不会产生过多的 GC 开销。例如，如果使用几秒钟的批处理间隔且没有窗口操作，则可以尝试通过相应地显式设置存储级别来禁用持久化数据中的序列化。这将减少由于序列化而导致的 CPU 开销，从而有可能在不产生太多 GC 开销的情况下提高性能。</p>
<h3 id="任务启动开销"><a href="#任务启动开销" class="headerlink" title="任务启动开销"></a>任务启动开销</h3><p>如果每秒启动的任务数很高（例如，每秒 50 个或更多），则向执行程序发送任务的开销可能很大，并且很难实现亚秒级延迟。可以通过以下更改来减少开销：</p>
<ul>
<li><strong>执行模式</strong>：在独立模式或粗粒度 Mesos 模式下运行 Spark 可比细粒度 Mesos 模式获得更好的任务启动时间。有关更多详细信息，请参阅在 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/running-on-mesos.html">Mesos 上运行指南</a>。</li>
</ul>
<p>这些更改可以将批处理时间减少 100 毫秒，从而使亚秒级批大小可行。</p>
<hr>
<h2 id="设置正确的批处理间隔"><a href="#设置正确的批处理间隔" class="headerlink" title="设置正确的批处理间隔"></a>设置正确的批处理间隔</h2><p>为了使在群集上运行的 Spark 流式处理应用程序保持稳定，系统应该能够像接收数据一样快地处理数据。换句话说，批量数据的处理速度应与生成数据的速度一样快。通过<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#monitoring-applications">监视</a>流式处理 Web UI 中的处理时间（其中批处理时间应小于批处理间隔），可以找到应用程序是否属于这种情况。</p>
<p>根据流式计算的性质，使用的批处理间隔可能会对应用程序在一组固定的群集资源上可以维持的数据速率产生重大影响。例如，让我们考虑前面的 WordCountNetwork 示例。对于特定的数据速率，系统可能能够每2秒（即，2秒的批处理间隔）跟上报告字数，但不是每500毫秒。因此，需要设置批次间隔，以便可以维持生产中的预期数据速率。</p>
<p>为应用程序确定正确的批大小是使用保守的批处理间隔（例如，5-10 秒）和低数据速率对其进行测试。若要验证系统是否能够跟上数据速率，可以检查每个已处理批次所经历的端到端延迟值（在 Spark 驱动程序 log4j 日志中查找”总延迟”，或使用<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/streaming/scheduler/StreamingListener.html">流式处理听信器</a>接口）。如果将延迟保持为与批大小相当，则系统是稳定的。否则，如果延迟不断增加，则意味着系统无法跟上，因此不稳定。一旦您了解了稳定的配置，就可以尝试提高数据速率和&#x2F;或减小批大小。请注意，由于临时数据速率增加而导致的延迟暂时增加可能很好，只要延迟减小到较低值（即小于批大小）。</p>
<hr>
<h2 id="内存调整"><a href="#内存调整" class="headerlink" title="内存调整"></a>内存调整</h2><p>优化<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/tuning.html#memory-tuning">指南</a>中已经详细讨论了调整 Spark 应用程序的内存使用情况和 GC 行为。强烈建议您阅读该书。在本节中，我们将专门讨论 Spark 流式处理应用程序上下文中的一些优化参数。</p>
<p>Spark 流式处理应用程序所需的群集内存量在很大程度上取决于所使用的转换类型。例如，如果要对过去 10 分钟的数据使用窗口操作，则集群应具有足够的内存，以便在内存中保存 10 分钟的数据。或者，如果您想与大量按键一起使用，那么必要的内存将很高。相反，如果要执行简单的映射-过滤器-存储操作，则必要的内存将不足。<code>updateStateByKey</code></p>
<p>通常，由于通过接收器接收的数据与StorageLevel.MEMORY_AND_DISK_SER_2一起存储，因此不适合内存的数据将溢出到磁盘。这可能会降低流式处理应用程序的性能，因此建议根据流式处理应用程序的要求提供足够的内存。最好尝试以小规模查看内存使用情况并进行相应的估计。</p>
<p>内存调整的另一个方面是垃圾回收。对于需要低延迟的流式处理应用程序，不希望 JVM 垃圾回收导致大量暂停。</p>
<p>有几个参数可以帮助您调整内存使用情况和 GC 开销：</p>
<ul>
<li><strong>DStream 的持久性级别</strong>：如前所述<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#data-serialization">，在数据序列化</a>部分中，输入数据和 RDD 默认保留为序列化字节。与反序列化持久性相比，这减少了内存使用量和 GC 开销。启用 Kryo 序列化可进一步减少序列化大小和内存使用量。通过压缩可以进一步减少内存使用量（参见 Spark 配置），但代价是 CPU 时间。<code>spark.rdd.compress</code></li>
<li><strong>清除旧数据</strong>：默认情况下，将自动清除 DStream 转换生成的所有输入数据和持久化 RDD。Spark 流式处理根据使用的转换来决定何时清除数据。例如，如果您使用的是 10 分钟的窗口操作，则 Spark 流式处理将保留最后 10 分钟左右的数据，并主动丢弃较旧的数据。通过设置 ， 数据可以保留更长的时间（例如，以交互方式查询较旧的数据）。<code>streamingContext.remember</code></li>
<li><strong>CMS 垃圾回收器</strong>：强烈建议使用并发标记和扫描 GC，以保持与 GC 相关的暂停始终保持在较低水平。尽管已知并发 GC 会降低系统的整体处理吞吐量，但仍建议使用 GC 来实现更一致的批处理时间。确保在驱动程序（使用 in）和执行程序（使用 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#runtime-environment">Spark 配置</a>）上设置 CMS GC。<code>--driver-java-options``spark-submit``spark.executor.extraJavaOptions</code></li>
<li><strong>其他提示</strong>：要进一步减少 GC 开销，下面是一些其他提示供您尝试。<ul>
<li>使用存储级别保留 RDD。有关更多详细信息，请参阅 <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/rdd-programming-guide.html#rdd-persistence">Spark 编程指南</a>。<code>OFF_HEAP</code></li>
<li>使用更多堆大小较小的执行程序。这将减少每个 JVM 堆中的 GC 压力。</li>
</ul>
</li>
</ul>
<hr>
<h5 id="要记住的要点："><a href="#要记住的要点：" class="headerlink" title="要记住的要点："></a>要记住的要点：</h5><ul>
<li>DStream 与单个接收器相关联。为了实现读取并行性，需要创建多个接收器，即多个DStream。接收器在执行器内运行。它占据一个核心。确保在预订接收器插槽后有足够的内核进行处理，即 应考虑接收器插槽。接收方以轮循机制方式分配给执行程序。<code>spark.cores.max</code></li>
<li>从流源接收数据时，接收方会创建数据块。每隔一毫秒就会生成一个新的数据块。在 batchInterval 期间创建 N 个数据块，其中 N &#x3D; batchInterval&#x2F;blockInterval。这些块由当前执行器的 BlockManager 分发给其他执行器的块管理器。之后，在驱动程序上运行的网络输入跟踪器将被告知块位置以进行进一步处理。</li>
<li>在驱动程序上为批处理期间创建的块创建 RDD。在批处理期间生成的块是 RDD 的分区。每个分区都是 spark 中的一个任务。blockInterval&#x3D;&#x3D; batchinterval 意味着创建了一个分区，并且可能在本地进行处理。</li>
<li>块上的映射任务在执行器（一个接收块，另一个复制块的位置）中处理，该执行器具有块而不考虑块间隔，除非非本地调度启动。拥有更大的区块意味着更大的区块。较高的 值 会增加处理本地节点上的块的几率。需要在这两个参数之间找到平衡，以确保较大的块在本地处理。<code>spark.locality.wait</code></li>
<li>无需依赖 batchInterval 和 blockInterval，您可以通过调用 来定义分区数。这会随机重新排列 RDD 中的数据，以创建 n 个分区。是的，为了提高并行性。虽然是以洗牌为代价的。RDD 的处理由驾驶员的作业分拣员安排为作业。在给定的时间点，只有一个作业处于活动状态。因此，如果一个作业正在执行，则其他作业将排队。<code>inputDstream.repartition(n)</code></li>
<li>如果您有两个 dstream，则将形成两个 RDD，并且将创建两个作业，这些作业将一个接一个地安排。为避免这种情况，可以合并两个 dstream。这将确保为 d 流的两个 RDD 形成一个联合 RDD。然后，该工会RDD被视为单一工作。但是，RDD 的分区不受影响。</li>
<li>如果批处理时间超过批处理间隔，那么显然接收方的内存将开始填满，并最终引发异常（最有可能是BlockNotFoundException）。目前，无法暂停接收机。使用SparkConf配置，接收器的速率可以受到限制。<code>spark.streaming.receiver.maxRate</code></li>
</ul>
<hr>
<hr>
<h1 id="容错语义"><a href="#容错语义" class="headerlink" title="容错语义"></a>容错语义</h1><p>在本节中，我们将讨论 Spark 流式处理应用程序在发生故障时的行为。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了理解 Spark Streaming 提供的语义，让我们记住 Spark 的 RDD 的基本容错语义。</p>
<ol>
<li>RDD 是不可变的、确定性可重计算的分布式数据集。每个 RDD 都会记住用于创建容错输入数据集的确定性操作的沿袭。</li>
<li>如果RDD的任何分区由于工作线程节点故障而丢失，则可以使用操作沿袭从原始容错数据集重新计算该分区。</li>
<li>假设所有 RDD 转换都是确定性的，则无论 Spark 群集中发生故障，最终转换的 RDD 中的数据都将始终相同。</li>
</ol>
<p>Spark 对 HDFS 或 S3 等容错文件系统中的数据进行操作。因此，从容错数据生成的所有RDD也是容错的。但是，Spark 流的情况并非如此，因为在大多数情况下，数据是通过网络接收的（除非使用时）。为了对所有生成的 RDD 实现相同的容错属性，将在群集中工作线程节点中的多个 Spark 执行程序之间复制接收到的数据（默认复制因子为 2）。这导致系统中的两种数据在发生故障时需要恢复：<code>fileStream</code></p>
<ol>
<li><em>接收和复制的数据</em> - 此数据在单个工作线程节点发生故障后仍会继续存在，因为该数据的副本存在于其他节点之一上。</li>
<li><em>已接收但缓冲以进行复制的数据</em> - 由于不会复制此数据，因此恢复此数据的唯一方法是从源中再次获取它。</li>
</ol>
<p>此外，我们应该关注两种故障：</p>
<ol>
<li><em>工作节点故障</em> - 任何运行执行程序的工作线程节点都可能失败，并且这些节点上的所有内存中数据都将丢失。如果任何接收器在故障节点上运行，则其缓冲数据将丢失。</li>
<li><em>驱动程序节点故障</em> - 如果运行 Spark 流式处理应用程序的驱动程序节点失败，则显然 SparkContext 将丢失，并且所有执行程序及其内存中数据都将丢失。</li>
</ol>
<p>有了这些基本知识，让我们了解一下 Spark 流的容错语义。</p>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>流系统的语义通常根据系统可以处理每条记录的次数来捕获。系统可以在所有可能的操作条件下提供三种类型的保证（尽管出现故障等）。</p>
<ol>
<li><em>最多一次</em>：每条记录将处理一次或根本不处理。</li>
<li><em>至少一次</em>：每条记录将被处理一次或多次。这比<em>最多一次</em>更强大，因为它确保不会丢失任何数据。但可能会有重复。</li>
<li><em>正好一次</em>：每条记录将只处理一次 - 不会丢失任何数据，也不会多次处理任何数据。这显然是三者中最有力的保证。</li>
</ol>
<h2 id="基本语义"><a href="#基本语义" class="headerlink" title="基本语义"></a>基本语义</h2><p>从广义上讲，在任何流处理系统中，处理数据都有三个步骤。</p>
<ol>
<li><em>接收数据</em>：使用接收器或其他方式从源接收数据。</li>
<li><em>转换数据</em>：使用 DStream 和 RDD 转换转换接收到的数据。</li>
<li><em>推出数据</em>：最终转换后的数据被推送到外部系统，如文件系统、数据库、仪表板等。</li>
</ol>
<p>如果流应用程序必须实现端到端的精确一次保证，那么每个步骤都必须提供一次完全相同的保证。也就是说，每个记录必须只接收一次，转换一次，然后推送到下游系统一次。让我们在 Spark 流式处理的上下文中了解这些步骤的语义。</p>
<ol>
<li><em>接收数据</em>：不同的输入源提供不同的保证。这将在下一小节中详细讨论。</li>
<li><em>转换数据</em>：由于RDD提供的保证，所有已收到的数据将只处理一<em>次</em>。即使出现故障，只要接收到的输入数据可访问，最终转换的RDD将始终具有相同的内容。</li>
<li><em>推出数据</em>：默认情况下，输出操作确保<em>至少一次</em>语义，因为它取决于输出操作的类型（幂等或不幂等）和下游系统的语义（是否支持事务）。但是用户可以实现自己的事务机制来实现<em>恰好一次</em>的语义。本节稍后将对此进行更详细的讨论。</li>
</ol>
<h2 id="接收数据的语义"><a href="#接收数据的语义" class="headerlink" title="接收数据的语义"></a>接收数据的语义</h2><p>不同的输入源提供不同的保证，从<em>至少一次</em>到<em>恰好一次</em>不等。阅读了解更多详情。</p>
<h3 id="使用文件"><a href="#使用文件" class="headerlink" title="使用文件"></a>使用文件</h3><p>如果所有输入数据都已存在于容错文件系统（如HDFS）中，则Spark Streaming始终可以从任何故障中恢复并处理所有数据。这给出了<em>恰好一次</em>的语义，这意味着无论什么失败，所有数据都将被精确地处理一次。</p>
<h3 id="使用基于接收器的源"><a href="#使用基于接收器的源" class="headerlink" title="使用基于接收器的源"></a>使用基于接收器的源</h3><p>对于基于接收器的输入源，容错语义取决于故障场景和接收器类型。如前所述<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#receiver-reliability">，有</a>两种类型的接收器：</p>
<ol>
<li><em>可靠接收器</em> - 这些接收器仅在确保已复制接收的数据后才确认可靠的来源。如果此类接收器发生故障，则源将不会收到缓冲（未复制）数据的确认。因此，如果接收方重新启动，源将重新发送数据，并且不会因故障而丢失任何数据。</li>
<li><em>不可靠的接收器</em> - 此类接收器<em>不</em>发送确认，因此当它们由于工作线程或驱动程序故障而失败时<em>可能会</em>丢失数据。</li>
</ol>
<p>根据所使用的接收器类型，我们实现以下语义。如果工作线程节点发生故障，则使用可靠的接收器不会丢失数据。对于不可靠的接收器，接收但未复制的数据可能会丢失。如果驱动程序节点发生故障，则除了这些损失之外，在内存中接收和复制的所有过去数据都将丢失。这将影响有状态转换的结果。</p>
<p>为了避免过去接收数据的这种丢失，Spark 1.2引入了<em>预写日志</em>，将接收到的数据保存到容错存储中。启用<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#deploying-applications">预写日志</a>和可靠的接收器后，数据丢失为零。在语义方面，它提供了至少一次的保证。</p>
<p>下表总结了失败下的语义：</p>
<table>
<thead>
<tr>
<th align="left">部署方案</th>
<th align="left">工作线程故障</th>
<th align="left">驱动程序故障</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><em>Spark 1.1 或更早版本，</em>或 <em>Spark 1.2 或更高版本，不带预写日志</em></td>
<td align="left">使用不可靠的接收器丢失缓冲数据 使用可靠的接收器 实现零数据丢失 至少一次语义</td>
<td align="left">不可靠的接收器丢失的缓冲数据 所有接收器丢失的过去数据 未定义的语义</td>
</tr>
<tr>
<td align="left"><em>Spark 1.2 或更高版本，带有预写日志</em></td>
<td align="left">使用可靠的接收器 实现零数据丢失 至少一次语义</td>
<td align="left">通过可靠的接收器和文件 实现零数据丢失 至少一次语义</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h3 id="使用-Kafka-Direct-API"><a href="#使用-Kafka-Direct-API" class="headerlink" title="使用 Kafka Direct API"></a>使用 Kafka Direct API</h3><p>在 Spark 1.3 中，我们引入了一个新的 Kafka Direct API，它可以确保 Spark Streaming 只接收一次所有 Kafka 数据。除此之外，如果您实现精确一次的输出操作，则可以实现端到端的精确一次保证。<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/streaming-kafka-0-10-integration.html">《Kafka 集成指南</a>》进一步讨论了这种方法。</p>
<h2 id="输出操作的语义"><a href="#输出操作的语义" class="headerlink" title="输出操作的语义"></a>输出操作的语义</h2><p>输出操作（如 ）<em>至少具有一次</em>语义，也就是说，在工作线程发生故障时，转换后的数据可能会多次写入外部实体。虽然这对于使用操作保存到文件系统是可以接受的（因为文件只会被相同的数据覆盖），但可能需要额外的工作来实现恰好一次的语义。有两种方法。<code>foreachRDD``saveAs***Files</code></p>
<ul>
<li><em>幂等更新</em>：多次尝试始终写入相同的数据。例如，始终将相同的数据写入生成的文件。<code>saveAs***Files</code></li>
<li><em>事务性更新</em>：所有更新都是以事务方式进行的，因此更新恰好以原子方式进行一次。执行此操作的一种方法是以下方法。<ul>
<li>使用 RDD 的批处理时间（在 中可用）和分区索引来创建标识符。此标识符唯一标识流式处理应用程序中的 Blob 数据。<code>foreachRDD</code></li>
<li>使用标识符以事务方式（即，正好一次，原子方式）使用此 Blob 更新外部系统。也就是说，如果标识符尚未提交，请以原子方式提交分区数据和标识符。否则，如果已提交此操作，请跳过更新。</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://s-luping.github.io/luping">luping</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://s-luping.github.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">https://s-luping.github.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/luping/tags/spark/">spark</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=null" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/luping/2021/10/02/spark%20job%E7%9A%84%E5%87%A0%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"><img class="prev-cover" src="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&amp;riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&amp;ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1" onerror="onerror=null;src='/luping/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">spark job的几种提交流程</div></div></a></div><div class="next-post pull-right"><a href="/luping/2021/04/02/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"><img class="next-cover" src="https://tse1-mm.cn.bing.net/th/id/OIP-C.kwcwQcrglWgnKVXlEww87gHaHa?pid=ImgDet&amp;rs=1" onerror="onerror=null;src='/luping/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Hadoop学习记录</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/luping/2021/10/02/spark%20job%E7%9A%84%E5%87%A0%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/" title="spark job的几种提交流程"><img class="cover" src="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-01</div><div class="title">spark job的几种提交流程</div></div></a></div><div><a href="/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/" title="spark将数据加载到hbase--bulkload方式"><img class="cover" src="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-22</div><div class="title">spark将数据加载到hbase--bulkload方式</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://tse1-mm.cn.bing.net/th/id/R-C.50a11e325200050a0094a0bc5e302cf8?rik=%2bxttpKGOrVrr5w&amp;riu=http%3a%2f%2fwww.gx8899.com%2fuploads%2fallimg%2f2017110610%2fogkgrubpb00.jpg&amp;ehk=GRkc8RLcFB9TWPLsMsgxie8Z%2fuHG8VS1eT%2f2%2bPUR8Nc%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1" onerror="this.onerror=null;this.src='/luping/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">luping</div><div class="author-info__description">there are some notes of luping</div></div><div class="card-info-data site-data is-center"><a href="/luping/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a><a href="/luping/tags/"><div class="headline">Tags</div><div class="length-num">10</div></a><a href="/luping/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/s-luping" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:461093788@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to my Blog!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">spark介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark%E4%B8%8EHadoop%E5%B7%AE%E5%BC%82"><span class="toc-number">1.1.</span> <span class="toc-text">Spark与Hadoop差异</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E5%AE%89%E8%A3%85"><span class="toc-number">2.</span> <span class="toc-text">spark安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E8%A7%A3%E5%8E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">下载解压</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">2.2.</span> <span class="toc-text">添加系统环境变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-shell"><span class="toc-number">2.3.</span> <span class="toc-text">spark-shell</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%89%A7%E8%A1%8C"><span class="toc-number">3.</span> <span class="toc-text">spark任务提交执行</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#standalone-spark%E8%87%AA%E4%B8%BB%E7%AE%A1%E7%90%86%E7%9A%84%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F"><span class="toc-number">3.1.</span> <span class="toc-text">standalone spark自主管理的集群模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-on-yarn-%E6%8F%90%E4%BA%A4%E5%88%B0hadoop%E7%9A%84yarn%E9%9B%86%E7%BE%A4%E6%89%A7%E8%A1%8C"><span class="toc-number">3.2.</span> <span class="toc-text">spark on yarn 提交到hadoop的yarn集群执行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E7%9A%84%E6%A8%A1%E5%9D%97"><span class="toc-number">4.</span> <span class="toc-text">spark的模块</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-Core"><span class="toc-number">4.1.</span> <span class="toc-text">spark Core</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E6%93%8D%E4%BD%9C%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.1.</span> <span class="toc-text">RDD操作函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DAG-stage-%E5%88%92%E5%88%86%E4%BE%9D%E6%8D%AE"><span class="toc-number">4.1.2.</span> <span class="toc-text">DAG stage 划分依据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#shuffle%E4%BC%98%E5%8C%96"><span class="toc-number">4.1.3.</span> <span class="toc-text">shuffle优化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90"><span class="toc-number">5.</span> <span class="toc-text">spark常用算子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90-Transformations"><span class="toc-number">5.1.</span> <span class="toc-text">转换算子(Transformations)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90-Actions"><span class="toc-number">5.2.</span> <span class="toc-text">行动算子(Actions)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%BC%93%E5%AD%98"><span class="toc-number">5.3.</span> <span class="toc-text">RDD缓存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-number">5.4.</span> <span class="toc-text">广播变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-number">5.5.</span> <span class="toc-text">累加器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark-SQL"><span class="toc-number">6.</span> <span class="toc-text">spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#UDF%E5%92%8CUDAF"><span class="toc-number">6.1.</span> <span class="toc-text">UDF和UDAF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSources"><span class="toc-number">6.2.</span> <span class="toc-text">DataSources</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#load-x2F-save%E5%87%BD%E6%95%B0"><span class="toc-number">6.2.1.</span> <span class="toc-text">load &#x2F;save函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Save-Modes"><span class="toc-number">6.2.2.</span> <span class="toc-text">Save Modes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON-file"><span class="toc-number">6.2.3.</span> <span class="toc-text">JSON file</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hive-%E8%A1%A8"><span class="toc-number">6.2.4.</span> <span class="toc-text">hive 表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JDBC-TO-OTHER-Databases"><span class="toc-number">6.2.5.</span> <span class="toc-text">JDBC TO OTHER Databases</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98"><span class="toc-number">6.3.</span> <span class="toc-text">性能调优</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%93%E5%AD%98%E5%86%85%E5%AD%98%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E2%80%8E"><span class="toc-number">6.4.</span> <span class="toc-text">缓存内存中的数据‎</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%8E%E5%85%B6%E4%BB%96%E9%85%8D%E7%BD%AE%E9%80%89%E9%A1%B9%E2%80%8E"><span class="toc-number">6.5.</span> <span class="toc-text">‎其他配置选项‎</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SQL-%E6%9F%A5%E8%AF%A2%E7%9A%84%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-number">6.6.</span> <span class="toc-text">SQL 查询的使用广播变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E-Apache-Hive-%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="toc-number">6.7.</span> <span class="toc-text">与 Apache Hive 的兼容性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8D%95%E5%85%83%E5%8A%9F%E8%83%BD"><span class="toc-number">6.7.1.</span> <span class="toc-text">支持的配置单元功能</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#structure-treaming"><span class="toc-number">6.8.</span> <span class="toc-text">structure treaming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Output-Sinks"><span class="toc-number">6.8.1.</span> <span class="toc-text">Output Sinks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Triggers-%E8%A7%A6%E5%8F%91%E5%99%A8"><span class="toc-number">6.8.2.</span> <span class="toc-text">Triggers 触发器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sparkstreaming"><span class="toc-number">7.</span> <span class="toc-text">sparkstreaming</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#A-Quick-Example"><span class="toc-number">8.</span> <span class="toc-text">A Quick Example</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E2%80%8E"><span class="toc-number">8.1.</span> <span class="toc-text">基本概念‎</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%8E%E5%88%9D%E5%A7%8B%E5%8C%96spark%E4%B8%8A%E4%B8%8B%E6%96%87"><span class="toc-number">8.2.</span> <span class="toc-text">‎初始化spark上下文</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%80%8E%E8%A6%81%E8%AE%B0%E4%BD%8F%E7%9A%84%E8%A6%81%E7%82%B9%EF%BC%9A%E2%80%8E"><span class="toc-number">8.2.0.0.1.</span> <span class="toc-text">‎要记住的要点：‎</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DStream"><span class="toc-number">8.3.</span> <span class="toc-text">DStream</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%B5%81"><span class="toc-number">8.3.1.</span> <span class="toc-text">文件流</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%8E%E5%A6%82%E4%BD%95%E7%9B%91%E8%A7%86%E7%9B%AE%E5%BD%95%E2%80%8E"><span class="toc-number">8.3.1.1.</span> <span class="toc-text">‎如何监视目录‎</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%8E%E6%96%87%E4%BB%B6%E5%86%99%E5%85%A5%E6%97%B6%E4%B8%8D%E8%A6%81%E7%9B%91%E6%8E%A7"><span class="toc-number">8.3.1.2.</span> <span class="toc-text">‎文件写入时不要监控</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%8EDStream-%E4%B8%8A%E7%9A%84%E8%BD%AC%E6%8D%A2%E2%80%8E"><span class="toc-number">8.4.</span> <span class="toc-text">‎DStream 上的转换‎</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%8EUpdateStateByKey-%E6%93%8D%E4%BD%9C%E2%80%8E"><span class="toc-number">8.4.0.1.</span> <span class="toc-text">‎UpdateStateByKey 操作‎</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%8E%E8%BD%AC%E6%8D%A2%E6%93%8D%E4%BD%9C%E2%80%8E"><span class="toc-number">8.4.0.2.</span> <span class="toc-text">‎转换操作‎</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%8E%E7%AA%97%E5%8F%A3%E6%93%8D%E4%BD%9C%E2%80%8E"><span class="toc-number">8.4.0.3.</span> <span class="toc-text">‎窗口操作‎</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%8E%E8%81%94%E6%8E%A5%E6%93%8D%E4%BD%9C%E2%80%8E"><span class="toc-number">8.4.0.4.</span> <span class="toc-text">‎联接操作‎</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E2%80%8E%E6%B5%81-%E6%B5%81%E8%81%94%E6%8E%A5%E2%80%8E"><span class="toc-number">8.4.0.4.1.</span> <span class="toc-text">‎流-流联接‎</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%8EDStream-%E4%B8%8A%E7%9A%84%E8%BE%93%E5%87%BA%E6%93%8D%E4%BD%9C%E2%80%8E"><span class="toc-number">8.5.</span> <span class="toc-text">‎DStream 上的输出操作‎</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%8E%E7%BC%93%E5%AD%98-x2F-%E6%8C%81%E4%B9%85%E6%80%A7%E2%80%8E"><span class="toc-number">8.6.</span> <span class="toc-text">‎缓存&#x2F;持久性‎</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%8E%E6%A3%80%E6%9F%A5%E7%82%B9%E2%80%8E"><span class="toc-number">8.7.</span> <span class="toc-text">‎检查点‎</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%8E%E4%BD%95%E6%97%B6%E5%90%AF%E7%94%A8%E6%A3%80%E6%9F%A5%E7%82%B9%E2%80%8E"><span class="toc-number">8.7.0.1.</span> <span class="toc-text">‎何时启用检查点‎</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E2%80%8E%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AE%E6%A3%80%E6%9F%A5%E7%82%B9%E2%80%8E"><span class="toc-number">8.7.0.2.</span> <span class="toc-text">‎如何配置检查点‎</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%8E%E9%83%A8%E7%BD%B2%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E2%80%8E"><span class="toc-number">8.8.</span> <span class="toc-text">‎部署应用程序‎</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E2%80%8E%E8%A6%81%E6%B1%82%E2%80%8E"><span class="toc-number">8.8.1.</span> <span class="toc-text">‎要求‎</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%87%E7%BA%A7%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E4%BB%A3%E7%A0%81"><span class="toc-number">8.8.2.</span> <span class="toc-text">升级应用程序代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%91%E6%8E%A7%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F"><span class="toc-number">8.9.</span> <span class="toc-text">监控应用程序</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98-1"><span class="toc-number">9.</span> <span class="toc-text">性能调优</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%8F%E5%B0%91%E6%89%B9%E5%A4%84%E7%90%86%E6%97%B6%E9%97%B4"><span class="toc-number">9.1.</span> <span class="toc-text">减少批处理时间</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%8E%A5%E6%94%B6%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%BA%A6%E7%BA%A7%E5%88%AB"><span class="toc-number">9.1.1.</span> <span class="toc-text">数据接收的并行度级别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%BA%A6%E7%BA%A7%E5%88%AB"><span class="toc-number">9.1.2.</span> <span class="toc-text">数据处理中的并行度级别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">9.1.3.</span> <span class="toc-text">数据序列化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E5%90%AF%E5%8A%A8%E5%BC%80%E9%94%80"><span class="toc-number">9.1.4.</span> <span class="toc-text">任务启动开销</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%89%B9%E5%A4%84%E7%90%86%E9%97%B4%E9%9A%94"><span class="toc-number">9.2.</span> <span class="toc-text">设置正确的批处理间隔</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E8%B0%83%E6%95%B4"><span class="toc-number">9.3.</span> <span class="toc-text">内存调整</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%A6%81%E8%AE%B0%E4%BD%8F%E7%9A%84%E8%A6%81%E7%82%B9%EF%BC%9A"><span class="toc-number">9.3.0.0.1.</span> <span class="toc-text">要记住的要点：</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%B9%E9%94%99%E8%AF%AD%E4%B9%89"><span class="toc-number">10.</span> <span class="toc-text">容错语义</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">10.1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">10.2.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E4%B9%89"><span class="toc-number">10.3.</span> <span class="toc-text">基本语义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8E%A5%E6%94%B6%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%AD%E4%B9%89"><span class="toc-number">10.4.</span> <span class="toc-text">接收数据的语义</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%87%E4%BB%B6"><span class="toc-number">10.4.1.</span> <span class="toc-text">使用文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9F%BA%E4%BA%8E%E6%8E%A5%E6%94%B6%E5%99%A8%E7%9A%84%E6%BA%90"><span class="toc-number">10.4.2.</span> <span class="toc-text">使用基于接收器的源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-Kafka-Direct-API"><span class="toc-number">10.4.3.</span> <span class="toc-text">使用 Kafka Direct API</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E6%93%8D%E4%BD%9C%E7%9A%84%E8%AF%AD%E4%B9%89"><span class="toc-number">10.5.</span> <span class="toc-text">输出操作的语义</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/luping/2022/02/11/vue%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" title="vue学习记录"><img src="https://img-hello-world.oss-cn-beijing.aliyuncs.com/imgs/353233a188d5164d6252d9547c6120d8.jpg" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="vue学习记录"/></a><div class="content"><a class="title" href="/luping/2022/02/11/vue%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" title="vue学习记录">vue学习记录</a><time datetime="2022-02-10T20:43:38.000Z" title="Created 2022-02-10 12:43:38">2022-02-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/" title="spark将数据加载到hbase--bulkload方式"><img src="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&amp;riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&amp;ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="spark将数据加载到hbase--bulkload方式"/></a><div class="content"><a class="title" href="/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/" title="spark将数据加载到hbase--bulkload方式">spark将数据加载到hbase--bulkload方式</a><time datetime="2021-11-22T21:30:25.000Z" title="Created 2021-11-22 13:30:25">2021-11-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/luping/2021/11/02/sudo%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E5%91%BD%E4%BB%A4/" title="ubuntu sudo执行shell脚本环境变量失效"><img src="https://tse1-mm.cn.bing.net/th/id/R-C.4e264b4b01124b6c8e1db08158f3ad2c?rik=pPWPcA6A%2fM2iEA&amp;riu=http%3a%2f%2fsucai.laixuexi.cc%3a88%2fimg2019%2fsoft%2f2018%2f0116%2f125837_61632181.jpg&amp;ehk=72tDFIPppBR0JpkunQGY%2bw7caz%2fbxck5AAkYkOQYKms%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="ubuntu sudo执行shell脚本环境变量失效"/></a><div class="content"><a class="title" href="/luping/2021/11/02/sudo%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E5%91%BD%E4%BB%A4/" title="ubuntu sudo执行shell脚本环境变量失效">ubuntu sudo执行shell脚本环境变量失效</a><time datetime="2021-11-01T20:43:38.000Z" title="Created 2021-11-01 12:43:38">2021-11-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/luping/2021/10/27/Hive%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="Hive网站日志采集统计分析"><img src="https://tse4-mm.cn.bing.net/th/id/OIP-C.uRqW4OzIZXQTJoE0CTBE0gHaFj?pid=ImgDet&amp;rs=1" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="Hive网站日志采集统计分析"/></a><div class="content"><a class="title" href="/luping/2021/10/27/Hive%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="Hive网站日志采集统计分析">Hive网站日志采集统计分析</a><time datetime="2021-10-26T22:12:06.000Z" title="Created 2021-10-26 14:12:06">2021-10-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/luping/2021/10/11/Hive%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E2%BD%87%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" title="Hive新浪微博日志查询分析"><img src="https://tse4-mm.cn.bing.net/th/id/OIP-C.uRqW4OzIZXQTJoE0CTBE0gHaFj?pid=ImgDet&amp;rs=1" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="Hive新浪微博日志查询分析"/></a><div class="content"><a class="title" href="/luping/2021/10/11/Hive%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E2%BD%87%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" title="Hive新浪微博日志查询分析">Hive新浪微博日志查询分析</a><time datetime="2021-10-10T22:12:06.000Z" title="Created 2021-10-10 14:12:06">2021-10-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&amp;riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&amp;ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By luping</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/luping/js/utils.js"></script><script src="/luping/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><div><a target="_blank" href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=u8jXzsvS1dz73dTD1trS15XY1NY" style="text-decoration:none;right:0"> <img src="http://rescdn.qqmail.com/zh_CN/htmledition/images/function/qm_open/ico_mailme_02.png"/> </a></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>