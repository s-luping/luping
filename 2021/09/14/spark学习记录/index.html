<!DOCTYPE html><html lang="zh" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Spark学习记录 | Lu Ping's blog</title><meta name="keywords" content="spark"><meta name="author" content="luping"><meta name="copyright" content="luping"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="spark学习记录">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark学习记录">
<meta property="og:url" content="https://sluping.gitee.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="Lu Ping&#39;s blog">
<meta property="og:description" content="spark学习记录">
<meta property="og:locale">
<meta property="og:image" content="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1">
<meta property="article:published_time" content="2021-09-13T19:38:03.000Z">
<meta property="article:modified_time" content="2022-03-29T10:07:28.691Z">
<meta property="article:author" content="luping">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1"><link rel="shortcut icon" href="/luping/img/favicon.png"><link rel="canonical" href="https://sluping.gitee.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/luping/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/luping/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark学习记录',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-03-29 10:07:28'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/luping/atom.xml" title="Lu Ping's blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://tse1-mm.cn.bing.net/th/id/R-C.50a11e325200050a0094a0bc5e302cf8?rik=%2bxttpKGOrVrr5w&amp;riu=http%3a%2f%2fwww.gx8899.com%2fuploads%2fallimg%2f2017110610%2fogkgrubpb00.jpg&amp;ehk=GRkc8RLcFB9TWPLsMsgxie8Z%2fuHG8VS1eT%2f2%2bPUR8Nc%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/luping/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a><a href="/luping/tags/"><div class="headline">Tags</div><div class="length-num">10</div></a><a href="/luping/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/luping/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/luping/archives/"><i class="fa-fw fas fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://blog.csdn.net/kun666666?spm=1010.2135.3001.5343"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&amp;riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&amp;ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/luping/">Lu Ping's blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/luping/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/luping/archives/"><i class="fa-fw fas fa-archive"></i><span> 档案</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://blog.csdn.net/kun666666?spm=1010.2135.3001.5343"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark学习记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-09-13T19:38:03.000Z" title="Created 2021-09-13 19:38:03">2021-09-13</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-03-29T10:07:28.691Z" title="Updated 2022-03-29 10:07:28">2022-03-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/luping/categories/bigdata/">bigdata</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark学习记录"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="spark介绍"><a href="#spark介绍" class="headerlink" title="spark介绍"></a>spark介绍</h1><p>Spark是加州大学伯克利分校AMP实验室（Algorithms, Machines, and People Lab）开发的通用内存并行计算框架<br>Spark使用Scala语言进行实现，它是一种面向对象、函数式编程语言，能够像操作本地集合对象一样轻松地操作分布式数据集，具有以下特点:<br>1.运行速度快：Spark拥有DAG执行引擎，支持在内存中对数据进行迭代计算。官方提供的数据表明，如果数据由磁盘读取，速度是Hadoop MapReduce的10倍以上，如果数据从内存中读取，速度可以高达100多倍。<br>2.易用性好：Spark不仅支持Scala编写应用程序，而且支持Java和Python等语言进行编写，特别是Scala是一种高效、可拓展的语言，能够用简洁的代码处理较为复杂的处理工作。<br>3.通用性强：Spark生态圈即BDAS（伯克利数据分析栈）包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX等组件，这些组件分别处理Spark Core提供内存计算框架、SparkStreaming的实时处理应用、Spark SQL的即席查询、MLlib或MLbase的机器学习和GraphX的图处理。<br>4.随处运行：Spark具有很强的适应性，能够读取HDFS、Cassandra、HBase、S3和Techyon为持久层读写原生数据，能够以Mesos、YARN和自身携带的Standalone作为资源管理器调度job，来完成Spark应用程序的计算</p>
<h2 id="Spark与Hadoop差异"><a href="#Spark与Hadoop差异" class="headerlink" title="Spark与Hadoop差异"></a>Spark与Hadoop差异</h2><p>Spark是在借鉴了MapReduce之上发展而来的，继承了其分布式并行计算的优点并改进了MapReduce明显的缺陷，具体如下:<br>首先，Spark把中间数据放到内存中，迭代运算效率高。MapReduce中计算结果需要落地，保存到磁盘上，这样势必会影响整体速度，而Spark支持DAG图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，提高了处理效率。<br>其次，Spark容错性高。Spark引进了弹性分布式数据集RDD (Resilient Distributed Dataset) 的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，如果数据集一部分丢失，则可以根据“血统”（即充许基于数据衍生过程）对它们进行重建。另外在RDD计算时可以通过CheckPoint来实现容错，而CheckPoint有两种方式：CheckPoint Data，和Logging The Updates，用户可以控制采用哪种方式来实现容错。<br>最后，Spark更加通用。不像Hadoop只提供了Map和Reduce两种操作，Spark提供的数据集操作类型有很多种，大致分为：Transformations和Actions两大类。Transformations包括Map、Filter、FlatMap、Sample、GroupByKey、ReduceByKey、Union、Join、Cogroup、MapValues、Sort和PartionBy等多种操作类型，同时还提供Count, Actions包括Collect、Reduce、Lookup和Save等操作。另外各个处理节点之间的通信模型不再像Hadoop只有Shuffle一种模式，用户可以命名、物化，控制中间结果的存储、分区等。<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/kxiaozhuk/article/details/82699175">原文链接</a></p>
<h1 id="spark安装"><a href="#spark安装" class="headerlink" title="spark安装"></a>spark安装</h1><h2 id="下载解压"><a href="#下载解压" class="headerlink" title="下载解压"></a>下载解压</h2><p>下载安装包 解压到本地软件安装目录<br><a target="_blank" rel="noopener" href="https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8.tgz">spark-2.4.8.tgz</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers</span><br><span class="line">wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8.tgz</span><br><span class="line">tar xvf spark-2.4.8.tgz .</span><br></pre></td></tr></table></figure>
<h2 id="添加系统环境变量"><a href="#添加系统环境变量" class="headerlink" title="添加系统环境变量"></a>添加系统环境变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/servers/spark-2.4.8</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>
<h2 id="spark-shell"><a href="#spark-shell" class="headerlink" title="spark-shell"></a>spark-shell</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure>
<p><img src="/luping/img_6.png" alt="img_5.png"></p>
<h1 id="spark任务提交执行"><a href="#spark任务提交执行" class="headerlink" title="spark任务提交执行"></a>spark任务提交执行</h1><h2 id="standalone-spark自主管理的集群模式"><a href="#standalone-spark自主管理的集群模式" class="headerlink" title="standalone spark自主管理的集群模式"></a>standalone spark自主管理的集群模式</h2><p><strong>要配置spark安装目录下的slaves文件</strong>添加本地注意域名映射<br><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_7.png" alt="img_7.png"><br>通过spark-submit提交任务时，在任务提交节点或Client启动driver，<br>在driver创建并初始化sparkContext对象包含DAGScheduler和TaskScheduler，<br>与master通信申请资源，master指派worker为其启动executor<br>生成job阶段，遇到行动算子生成一个job<br>DAGScheduler负责把Sparkjob转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；<br>TaskScheduler与Master节点通讯申请注册Application，Master节点接收到Application的注册请求后，通过资源调度算法，在自己的集群的worker上启动Executor进程；启动的Executor也会反向注册到TaskScheduler上<br>所有task运行完成后，SparkContext向Master注销，释放资源；<br>Stage阶段划分<br>根据宽依赖窄依赖划分阶段，判断宽依赖和窄依赖的依据是是否进行shuffle操作，不需要shuffle的窄依赖分到一个阶段中间的RDD转换操作无需落地，而宽依赖需要shuffle的过程数据需要落地磁盘</p>
<h2 id="spark-on-yarn-提交到hadoop的yarn集群执行"><a href="#spark-on-yarn-提交到hadoop的yarn集群执行" class="headerlink" title="spark on yarn 提交到hadoop的yarn集群执行"></a>spark on yarn 提交到hadoop的yarn集群执行</h2><p><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_8.png" alt="img_8.png"><br>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler<br>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster<br>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源<br>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver<br>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task<br>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p>
<h1 id="spark的模块"><a href="#spark的模块" class="headerlink" title="spark的模块"></a>spark的模块</h1><h2 id="spark-Core"><a href="#spark-Core" class="headerlink" title="spark Core"></a>spark Core</h2><p>###RDD<br>Spark提供的主要抽象是弹性分布式数据集(RDD),它是跨集群节点分区的元素集合,可以并行操作.<br>RDD特点:</p>
<ul>
<li>1.它是在集群节点上的不可变的、已分区的集合对象;</li>
<li>2.通过并行转换的方式来创建(如 Map、 filter、join 等);</li>
<li>3.失败自动重建;</li>
<li>4.可以控制存储级别(内存、磁盘等)来进行重用;</li>
<li>5.必须是可序列化的;</li>
<li>6.是静态类型的(只读)。</li>
</ul>
<h3 id="RDD操作函数"><a href="#RDD操作函数" class="headerlink" title="RDD操作函数"></a>RDD操作函数</h3><p>RDD的操作函数主要分为2种类型行动算子(Transformation)和转换算子(Action).<br>可以对RDD进行函数操作,当你对一个RDD进行了操作,那么结果将会是一个新的RDD<br><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_9.png" alt="img_9.png"><br>Transformation操作不是马上提交Spark集群执行,Spark在遇到 Transformation操作时只会记录需要这样的操作,并不会去执行,需要等到有Action 操作的时候才会真正启动计算过程进行计算.<br>针对每个 Action,Spark 会生成一个Job, 从数据的创建开始,经过 Transformation, 结尾是 Action 操作.<br>这些操作对应形成一个有向无环图(DAG),形成 DAG 的先决条件是最后的函数操作是一个Action.</p>
<h3 id="DAG-stage-划分依据"><a href="#DAG-stage-划分依据" class="headerlink" title="DAG stage 划分依据"></a>DAG stage 划分依据</h3><p><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_10.png" alt="img_10.png"><br>spark dagscheduler将任务划分stage,shuffle是划分DAG中stage 的标识,同时影响Spark执行速度的关键步骤.<br>RDD 的 Transformation 函数中,又分为窄依赖(narrow dependency)和宽依赖(wide dependency)的操作.<br>窄依赖跟宽依赖的区别是是否发生 shuffle(洗牌) 操作.宽依赖会发生 shuffle 操作.<br>窄依赖是子 RDD的各个分片(partition)不依赖于其他分片,能够独立计算得到结果,<br>宽依赖指子 RDD 的各个分片会依赖于父RDD 的多个分片,所以会造成父 RDD 的各个分片在集群中重新分片</p>
<h3 id="shuffle优化"><a href="#shuffle优化" class="headerlink" title="shuffle优化"></a>shuffle优化</h3><p>shuffle涉及网络传输和磁盘io,非常消耗资源 因此需要对shuffle优化<br><strong>一是如果可以避免shuffle则不选择涉及shuffle的算子</strong><br>rdd.groupByKey().mapValues(_ .sum) 与 rdd.reduceByKey(_ + _) 执行的结果是一样的，但是前者需要把全部的数据通过网络传递一遍，而后者只需要根据每个 key 局部的 partition 累积结果，在 shuffle 的之后把局部的累积值相加后得到结果.<br><strong>缓存机制 cache persist</strong><br>Spark中对于一个RDD执行多次算子(函数操作)的默认原理是这样的:每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。<br>对于这种情况,可对多次使用的RDD进行持久化。<br>cache 是使用的默认缓存选项,一般默认为Memoryonly(内存中缓存),<br>persist 则可以在缓存的时候选择任意一种缓存类型.事实上,cache内部调用的是默认的persist.persist可选择的方式很多缓存到磁盘或是内存磁盘组合缓存等</p>
<h1 id="spark常用算子"><a href="#spark常用算子" class="headerlink" title="spark常用算子"></a>spark常用算子</h1><h2 id="转换算子-Transformations"><a href="#转换算子-Transformations" class="headerlink" title="转换算子(Transformations)"></a>转换算子(Transformations)</h2><table>
<thead>
<tr>
<th>Transformations</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>map(func)</td>
<td>通过函数func传递源的每个元素，返回一个新的分布式数据集。</td>
</tr>
<tr>
<td>filter(func)</td>
<td>过滤数据，通过选择func返回true的源元素返回一个新的数据集。</td>
</tr>
<tr>
<td>flatMap(func)</td>
<td>与map类似，但是每个输入项都可以映射到0个或更多的输出项(因此func应该返回一个Seq而不是单个项)。展平 多个集合 汇总成一个集合</td>
</tr>
<tr>
<td>mapPartitions(func)</td>
<td>与map类似，但在RDD的每个分区(块)上分别运行，因此在类型为T的RDD上运行时，func必须是Iterator &#x3D;&gt; Iterator</td>
</tr>
<tr>
<td>mapPartitionsWithIndex(func)</td>
<td>与mapPartitions类似，但也为func提供了一个表示分区索引的整数值，因此func必须是类型(Int, Iterator) &#x3D;&gt; Iterator时，类型为T的RDD。</td>
</tr>
<tr>
<td>sample(withReplacement, fraction, seed)</td>
<td>使用给定的随机数生成器种子，对数据的一小部分进行抽样，无论是否进行替换。</td>
</tr>
<tr>
<td>union(otherDataset)</td>
<td>合并，返回一个新数据集，其中包含源数据集中的元素和参数的并集。</td>
</tr>
<tr>
<td>intersection(otherDataset)</td>
<td>交集，返回一个新的RDD，其中包含源数据集中的元素和参数的交集。</td>
</tr>
<tr>
<td>distinct([numPartitions]))</td>
<td>去重，返回包含源数据集的不同元素的新数据集。</td>
</tr>
<tr>
<td>groupByKey([numPartitions])</td>
<td>当对一个(K, V)对的数据集调用时，返回一个(K，可迭代)对的数据集。注意:如果您要对每个键进行分组以执行聚合(比如求和或平均)，那么使用reduceByKey或aggregateByKey将产生更好的性能。注意:默认情况下，输出中的并行级别取决于父RDD的分区数量。您可以传递一个可选的numPartitions参数来设置不同数量的任务。</td>
</tr>
<tr>
<td>reduceByKey(func, [numPartitions])</td>
<td>在（K，V）对的数据集上调用时，返回一个（K，V）对的数据集，其中每个键的值使用给定的reduce函数func进行聚合，该函数的类型必须是（V，V）&#x3D;&gt;V。与groupByKey一样，reduce任务的数量可以通过可选的第二个参数进行配置。</td>
</tr>
<tr>
<td>aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])</td>
<td>当对一个(K, V)对的数据集调用时，返回一个(K, U)对的数据集，其中每个键的值使用给定的combine函数和一个中立的“零”值进行聚合。允许不同于输入值类型的聚合值类型，同时避免不必要的分配。与groupByKey类似，reduce任务的数量可以通过第二个可选参数进行配置。</td>
</tr>
<tr>
<td>sortByKey([ascending], [numPartitions])</td>
<td>当对一个(K, V)对的数据集(K, V)调用时，K实现有序，返回一个(K, V)对的数据集，按键序升序或降序排序，如布尔升序参数中指定的那样。</td>
</tr>
<tr>
<td>join(otherDataset, [numPartitions])</td>
<td>当对类型(K, V)和(K, W)的数据集调用时，返回一个(K， (V, W))对的数据集，其中包含每个键的所有元素对。通过leftOuterJoin、right touterjoin和fullOuterJoin来支持外部连接。</td>
</tr>
<tr>
<td>cogroup(otherDataset, [numPartitions])</td>
<td>当对类型(K, V)和(K, W)的数据集调用时，返回一个元组(K， (Iterable， Iterable))的数据集。这个操作也称为groupWith。</td>
</tr>
<tr>
<td>cartesian(otherDataset)</td>
<td>当对T和U类型的数据集调用时，返回一个(T, U)对的数据集(所有元素对)。</td>
</tr>
<tr>
<td>pipe(command, [envVars])</td>
<td>通过shell命令(例如Perl或bash脚本)管道传输RDD的每个分区。RDD元素被写入到进程的stdin中，并以字符串的RDD形式返回到它的stdout中的行输出。</td>
</tr>
<tr>
<td>coalesce(numPartitions)</td>
<td>将RDD中的分区数减少到numPartitions。用于筛选大型数据集后更有效地运行操作。</td>
</tr>
<tr>
<td>repartition(numPartitions)</td>
<td>随机重组RDD中的数据，创建更多或更少的分区，并在这些分区之间进行平衡。这总是在网络上对所有数据进行无序处理。</td>
</tr>
<tr>
<td>repartitionAndSortWithinPartitions(partitioner)</td>
<td>根据给定的分区器重新分区RDD，并在每个结果分区中按关键字对记录进行排序。这比在每个分区内调用重新分区然后进行排序更有效，因为它可以将排序向下推到无序处理机制中。</td>
</tr>
</tbody></table>
<h2 id="行动算子-Actions"><a href="#行动算子-Actions" class="headerlink" title="行动算子(Actions)"></a>行动算子(Actions)</h2><p>行动算子从功能上来说作为一个触发器，会触发提交整个作业并开始执行。从代码上来说，它与转换算子的最大不同之处在于：转换算子返回的还是 RDD，行动算子返回的是非 RDD 类型的值，如整数，或者根本没有返回值。</p>
<table>
<thead>
<tr>
<th>Actions</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td>reduce(func)</td>
<td>使用函数func（接受两个参数并返回一个）聚合数据集的元素。函数应该是可交换的和相联的，从而可以并行计算</td>
</tr>
<tr>
<td>collect()</td>
<td>在驱动程序中将数据集的所有元素作为数组返回。这通常在过滤器或其他返回足够小的数据子集的操作之后有用。</td>
</tr>
<tr>
<td>count()</td>
<td>返回数据集中元素的数量。</td>
</tr>
<tr>
<td>first()</td>
<td>返回数据集的第一个元素(类似于take(1))。</td>
</tr>
<tr>
<td>take(n)</td>
<td>返回一个包含数据集前n个元素的数组。</td>
</tr>
<tr>
<td>takeSample(withReplacement, num, [seed])</td>
<td>返回数据集num元素的随机样本数组，可选地预先指定随机数生成器种子，是否进行替换。</td>
</tr>
<tr>
<td>takeOrdered(n, [ordering])</td>
<td>使用自然顺序或自定义比较器返回RDD的前n个元素。</td>
</tr>
<tr>
<td>saveAsTextFile(path)</td>
<td>将数据集的元素作为文本文件(或一组文本文件)写入本地文件系统、HDFS或任何其他hadoop支持的文件系统的给定目录中。Spark将对每个元素调用toString，将其转换为文件中的一行文本。</td>
</tr>
<tr>
<td>saveAsSequenceFile(path)(Java and Scala)</td>
<td>在本地文件系统、HDFS或任何其他Hadoop支持的文件系统的给定路径中，将数据集的元素作为Hadoop序列文件编写。这在实现Hadoop可写接口的键值对RDDs上可用。在Scala中，它还可以用于隐式转换为可写的类型(Spark包括基本类型的转换，如Int、Double、String等)。</td>
</tr>
<tr>
<td>saveAsObjectFile(path)(Java and Scala)</td>
<td>使用Java序列化以简单的格式编写数据集的元素，然后可以使用SparkContext.objectFile()加载这些元素。</td>
</tr>
<tr>
<td>countByKey()</td>
<td>只在类型(K, V)的RDDs上可用。返回一个(K, Int)对的hashmap，并记录每个键的计数。</td>
</tr>
<tr>
<td>foreach(func)</td>
<td>对数据集的每个元素运行函数func。这通常是为了避免副作用，如更新累加器或与外部存储系统交互。注意：在foreach（）之外修改除累加器以外的变量可能会导致未定义的行为。有关更多详细信息，请参见理解闭包。</td>
</tr>
</tbody></table>
<h2 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h2><p>缓存是迭代算法和快速交互式使用的关键工具。‎第一次在操作中计算它时，它将保存在节点上的内存中。Spark的缓存是容错的 - 如果RDD的任何分区丢失，它将使用最初创建它的转换自动重新计算。‎persist() cache()</p>
<table>
<thead>
<tr>
<th align="left">Storage Level</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left">MEMORY_ONLY</td>
<td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</td>
</tr>
<tr>
<td align="left">MEMORY_AND_DISK</td>
<td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed.</td>
</tr>
<tr>
<td align="left">MEMORY_ONLY_SER (Java and Scala)</td>
<td align="left">Store RDD as <em>serialized</em> Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/2.4.5/tuning.html">fast serializer</a>, but more CPU-intensive to read.</td>
</tr>
<tr>
<td align="left">MEMORY_AND_DISK_SER (Java and Scala)</td>
<td align="left">Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of recomputing them on the fly each time they’re needed.</td>
</tr>
<tr>
<td align="left">DISK_ONLY</td>
<td align="left">Store the RDD partitions only on disk.</td>
</tr>
<tr>
<td align="left">MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td>
<td align="left">Same as the levels above, but replicate each partition on two cluster nodes.</td>
</tr>
<tr>
<td align="left">OFF_HEAP (experimental)</td>
<td align="left">Similar to MEMORY_ONLY_SER, but store the data in <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/2.4.5/configuration.html#memory-management">off-heap memory</a>. This requires off-heap memory to be enabled.</td>
</tr>
</tbody></table>
<p>unpersist()</p>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3)</span><br></pre></td></tr></table></figure>

<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val accum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))</span><br><span class="line">10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</span><br><span class="line"></span><br><span class="line">scala&gt; accum.value</span><br><span class="line">res2: Long = 10</span><br></pre></td></tr></table></figure>

<h1 id="spark-SQL"><a href="#spark-SQL" class="headerlink" title="spark SQL"></a>spark SQL</h1><p>SparkSession中所有功能的入口点是SparkSession类.使用:SparkSessionSparkSession.builder()创建SparkSession对象</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Spark SQL example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>

<p>创建dataframes</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure>

<h2 id="UDF和UDAF"><a href="#UDF和UDAF" class="headerlink" title="UDF和UDAF"></a>UDF和UDAF</h2><p>UDF用户自定义非聚合函数</p>
<p>UDAF用户自定义聚合操作函数</p>
<h2 id="DataSources"><a href="#DataSources" class="headerlink" title="DataSources"></a>DataSources</h2><h3 id="load-x2F-save函数"><a href="#load-x2F-save函数" class="headerlink" title="load &#x2F;save函数"></a>load &#x2F;save函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.load(&quot;examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;namesAndFavColors.parquet&quot;)</span><br></pre></td></tr></table></figure>

<p>format</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val peopleDFCsv = spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)</span><br><span class="line">  .load(&quot;examples/src/main/resources/people.csv&quot;)</span><br><span class="line">  </span><br><span class="line">  usersDF.write.format(&quot;orc&quot;)</span><br><span class="line">  .option(&quot;orc.bloom.filter.columns&quot;, &quot;favorite_color&quot;)</span><br><span class="line">  .option(&quot;orc.dictionary.key.threshold&quot;, &quot;1.0&quot;)</span><br><span class="line">  .save(&quot;users_with_options.orc&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="Save-Modes"><a href="#Save-Modes" class="headerlink" title="Save Modes"></a>Save Modes</h3><table>
<thead>
<tr>
<th align="left">Scala&#x2F;Java</th>
<th align="left">Any Language</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>SaveMode.ErrorIfExists</code> (default)</td>
<td align="left">erroror errorifexists(default)</td>
<td align="left">When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td>
</tr>
<tr>
<td align="left"><code>SaveMode.Append</code></td>
<td align="left">append</td>
<td align="left">When saving a DataFrame to a data source, if data&#x2F;table already exists, contents of the DataFrame are expected to be appended to existing data.</td>
</tr>
<tr>
<td align="left">SaveMode.Overwrite</td>
<td align="left">overwrite</td>
<td align="left">Overwrite mode means that when saving a DataFrame to a data source, if data&#x2F;table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td>
</tr>
<tr>
<td align="left">SaveMode.Ignore</td>
<td align="left">ignore</td>
<td align="left">Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected not to save the contents of the DataFrame and not to change the existing data. This is similar to a <code>CREATE TABLE IF NOT EXISTS</code> in SQL.</td>
</tr>
</tbody></table>
<h3 id="JSON-file"><a href="#JSON-file" class="headerlink" title="JSON file"></a>JSON file</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span><br><span class="line">// supported by importing this when creating a Dataset.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// A JSON dataset is pointed to by path.</span><br><span class="line">// The path can be either a single text file or a directory storing text files</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line">// The inferred schema can be visualized using the printSchema() method</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: long (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// Creates a temporary view using the DataFrame</span><br><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL statements can be run by using the sql methods provided by spark</span><br><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">// +------+</span><br><span class="line">// |  name|</span><br><span class="line">// +------+</span><br><span class="line">// |Justin|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><br><span class="line">// a Dataset[String] storing one JSON object per string</span><br><span class="line">val otherPeopleDataset = spark.createDataset(</span><br><span class="line">  &quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |        address|name|</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |[Columbus,Ohio]| Yin|</span><br><span class="line">// +---------------+----+</span><br></pre></td></tr></table></figure>

<h3 id="hive-表"><a href="#hive-表" class="headerlink" title="hive 表"></a>hive 表</h3><p>通过将 中的 （对于安全配置）和（对于 HDFS 配置）文件，可以完成 Hive 的配置。‎<code>hive-site.xml``core-site.xml``hdfs-site.xml``conf/</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &#x27;examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">  case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span><br><span class="line">// `USING hive`</span><br><span class="line">sql(&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;)</span><br><span class="line">// Save DataFrame to the Hive managed table</span><br><span class="line">val df = spark.table(&quot;src&quot;)</span><br><span class="line">df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;)</span><br><span class="line">// After insertion, the Hive managed table has data now</span><br><span class="line">sql(&quot;SELECT * FROM hive_records&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Prepare a Parquet data directory</span><br><span class="line">val dataDir = &quot;/tmp/parquet_data&quot;</span><br><span class="line">spark.range(10).write.parquet(dataDir)</span><br><span class="line">// Create a Hive external Parquet table</span><br><span class="line">sql(s&quot;CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION &#x27;$dataDir&#x27;&quot;)</span><br><span class="line">// The Hive external table should already have data</span><br><span class="line">sql(&quot;SELECT * FROM hive_bigints&quot;).show()</span><br><span class="line">// +---+</span><br><span class="line">// | id|</span><br><span class="line">// +---+</span><br><span class="line">// |  0|</span><br><span class="line">// |  1|</span><br><span class="line">// |  2|</span><br><span class="line">// ... Order may vary, as spark processes the partitions in parallel.</span><br><span class="line"></span><br><span class="line">// Turn on flag for Hive Dynamic Partitioning</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)</span><br><span class="line">// Create a Hive partitioned table using DataFrame API</span><br><span class="line">df.write.partitionBy(&quot;key&quot;).format(&quot;hive&quot;).saveAsTable(&quot;hive_part_tbl&quot;)</span><br><span class="line">// Partitioned column `key` will be moved to the end of the schema.</span><br><span class="line">sql(&quot;SELECT * FROM hive_part_tbl&quot;).show()</span><br><span class="line">// +-------+---+</span><br><span class="line">// |  value|key|</span><br><span class="line">// +-------+---+</span><br><span class="line">// |val_238|238|</span><br><span class="line">// | val_86| 86|</span><br><span class="line">// |val_311|311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>

<h3 id="JDBC-TO-OTHER-Databases"><a href="#JDBC-TO-OTHER-Databases" class="headerlink" title="JDBC TO OTHER Databases"></a>JDBC TO OTHER Databases</h3><table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>url</code></td>
<td align="left">The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., <code>jdbc:postgresql://localhost/test?user=fred&amp;password=secret</code></td>
</tr>
<tr>
<td align="left"><code>dbtable</code></td>
<td align="left">The JDBC table that should be read from or written into. Note that when using it in the read path anything that is valid in a <code>FROM</code> clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses. It is not allowed to specify <code>dbtable</code> and <code>query</code> options at the same time.</td>
</tr>
<tr>
<td align="left"><code>query</code></td>
<td align="left">A query that will be used to read data into Spark. The specified query will be parenthesized and used as a subquery in the <code>FROM</code> clause. Spark will also assign an alias to the subquery clause. As an example, spark will issue a query of the following form to the JDBC Source.  <code>SELECT &lt;columns&gt; FROM (&lt;user_specified_query&gt;) spark_gen_alias</code>  Below are couple of restrictions while using this option. It is not allowed to specify <code>dbtable</code> and <code>query</code> options at the same time.It is not allowed to specify <code>query</code> and <code>partitionColumn</code> options at the same time. When specifying <code>partitionColumn</code> option is required, the subquery can be specified using <code>dbtable</code> option instead and partition columns can be qualified using the subquery alias provided as part of <code>dbtable</code>. Example: <code>spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, jdbcUrl).option(&quot;query&quot;, &quot;select c1, c2 from t1&quot;).load()</code></td>
</tr>
<tr>
<td align="left"><code>driver</code></td>
<td align="left">The class name of the JDBC driver to use to connect to this URL.</td>
</tr>
<tr>
<td align="left"><code>partitionColumn, lowerBound, upperBound</code></td>
<td align="left">These options must all be specified if any of them is specified. In addition, <code>numPartitions</code> must be specified. They describe how to partition the table when reading in parallel from multiple workers. <code>partitionColumn</code> must be a numeric, date, or timestamp column from the table in question. Notice that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.</td>
</tr>
<tr>
<td align="left"><code>numPartitions</code></td>
<td align="left">The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling <code>coalesce(numPartitions)</code> before writing.</td>
</tr>
<tr>
<td align="left"><code>queryTimeout</code></td>
<td align="left">The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on how JDBC drivers implement the API <code>setQueryTimeout</code>, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch. It defaults to <code>0</code>.</td>
</tr>
<tr>
<td align="left"><code>fetchsize</code></td>
<td align="left">The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading.</td>
</tr>
<tr>
<td align="left"><code>batchsize</code></td>
<td align="left">The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to <code>1000</code>.</td>
</tr>
<tr>
<td align="left"><code>isolationLevel</code></td>
<td align="left">The transaction isolation level, which applies to current connection. It can be one of <code>NONE</code>, <code>READ_COMMITTED</code>, <code>READ_UNCOMMITTED</code>, <code>REPEATABLE_READ</code>, or <code>SERIALIZABLE</code>, corresponding to standard transaction isolation levels defined by JDBC’s Connection object, with default of <code>READ_UNCOMMITTED</code>. This option applies only to writing. Please refer the documentation in <code>java.sql.Connection</code>.</td>
</tr>
<tr>
<td align="left"><code>sessionInitStatement</code></td>
<td align="left">After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL&#x2F;SQL block). Use this to implement session initialization code. Example: <code>option(&quot;sessionInitStatement&quot;, &quot;&quot;&quot;BEGIN execute immediate &#39;alter session set &quot;_serial_direct_read&quot;=true&#39;; END;&quot;&quot;&quot;)</code></td>
</tr>
<tr>
<td align="left"><code>truncate</code></td>
<td align="left">This is a JDBC writer related option. When <code>SaveMode.Overwrite</code> is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to <code>false</code>. This option applies only to writing.</td>
</tr>
<tr>
<td align="left"><code>cascadeTruncate</code></td>
<td align="left">This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a <code>TRUNCATE TABLE t CASCADE</code> (in the case of PostgreSQL a <code>TRUNCATE TABLE ONLY t CASCADE</code> is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care. This option applies only to writing. It defaults to the default cascading truncate behaviour of the JDBC database in question, specified in the <code>isCascadeTruncate</code> in each JDBCDialect.</td>
</tr>
<tr>
<td align="left"><code>createTableOptions</code></td>
<td align="left">This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., <code>CREATE TABLE t (name string) ENGINE=InnoDB.</code>). This option applies only to writing.</td>
</tr>
<tr>
<td align="left"><code>createTableColumnTypes</code></td>
<td align="left">The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: <code>&quot;name CHAR(64), comments VARCHAR(1024)&quot;)</code>. The specified types should be valid spark sql data types. This option applies only to writing.</td>
</tr>
<tr>
<td align="left"><code>customSchema</code></td>
<td align="left">The custom schema to use for reading data from JDBC connectors. For example, <code>&quot;id DECIMAL(38, 0), name STRING&quot;</code>. You can also specify partial fields, and the others use the default type mapping. For example, <code>&quot;id DECIMAL(38, 0)&quot;</code>. The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults. This option applies only to reading.</td>
</tr>
<tr>
<td align="left"><code>pushDownPredicate</code></td>
<td align="left">The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><br><span class="line">// Loading data from a JDBC source</span><br><span class="line">val jdbcDF = spark.read</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">connectionProperties.put(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line">// Specifying the custom data types of the read schema</span><br><span class="line">connectionProperties.put(&quot;customSchema&quot;, &quot;id DECIMAL(38, 0), name STRING&quot;)</span><br><span class="line">val jdbcDF3 = spark.read</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">  .save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br></pre></td></tr></table></figure>

<h2 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h2><h2 id="缓存内存中的数据‎"><a href="#缓存内存中的数据‎" class="headerlink" title="缓存内存中的数据‎"></a>缓存内存中的数据‎</h2><p>‎Spark SQL 可以通过调用 或 使用 内存中列式格式 来缓存表。然后，Spark SQL 将仅扫描所需的列，并将自动调整压缩，以最大程度地减少内存使用量和 GC 压力。您可以调用以从内存中删除该表。‎</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.catalog.cacheTable(&quot;tableName&quot;)</span><br><span class="line">dataFrame.cache()</span><br><span class="line">spark.catalog.uncacheTable(&quot;tableName&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="‎其他配置选项‎"><a href="#‎其他配置选项‎" class="headerlink" title="‎其他配置选项‎"></a>‎其他配置选项‎</h2><p>‎以下选项还可用于优化查询执行的性能。这些选项可能会在将来的版本中弃用，因为会自动执行更多优化。‎</p>
<table>
<thead>
<tr>
<th align="left">Property Name</th>
<th align="left">Default</th>
<th align="left">Meaning</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><code>spark.sql.files.maxPartitionBytes</code></td>
<td align="left">134217728 (128 MB)</td>
<td align="left">The maximum number of bytes to pack into a single partition when reading files.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.files.openCostInBytes</code></td>
<td align="left">4194304 (4 MB)</td>
<td align="left">The estimated cost to open a file, measured by the number of bytes could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over-estimated, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</td>
</tr>
<tr>
<td align="left"><code>spark.sql.broadcastTimeout</code></td>
<td align="left">300</td>
<td align="left">Timeout in seconds for the broadcast wait time in broadcast joins</td>
</tr>
<tr>
<td align="left"><code>spark.sql.autoBroadcastJoinThreshold</code></td>
<td align="left">10485760 (10 MB)</td>
<td align="left">Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code> has been run.</td>
</tr>
<tr>
<td align="left"><code>spark.sql.shuffle.partitions</code></td>
<td align="left">200</td>
<td align="left">Configures the number of partitions to use when shuffling data for joins or aggregations.</td>
</tr>
</tbody></table>
<h2 id="SQL-查询的使用广播变量"><a href="#SQL-查询的使用广播变量" class="headerlink" title="SQL 查询的使用广播变量"></a>SQL 查询的使用广播变量</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.functions.broadcast</span><br><span class="line">broadcast(spark.table(&quot;src&quot;)).join(spark.table(&quot;records&quot;), &quot;key&quot;).show()</span><br></pre></td></tr></table></figure>

<h2 id="与-Apache-Hive-的兼容性"><a href="#与-Apache-Hive-的兼容性" class="headerlink" title="与 Apache Hive 的兼容性"></a>与 Apache Hive 的兼容性</h2><h3 id="支持的配置单元功能"><a href="#支持的配置单元功能" class="headerlink" title="支持的配置单元功能"></a>支持的配置单元功能</h3><p>Spark SQL 支持绝大多数 Hive 功能，例如：</p>
<ul>
<li>Hive 查询语句，包括：<ul>
<li><code>SELECT</code></li>
<li><code>GROUP BY</code></li>
<li><code>ORDER BY</code></li>
<li><code>CLUSTER BY</code></li>
<li><code>SORT BY</code></li>
</ul>
</li>
<li>所有 Hive 运算符，包括：<ul>
<li>关系运算符 （， ， ， ， ， ， ， ， 等）<code>=``⇔``==``&lt;&gt;``&lt;``&gt;``&gt;=``&lt;=</code></li>
<li>算术运算符（、、、、等）<code>+``-``*``/``%</code></li>
<li>逻辑运算符（、、、、等）<code>AND``&amp;&amp;``OR``||</code></li>
<li>复杂类型构造函数</li>
<li>数学函数（、、、等）<code>sign``ln``cos</code></li>
<li>字符串函数（、、、等）<code>instr``length``printf</code></li>
</ul>
</li>
<li>用户定义函数 （UDF）</li>
<li>用户定义的聚合函数 （UDAF）</li>
<li>用户定义的序列化格式 （SerDes）</li>
<li>窗口函数</li>
<li>加入<ul>
<li><code>JOIN</code></li>
<li><code>&#123;LEFT|RIGHT|FULL&#125; OUTER JOIN</code></li>
<li><code>LEFT SEMI JOIN</code></li>
<li><code>CROSS JOIN</code></li>
</ul>
</li>
<li>工会</li>
<li>子查询<ul>
<li><code>SELECT col FROM ( SELECT a + b AS col from t1) t2</code></li>
</ul>
</li>
<li>采样</li>
<li>解释</li>
<li>分区表，包括动态分区插入</li>
<li>视图</li>
<li>所有 Hive DDL 函数，包括：<ul>
<li><code>CREATE TABLE</code></li>
<li><code>CREATE TABLE AS SELECT</code></li>
<li><code>ALTER TABLE</code></li>
</ul>
</li>
<li>大多数 Hive 数据类型，包括：<ul>
<li><code>TINYINT</code></li>
<li><code>SMALLINT</code></li>
<li><code>INT</code></li>
<li><code>BIGINT</code></li>
<li><code>BOOLEAN</code></li>
<li><code>FLOAT</code></li>
<li><code>DOUBLE</code></li>
<li><code>STRING</code></li>
<li><code>BINARY</code></li>
<li><code>TIMESTAMP</code></li>
<li><code>DATE</code></li>
<li><code>ARRAY&lt;&gt;</code></li>
<li><code>MAP&lt;&gt;</code></li>
<li><code>STRUCT&lt;&gt;</code></li>
</ul>
</li>
</ul>
<h2 id="structure-treaming"><a href="#structure-treaming" class="headerlink" title="structure treaming"></a>structure treaming</h2><p><strong>example</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Create DataFrame representing the stream of input lines from connection to localhost:9999</span><br><span class="line">val lines = spark.readStream</span><br><span class="line">  .format(&quot;socket&quot;)</span><br><span class="line">  .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">  .option(&quot;port&quot;, 9999)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">// Split the lines into words</span><br><span class="line">val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">// Generate running word count</span><br><span class="line">val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">// Start running the query that prints the running counts to the console</span><br><span class="line">val query = wordCounts.writeStream</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>数据源</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
<th align="left"></th>
</tr>
</thead>
<tbody><tr>
<td align="left">Source</td>
<td align="left">Options</td>
<td align="left">Fault-tolerant</td>
<td align="left">Notes</td>
</tr>
<tr>
<td align="left"><strong>File source</strong></td>
<td align="left"><code>path</code>: path to the input directory, and common to all file formats. <code>maxFilesPerTrigger</code>: maximum number of new files to be considered in every trigger (default: no max) <code>latestFirst</code>: whether to process the latest new files first, useful when there is a large backlog of files (default: false) <code>fileNameOnly</code>: whether to check new files based on only the filename instead of on the full path (default: false). With this set to <code>true</code>, the following files would be considered as the same file, because their filenames, “dataset.txt”, are the same: “file:&#x2F;&#x2F;&#x2F;dataset.txt” “s3:&#x2F;&#x2F;a&#x2F;dataset.txt” “s3n:&#x2F;&#x2F;a&#x2F;b&#x2F;dataset.txt” “s3a:&#x2F;&#x2F;a&#x2F;b&#x2F;c&#x2F;dataset.txt” <code>maxFileAge</code>: Maximum age of a file that can be found in this directory, before it is ignored. For the first batch all files will be considered valid. If <code>latestFirst</code> is set to <code>true</code> and <code>maxFilesPerTrigger</code> is set, then this parameter will be ignored, because old files that are valid, and should be processed, may be ignored. The max age is specified with respect to the timestamp of the latest file, and not the timestamp of the current system.(default: 1 week) <code>cleanSource</code>: option to clean up completed files after processing. Available options are “archive”, “delete”, “off”. If the option is not provided, the default value is “off”. When “archive” is provided, additional option <code>sourceArchiveDir</code> must be provided as well. The value of “sourceArchiveDir” must not match with source pattern in depth (the number of directories from the root directory), where the depth is minimum of depth on both paths. This will ensure archived files are never included as new source files. For example, suppose you provide ‘&#x2F;hello?&#x2F;spark&#x2F;<em>‘ as source pattern, ‘&#x2F;hello1&#x2F;spark&#x2F;archive&#x2F;dir’ cannot be used as the value of “sourceArchiveDir”, as ‘&#x2F;hello?&#x2F;spark&#x2F;</em>‘ and ‘&#x2F;hello1&#x2F;spark&#x2F;archive’ will be matched. ‘&#x2F;hello1&#x2F;spark’ cannot be also used as the value of “sourceArchiveDir”, as ‘&#x2F;hello?&#x2F;spark’ and ‘&#x2F;hello1&#x2F;spark’ will be matched. ‘&#x2F;archived&#x2F;here’ would be OK as it doesn’t match. Spark will move source files respecting their own path. For example, if the path of source file is <code>/a/b/dataset.txt</code> and the path of archive directory is <code>/archived/here</code>, file will be moved to <code>/archived/here/a/b/dataset.txt</code>. NOTE: Both archiving (via moving) or deleting completed files will introduce overhead (slow down, even if it’s happening in separate thread) in each micro-batch, so you need to understand the cost for each operation in your file system before enabling this option. On the other hand, enabling this option will reduce the cost to list source files which can be an expensive operation. Number of threads used in completed file cleaner can be configured with<code>spark.sql.streaming.fileSource.cleaner.numThreads</code> (default: 1). NOTE 2: The source path should not be used from multiple sources or queries when enabling this option. Similarly, you must ensure the source path doesn’t match to any files in output directory of file stream sink. NOTE 3: Both delete and move actions are best effort. Failing to delete or move files will not fail the streaming query. Spark may not clean up some source files in some circumstances - e.g. the application doesn’t shut down gracefully, too many files are queued to clean up.  For file-format-specific options, see the related methods in <code>DataStreamReader</code> (<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html">Scala</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/java/org/apache/spark/sql/streaming/DataStreamReader.html">Java</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader">Python</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/R/read.stream.html">R</a>). E.g. for “parquet” format options see <code>DataStreamReader.parquet()</code>.  In addition, there are session configurations that affect certain file-formats. See the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/sql-programming-guide.html">SQL Programming Guide</a> for more details. E.g., for “parquet”, see <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/sql-data-sources-parquet.html#configuration">Parquet configuration</a> section.</td>
<td align="left">Yes</td>
<td align="left">Supports glob paths, but does not support multiple comma-separated paths&#x2F;globs.</td>
</tr>
<tr>
<td align="left"><strong>Socket Source</strong></td>
<td align="left"><code>host</code>: host to connect to, must be specified <code>port</code>: port to connect to, must be specified</td>
<td align="left">No</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>Rate Source</strong></td>
<td align="left"><code>rowsPerSecond</code> (e.g. 100, default: 1): How many rows should be generated per second.  <code>rampUpTime</code> (e.g. 5s, default: 0s): How long to ramp up before the generating speed becomes <code>rowsPerSecond</code>. Using finer granularities than seconds will be truncated to integer seconds.  <code>numPartitions</code> (e.g. 10, default: Spark’s default parallelism): The partition number for the generated rows.  The source will try its best to reach <code>rowsPerSecond</code>, but the query may be resource constrained, and <code>numPartitions</code> can be tweaked to help reach the desired speed.</td>
<td align="left">Yes</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>Kafka Source</strong></td>
<td align="left">See the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a>.</td>
<td align="left">Yes</td>
<td align="left"></td>
</tr>
</tbody></table>
<h3 id="Output-Sinks"><a href="#Output-Sinks" class="headerlink" title="Output Sinks"></a>Output Sinks</h3><p>There are a few types of built-in output sinks.</p>
<ul>
<li><strong>File sink</strong> - Stores the output to a directory.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;parquet&quot;</span>)        <span class="comment">// can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span></span><br><span class="line">    .option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/destination/dir&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Kafka sink</strong> - Stores the output to one or more topics in Kafka.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;host1:port1,host2:port2&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;topic&quot;</span>, <span class="string">&quot;updates&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Foreach sink</strong> - Runs arbitrary computation on the records in the output. See later in the section for more details.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Console sink (for debugging)</strong> - Prints the output to the console&#x2F;stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Memory sink (for debugging)</strong> - The output is stored in memory as an in-memory table. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory. Hence, use it with caution.</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;memory&quot;</span>)</span><br><span class="line">    .queryName(<span class="string">&quot;tableName&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure>

<p>Some sinks are not fault-tolerant because they do not guarantee persistence of the output and are meant for debugging purposes only. See the earlier section on <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#fault-tolerance-semantics">fault-tolerance semantics</a>. Here are the details of all the sinks in Spark.</p>
<table>
<thead>
<tr>
<th align="left">Sink</th>
<th align="left">Supported Output Modes</th>
<th align="left">Options</th>
<th align="left">Fault-tolerant</th>
<th align="left">Notes</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>File Sink</strong></td>
<td align="left">Append</td>
<td align="left"><code>path</code>: path to the output directory, must be specified. <code>retention</code>: time to live (TTL) for output files. Output files which batches were committed older than TTL will be eventually excluded in metadata log. This means reader queries which read the sink’s output directory may not process them. You can provide the value as string format of the time. (like “12h”, “7d”, etc.) By default it’s disabled.  For file-format-specific options, see the related methods in DataFrameWriter (<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/sql/DataFrameWriter.html">Scala</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/java/org/apache/spark/sql/DataFrameWriter.html">Java</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter">Python</a>&#x2F;<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/api/R/write.stream.html">R</a>). E.g. for “parquet” format options see <code>DataFrameWriter.parquet()</code></td>
<td align="left">Yes (exactly-once)</td>
<td align="left">Supports writes to partitioned tables. Partitioning by time may be useful.</td>
</tr>
<tr>
<td align="left"><strong>Kafka Sink</strong></td>
<td align="left">Append, Update, Complete</td>
<td align="left">See the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a></td>
<td align="left">Yes (at-least-once)</td>
<td align="left">More details in the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a></td>
</tr>
<tr>
<td align="left"><strong>Foreach Sink</strong></td>
<td align="left">Append, Update, Complete</td>
<td align="left">None</td>
<td align="left">Yes (at-least-once)</td>
<td align="left">More details in the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch">next section</a></td>
</tr>
<tr>
<td align="left"><strong>ForeachBatch Sink</strong></td>
<td align="left">Append, Update, Complete</td>
<td align="left">None</td>
<td align="left">Depends on the implementation</td>
<td align="left">More details in the <a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch">next section</a></td>
</tr>
<tr>
<td align="left"><strong>Console Sink</strong></td>
<td align="left">Append, Update, Complete</td>
<td align="left"><code>numRows</code>: Number of rows to print every trigger (default: 20) <code>truncate</code>: Whether to truncate the output if too long (default: true)</td>
<td align="left">No</td>
<td align="left"></td>
</tr>
<tr>
<td align="left"><strong>Memory Sink</strong></td>
<td align="left">Append, Complete</td>
<td align="left">None</td>
<td align="left">No. But in Complete Mode, restarted query will recreate the full table.</td>
<td align="left">Table name is the query name.</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody></table>
<h3 id="Triggers-触发器"><a href="#Triggers-触发器" class="headerlink" title="Triggers 触发器"></a>Triggers 触发器</h3><table>
<thead>
<tr>
<th><strong>Fixed interval micro-batches</strong></th>
<th>‎查询将以微批处理模式执行，其中微批处理将按用户指定的时间间隔启动。‎‎如果前一个微批处理在间隔内完成，则引擎将等到间隔结束，然后再启动下一个微批次。‎‎如果前一个微批处理花费的时间超过完成间隔的时间（即，如果错过了间隔边界），则下一个微批处理将在前一个微批处理完成后立即启动（即，它不会等待下一个间隔边界）。‎‎如果没有新数据可用，则不会启动任何微批处理。‎</th>
</tr>
</thead>
<tbody><tr>
<td><strong>One-time micro-batch</strong></td>
<td>‎查询将仅执行‎<strong>‎一个‎</strong>‎微批处理来处理所有可用数据，然后自行停止。这在您希望定期启动群集、处理自上一个周期以来可用的所有内容，然后关闭群集的情况下非常有用。在某些情况下，这可能会节省大量成本。‎</td>
</tr>
<tr>
<td><strong>Continuous with fixed checkpoint interval</strong> <em>(experimental)</em></td>
<td>‎查询将在新的低延迟连续处理模式下执行。在下面的‎<a target="_blank" rel="noopener" href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#continuous-processing">‎”连续处理”部分中‎</a>‎阅读有关此内容的更多信息。‎</td>
</tr>
</tbody></table>
<h1 id="sparkstreaming"><a href="#sparkstreaming" class="headerlink" title="sparkstreaming"></a>sparkstreaming</h1></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://sluping.gitee.io/luping">luping</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://sluping.gitee.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/">https://sluping.gitee.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/luping/tags/spark/">spark</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=null" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/luping/2021/10/02/spark%20job%E7%9A%84%E5%87%A0%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"><img class="prev-cover" src="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&amp;riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&amp;ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1" onerror="onerror=null;src='/luping/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">spark job的几种提交流程</div></div></a></div><div class="next-post pull-right"><a href="/luping/2021/04/02/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"><img class="next-cover" src="https://tse1-mm.cn.bing.net/th/id/OIP-C.kwcwQcrglWgnKVXlEww87gHaHa?pid=ImgDet&amp;rs=1" onerror="onerror=null;src='/luping/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Hadoop学习记录</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/luping/2021/10/02/spark%20job%E7%9A%84%E5%87%A0%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/" title="spark job的几种提交流程"><img class="cover" src="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-01</div><div class="title">spark job的几种提交流程</div></div></a></div><div><a href="/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/" title="spark将数据加载到hbase--bulkload方式"><img class="cover" src="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&risl=&pid=ImgRaw&r=0&sres=1&sresct=1" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-22</div><div class="title">spark将数据加载到hbase--bulkload方式</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://tse1-mm.cn.bing.net/th/id/R-C.50a11e325200050a0094a0bc5e302cf8?rik=%2bxttpKGOrVrr5w&amp;riu=http%3a%2f%2fwww.gx8899.com%2fuploads%2fallimg%2f2017110610%2fogkgrubpb00.jpg&amp;ehk=GRkc8RLcFB9TWPLsMsgxie8Z%2fuHG8VS1eT%2f2%2bPUR8Nc%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1" onerror="this.onerror=null;this.src='/luping/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">luping</div><div class="author-info__description">there are some notes of luping</div></div><div class="card-info-data site-data is-center"><a href="/luping/archives/"><div class="headline">Articles</div><div class="length-num">12</div></a><a href="/luping/tags/"><div class="headline">Tags</div><div class="length-num">10</div></a><a href="/luping/categories/"><div class="headline">Categories</div><div class="length-num">4</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/s-luping" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:461093788@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to my Blog!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">spark介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark%E4%B8%8EHadoop%E5%B7%AE%E5%BC%82"><span class="toc-number">1.1.</span> <span class="toc-text">Spark与Hadoop差异</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E5%AE%89%E8%A3%85"><span class="toc-number">2.</span> <span class="toc-text">spark安装</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E8%A7%A3%E5%8E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">下载解压</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%BB%E5%8A%A0%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-number">2.2.</span> <span class="toc-text">添加系统环境变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-shell"><span class="toc-number">2.3.</span> <span class="toc-text">spark-shell</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E4%BB%BB%E5%8A%A1%E6%8F%90%E4%BA%A4%E6%89%A7%E8%A1%8C"><span class="toc-number">3.</span> <span class="toc-text">spark任务提交执行</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#standalone-spark%E8%87%AA%E4%B8%BB%E7%AE%A1%E7%90%86%E7%9A%84%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F"><span class="toc-number">3.1.</span> <span class="toc-text">standalone spark自主管理的集群模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-on-yarn-%E6%8F%90%E4%BA%A4%E5%88%B0hadoop%E7%9A%84yarn%E9%9B%86%E7%BE%A4%E6%89%A7%E8%A1%8C"><span class="toc-number">3.2.</span> <span class="toc-text">spark on yarn 提交到hadoop的yarn集群执行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E7%9A%84%E6%A8%A1%E5%9D%97"><span class="toc-number">4.</span> <span class="toc-text">spark的模块</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-Core"><span class="toc-number">4.1.</span> <span class="toc-text">spark Core</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E6%93%8D%E4%BD%9C%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.1.</span> <span class="toc-text">RDD操作函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DAG-stage-%E5%88%92%E5%88%86%E4%BE%9D%E6%8D%AE"><span class="toc-number">4.1.2.</span> <span class="toc-text">DAG stage 划分依据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#shuffle%E4%BC%98%E5%8C%96"><span class="toc-number">4.1.3.</span> <span class="toc-text">shuffle优化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90"><span class="toc-number">5.</span> <span class="toc-text">spark常用算子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90-Transformations"><span class="toc-number">5.1.</span> <span class="toc-text">转换算子(Transformations)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90-Actions"><span class="toc-number">5.2.</span> <span class="toc-text">行动算子(Actions)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD%E7%BC%93%E5%AD%98"><span class="toc-number">5.3.</span> <span class="toc-text">RDD缓存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-number">5.4.</span> <span class="toc-text">广播变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-number">5.5.</span> <span class="toc-text">累加器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#spark-SQL"><span class="toc-number">6.</span> <span class="toc-text">spark SQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#UDF%E5%92%8CUDAF"><span class="toc-number">6.1.</span> <span class="toc-text">UDF和UDAF</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataSources"><span class="toc-number">6.2.</span> <span class="toc-text">DataSources</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#load-x2F-save%E5%87%BD%E6%95%B0"><span class="toc-number">6.2.1.</span> <span class="toc-text">load &#x2F;save函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Save-Modes"><span class="toc-number">6.2.2.</span> <span class="toc-text">Save Modes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JSON-file"><span class="toc-number">6.2.3.</span> <span class="toc-text">JSON file</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hive-%E8%A1%A8"><span class="toc-number">6.2.4.</span> <span class="toc-text">hive 表</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#JDBC-TO-OTHER-Databases"><span class="toc-number">6.2.5.</span> <span class="toc-text">JDBC TO OTHER Databases</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98"><span class="toc-number">6.3.</span> <span class="toc-text">性能调优</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%93%E5%AD%98%E5%86%85%E5%AD%98%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E2%80%8E"><span class="toc-number">6.4.</span> <span class="toc-text">缓存内存中的数据‎</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%80%8E%E5%85%B6%E4%BB%96%E9%85%8D%E7%BD%AE%E9%80%89%E9%A1%B9%E2%80%8E"><span class="toc-number">6.5.</span> <span class="toc-text">‎其他配置选项‎</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SQL-%E6%9F%A5%E8%AF%A2%E7%9A%84%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="toc-number">6.6.</span> <span class="toc-text">SQL 查询的使用广播变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8E-Apache-Hive-%E7%9A%84%E5%85%BC%E5%AE%B9%E6%80%A7"><span class="toc-number">6.7.</span> <span class="toc-text">与 Apache Hive 的兼容性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E7%9A%84%E9%85%8D%E7%BD%AE%E5%8D%95%E5%85%83%E5%8A%9F%E8%83%BD"><span class="toc-number">6.7.1.</span> <span class="toc-text">支持的配置单元功能</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#structure-treaming"><span class="toc-number">6.8.</span> <span class="toc-text">structure treaming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Output-Sinks"><span class="toc-number">6.8.1.</span> <span class="toc-text">Output Sinks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Triggers-%E8%A7%A6%E5%8F%91%E5%99%A8"><span class="toc-number">6.8.2.</span> <span class="toc-text">Triggers 触发器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sparkstreaming"><span class="toc-number">7.</span> <span class="toc-text">sparkstreaming</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/luping/2022/02/11/vue%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" title="vue学习记录"><img src="https://img-hello-world.oss-cn-beijing.aliyuncs.com/imgs/353233a188d5164d6252d9547c6120d8.jpg" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="vue学习记录"/></a><div class="content"><a class="title" href="/luping/2022/02/11/vue%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" title="vue学习记录">vue学习记录</a><time datetime="2022-02-10T20:43:38.000Z" title="Created 2022-02-10 20:43:38">2022-02-10</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/" title="spark将数据加载到hbase--bulkload方式"><img src="https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&amp;riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&amp;ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="spark将数据加载到hbase--bulkload方式"/></a><div class="content"><a class="title" href="/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/" title="spark将数据加载到hbase--bulkload方式">spark将数据加载到hbase--bulkload方式</a><time datetime="2021-11-22T21:30:25.000Z" title="Created 2021-11-22 21:30:25">2021-11-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/luping/2021/11/02/sudo%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E5%91%BD%E4%BB%A4/" title="ubuntu sudo执行shell脚本环境变量失效"><img src="https://tse1-mm.cn.bing.net/th/id/R-C.4e264b4b01124b6c8e1db08158f3ad2c?rik=pPWPcA6A%2fM2iEA&amp;riu=http%3a%2f%2fsucai.laixuexi.cc%3a88%2fimg2019%2fsoft%2f2018%2f0116%2f125837_61632181.jpg&amp;ehk=72tDFIPppBR0JpkunQGY%2bw7caz%2fbxck5AAkYkOQYKms%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="ubuntu sudo执行shell脚本环境变量失效"/></a><div class="content"><a class="title" href="/luping/2021/11/02/sudo%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E5%91%BD%E4%BB%A4/" title="ubuntu sudo执行shell脚本环境变量失效">ubuntu sudo执行shell脚本环境变量失效</a><time datetime="2021-11-01T20:43:38.000Z" title="Created 2021-11-01 20:43:38">2021-11-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/luping/2021/10/27/Hive%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="Hive网站日志采集统计分析"><img src="https://tse4-mm.cn.bing.net/th/id/OIP-C.uRqW4OzIZXQTJoE0CTBE0gHaFj?pid=ImgDet&amp;rs=1" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="Hive网站日志采集统计分析"/></a><div class="content"><a class="title" href="/luping/2021/10/27/Hive%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/" title="Hive网站日志采集统计分析">Hive网站日志采集统计分析</a><time datetime="2021-10-26T22:12:06.000Z" title="Created 2021-10-26 22:12:06">2021-10-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/luping/2021/10/11/Hive%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E2%BD%87%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" title="Hive新浪微博日志查询分析"><img src="https://tse4-mm.cn.bing.net/th/id/OIP-C.uRqW4OzIZXQTJoE0CTBE0gHaFj?pid=ImgDet&amp;rs=1" onerror="this.onerror=null;this.src='/luping/img/404.jpg'" alt="Hive新浪微博日志查询分析"/></a><div class="content"><a class="title" href="/luping/2021/10/11/Hive%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E2%BD%87%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/" title="Hive新浪微博日志查询分析">Hive新浪微博日志查询分析</a><time datetime="2021-10-10T22:12:06.000Z" title="Created 2021-10-10 22:12:06">2021-10-10</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://tse1-mm.cn.bing.net/th/id/R-C.84a3ce741c765f6196daeca22617b27e?rik=LGfNMDZpJDig5Q&amp;riu=http%3a%2f%2fwww.ibm.com%2fanalytics%2fcommon%2fimg%2fopen-source%2fspark-icon.png&amp;ehk=w7HbEPKmvRduv92QFc%2bexmD9bOhFmgSKlUEznw2UFgk%3d&amp;risl=&amp;pid=ImgRaw&amp;r=0&amp;sres=1&amp;sresct=1')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By luping</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/luping/js/utils.js"></script><script src="/luping/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><div><a target="_blank" href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&email=u8jXzsvS1dz73dTD1trS15XY1NY" style="text-decoration:none;right:0"> <img src="http://rescdn.qqmail.com/zh_CN/htmledition/images/function/qm_open/ico_mailme_02.png"/> </a></div><canvas class="fireworks" mobile="false"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>