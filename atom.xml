<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lu Ping&#39;s blog</title>
  
  <subtitle>Welcome to my blog</subtitle>
  <link href="https://s-luping.github.io/luping/atom.xml" rel="self"/>
  
  <link href="https://s-luping.github.io/luping/"/>
  <updated>2022-03-13T12:28:56.437Z</updated>
  <id>https://s-luping.github.io/luping/</id>
  
  <author>
    <name>luping</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>vue学习记录</title>
    <link href="https://s-luping.github.io/luping/2022/02/11/vue%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://s-luping.github.io/luping/2022/02/11/vue%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2022-02-10T20:43:38.000Z</published>
    <updated>2022-03-13T12:28:56.437Z</updated>
    
    <content type="html"><![CDATA[<h1 id="windows开发环境"><a href="#windows开发环境" class="headerlink" title="windows开发环境"></a>windows开发环境</h1><h2 id="nodejs环境安装"><a href="#nodejs环境安装" class="headerlink" title="nodejs环境安装"></a>nodejs环境安装</h2><p>安装时将 nodejs和npm添加到系统环境变量<br> 测试是否安装成功：nodejs里面会安装npm指令，显示版本号安装成功。<br><img src="https://img-blog.csdnimg.cn/1edf6d3c931c4483b67f2f418715f899.png" alt="在这里插入图片描述"></p><h2 id="配置nodejs-prefix（全局）和cache（缓存）路径"><a href="#配置nodejs-prefix（全局）和cache（缓存）路径" class="headerlink" title="配置nodejs prefix（全局）和cache（缓存）路径"></a>配置nodejs prefix（全局）和cache（缓存）路径</h2><p>先找到nodejs的安装目录（E:\nodejs）<br><strong>在nodejs安装路径下，新建global和cache两个文件夹</strong><br>命令进行修改设置</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm config <span class="built_in">set</span> cache <span class="string">&quot;E:\nodejs\cache&quot;</span></span><br><span class="line">npm config <span class="built_in">set</span> prefix <span class="string">&quot;E:\nodejs\global&quot;</span></span><br></pre></td></tr></table></figure><h2 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h2><p>添加node_global HOME<br><img src="https://img-blog.csdnimg.cn/0a0276ef45ea42bfa0b08126d457abcd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>添加到path<br><img src="https://img-blog.csdnimg.cn/4e82b88a358e47ca94afeb838edad825.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>安装webpack、vue-cli脚手架构建工具</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install webpack <span class="literal">-g</span></span><br><span class="line">npm install vue<span class="literal">-cli</span> <span class="literal">-g</span></span><br></pre></td></tr></table></figure><p>查看安装是否成功<br><img src="https://img-blog.csdnimg.cn/2a0bf21f741b4f08916d17a621a9cbdb.png" alt="在这里插入图片描述"></p><h1 id="创建一个vue项目"><a href="#创建一个vue项目" class="headerlink" title="创建一个vue项目"></a>创建一个vue项目</h1><p><strong>在dos命令行</strong><br><strong>在硬盘上找一个文件夹放工程用的： cd 目录路径</strong><br>安装vue脚手架输入：vue init webpack demo，注意这里的“demo” 是项目的名称可以说是随便的起名，但是需要主要的是“不能用中文”。<br>然后根据需要选择选项</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br><span class="line">npm run dev </span><br></pre></td></tr></table></figure><h1 id="vue目录结构"><a href="#vue目录结构" class="headerlink" title="vue目录结构"></a>vue目录结构</h1><p><img src="https://img-blog.csdnimg.cn/2375db529a1447699e468a922f435b32.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="build"><a href="#build" class="headerlink" title="build"></a>build</h3><p><img src="https://img-blog.csdnimg.cn/37ee7e4d8b164afa9de4805999086b19.png" alt="在这里插入图片描述"></p><h3 id="config"><a href="#config" class="headerlink" title="config"></a>config</h3><p><img src="https://img-blog.csdnimg.cn/688b8e7d0da5484e9a69a5707d57b9cc.png" alt="在这里插入图片描述"></p><h1 id="项目练习"><a href="#项目练习" class="headerlink" title="项目练习"></a>项目练习</h1><p>vue整合elementUI</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i element<span class="literal">-ui</span> <span class="literal">-S</span></span><br></pre></td></tr></table></figure><p>main.js 添加<br>完整引入</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 完整引入 Element</span><br><span class="line">import Vue from <span class="string">&#x27;vue&#x27;</span></span><br><span class="line">import ElementUI from <span class="string">&#x27;element-ui&#x27;</span></span><br><span class="line">import locale from <span class="string">&#x27;element-ui/lib/locale/lang/en&#x27;</span></span><br><span class="line"></span><br><span class="line">Vue.use(ElementUI, &#123; locale &#125;)</span><br></pre></td></tr></table></figure><h1 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h1><h2 id="vue-axios-跨域访问"><a href="#vue-axios-跨域访问" class="headerlink" title="vue-axios 跨域访问"></a>vue-axios 跨域访问</h2><p>问题描述当前后端分离项目调试时，前端项目需要访问后端接口，使用axios请求时会出现跨域访问问题，<br>解决 ： 在config目录下修改index.js文件 如下部分<br><img src="https://img-blog.csdnimg.cn/1a92f7701a044160abe98c2b3c1ea781.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>使用vue的代理访问目标资源，然后改为自己的格式</p>]]></content>
    
    
    <summary type="html">vue学习记录</summary>
    
    
    
    <category term="web" scheme="https://s-luping.github.io/luping/categories/web/"/>
    
    
    <category term="vue" scheme="https://s-luping.github.io/luping/tags/vue/"/>
    
  </entry>
  
  <entry>
    <title>spark将数据加载到hbase--bulkload方式</title>
    <link href="https://s-luping.github.io/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/"/>
    <id>https://s-luping.github.io/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/</id>
    <published>2021-11-22T21:30:25.000Z</published>
    <updated>2022-03-13T12:27:51.224Z</updated>
    
    <content type="html"><![CDATA[<p><strong>通过bulkload方式加载数据优点：与put方式相比</strong><br>1.导入过程不占用Region资源<br>2.能快速导入海量的数据<br>3.节省内存<br>应该是业界将数据载入hbase常用方式之一，因此有必要学习掌握</p><h1 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h1><h2 id="步骤一-读取数据生成rdd"><a href="#步骤一-读取数据生成rdd" class="headerlink" title="步骤一  读取数据生成rdd"></a>步骤一  读取数据生成rdd</h2><p>读入数据是面向行的表，一行有多个字段，需要转换成面向列的数据，构造<strong>keyValue</strong>对象，一定要注意<strong>key们要排序</strong>，比如<em>user:age</em>列要在<em>user:gender</em>列之前<br>需要设计行键保证行键唯一和避免数据都涌入一个region，如我的是按时间设计的，好几个月的数据，因此将数据按月预分区。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;file:///&quot;</span>+filePath)</span><br><span class="line">    .flatMap(x=&gt;getLineData(x,rowKeyBase,<span class="type">HBaseUtils</span>.<span class="type">LOG_FIELD_NAMES</span>))</span><br><span class="line">    .sortByKey()</span><br><span class="line"><span class="comment">//处理每一条记录生成keyvalue对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLineData</span></span>(line:<span class="type">String</span>,rowkey:<span class="type">String</span>,fieldNames: <span class="type">TreeMap</span>[<span class="type">String</span>, <span class="type">Int</span>]): <span class="type">List</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">KeyValue</span>)] =&#123;</span><br><span class="line">  <span class="keyword">val</span> length = fieldNames.size</span><br><span class="line">  <span class="keyword">val</span> values:<span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;\\\t&quot;</span>)</span><br><span class="line">  <span class="keyword">if</span> (<span class="literal">null</span> == values || values.length!=length) <span class="keyword">return</span> <span class="type">Nil</span></span><br><span class="line">  <span class="comment">//println(rowkey+values(1)+Random.nextInt(100000).toString)</span></span><br><span class="line">  <span class="keyword">val</span> rowKey = <span class="type">Bytes</span>.toBytes(rowkey+values(<span class="number">1</span>)+<span class="type">Random</span>.nextInt(<span class="number">1000</span>).toString)</span><br><span class="line">  <span class="keyword">val</span> writable = <span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>(rowKey)</span><br><span class="line">  <span class="keyword">val</span> columnFamily = <span class="type">Bytes</span>.toBytes(<span class="string">&quot;detail&quot;</span>)</span><br><span class="line">  fieldNames.toList.map&#123;</span><br><span class="line">    <span class="keyword">case</span> (fieldName, fieldIndex) =&gt;</span><br><span class="line">      <span class="comment">// KeyValue实例对象</span></span><br><span class="line">      <span class="keyword">val</span> keyValue = <span class="keyword">new</span> <span class="type">KeyValue</span>(</span><br><span class="line">        rowKey, <span class="comment">//</span></span><br><span class="line">        columnFamily, <span class="comment">//</span></span><br><span class="line">        <span class="type">Bytes</span>.toBytes(fieldName), <span class="comment">//</span></span><br><span class="line">        <span class="type">Bytes</span>.toBytes(values(fieldIndex)) <span class="comment">//</span></span><br><span class="line">      )</span><br><span class="line">      <span class="comment">// 返回</span></span><br><span class="line">      (writable, keyValue)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="步骤二-配置输出HFile文件"><a href="#步骤二-配置输出HFile文件" class="headerlink" title="步骤二  配置输出HFile文件"></a>步骤二  配置输出HFile文件</h2><h3 id="输出前检查"><a href="#输出前检查" class="headerlink" title="输出前检查"></a>输出前检查</h3><h4 id="检查HFile输出目录是否存在"><a href="#检查HFile输出目录是否存在" class="headerlink" title="检查HFile输出目录是否存在"></a>检查HFile输出目录是否存在</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO：构建Job，设置相关配置信息，主要为输出格式</span></span><br><span class="line"><span class="comment">// a. 读取配置信息</span></span><br><span class="line"><span class="keyword">val</span> hbaseConfig: <span class="type">Configuration</span> = <span class="type">HBaseUtils</span>.getHBaseConfiguration(<span class="string">&quot;hbase&quot;</span>,<span class="string">&quot;2181&quot;</span>)</span><br><span class="line"><span class="comment">//  Configuration parameter hbase.mapreduce.hfileoutputformat.table.name cannot be empty</span></span><br><span class="line">hbaseConfig.set(<span class="string">&quot;hbase.mapreduce.hfileoutputformat.table.name&quot;</span>, <span class="string">&quot;log&quot;</span>)</span><br><span class="line"><span class="comment">// b. 如果输出目录存在，删除</span></span><br><span class="line"><span class="keyword">val</span> dfs = <span class="type">FileSystem</span>.get(hbaseConfig)</span><br><span class="line"><span class="keyword">val</span> outputPath: <span class="type">Path</span> = <span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;hdfs://hbase:9000/hbase/log/&quot;</span>+rowKeyBase)</span><br><span class="line"><span class="keyword">if</span> (dfs.exists(outputPath)) &#123;</span><br><span class="line">  dfs.delete(outputPath, <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br><span class="line">dfs.close()</span><br></pre></td></tr></table></figure><h4 id="配置HFileOutputFormat2"><a href="#配置HFileOutputFormat2" class="headerlink" title="配置HFileOutputFormat2"></a>配置HFileOutputFormat2</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO： 配置HFileOutputFormat2输出</span></span><br><span class="line"><span class="keyword">val</span> conn = <span class="type">ConnectionFactory</span>.createConnection(hbaseConfig)</span><br><span class="line"><span class="keyword">val</span> htableName = <span class="type">TableName</span>.valueOf(<span class="string">&quot;log&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(htableName)</span><br><span class="line"><span class="type">HFileOutputFormat2</span>.configureIncrementalLoad(</span><br><span class="line">  <span class="type">Job</span>.getInstance(hbaseConfig), <span class="comment">//</span></span><br><span class="line">  table, <span class="comment">//</span></span><br><span class="line">  conn.getRegionLocator(htableName) <span class="comment">//</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="输出HFile文件"><a href="#输出HFile文件" class="headerlink" title="输出HFile文件"></a>输出HFile文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO： 3. 保存数据为HFile文件//先排序</span></span><br><span class="line">rdd.sortBy(x=&gt;(x._1, x._2.getKeyString), ascending = <span class="literal">true</span>)</span><br><span class="line">  .saveAsNewAPIHadoopFile(</span><br><span class="line">    <span class="string">&quot;hdfs://hbase:9000/hbase/log/&quot;</span>+rowKeyBase,</span><br><span class="line">    classOf[<span class="type">ImmutableBytesWritable</span>], <span class="comment">//</span></span><br><span class="line">    classOf[<span class="type">KeyValue</span>], <span class="comment">//</span></span><br><span class="line">    classOf[<span class="type">HFileOutputFormat2</span>], <span class="comment">//</span></span><br><span class="line">    hbaseConfig)</span><br></pre></td></tr></table></figure><h2 id="将HFile文件bulkload到hbase表分区当中"><a href="#将HFile文件bulkload到hbase表分区当中" class="headerlink" title="将HFile文件bulkload到hbase表分区当中"></a>将HFile文件bulkload到hbase表分区当中</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO：4. 将输出HFile加载到HBase表中</span></span><br><span class="line"><span class="keyword">val</span> load = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(hbaseConfig)</span><br><span class="line">load.doBulkLoad(outputPath, conn.getAdmin, table,</span><br><span class="line">  conn.getRegionLocator(htableName))</span><br></pre></td></tr></table></figure><h1 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h1><p>写入权限<br>可以将HFile要输出的文件位置chmod 777 &#x2F;outputDir</p>]]></content>
    
    
    <summary type="html">spark将数据加载到hbase--bulkload方式</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="spark" scheme="https://s-luping.github.io/luping/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu sudo执行shell脚本环境变量失效</title>
    <link href="https://s-luping.github.io/luping/2021/11/02/sudo%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E5%91%BD%E4%BB%A4/"/>
    <id>https://s-luping.github.io/luping/2021/11/02/sudo%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E5%91%BD%E4%BB%A4/</id>
    <published>2021-11-01T20:43:38.000Z</published>
    <updated>2022-03-13T12:28:56.428Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p><strong>普通用户下，设置并export一个变量，然后利用sudo执行echo命令，能得到变量的值，但是如果把echo命令写入脚本，然后再sudo执行脚本，就找不到变量，未能获取到值，如题情况如下：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> tesh.sh </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$var</span> </span><br><span class="line">$ var=aaa </span><br><span class="line">$ <span class="built_in">export</span> var                       <span class="comment"># export 变量 </span></span><br><span class="line">$ sudo <span class="built_in">echo</span> <span class="variable">$var</span>                   <span class="comment"># sudo执行echo命令，返回变量值 </span></span><br><span class="line">aaa </span><br><span class="line">$ sudo bash test.sh                <span class="comment"># sudo执行脚本，不能获取变量值 </span></span><br><span class="line"></span><br><span class="line">$ bash test.sh                     <span class="comment"># 普通用户执行脚本，返回变量值 </span></span><br></pre></td></tr></table></figure><h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p><strong>sudo运行时，会默认重置环境变量为安全的环境变量，也即，但前面设置的变量都会失效，只有少数配置文件中指定的环境变量能保存下来。</strong></p><p><strong>sudo的配置文件是 &#x2F;etc&#x2F;sudoers 需要root权限才能读取:</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sed ‘/^<span class="comment">#/d;/^$/d’ /etc/sudoers</span></span><br><span class="line"></span><br><span class="line">Defaults env_reset </span><br><span class="line">Defaults mail_badpass </span><br><span class="line">Defaults secure_path=”/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin” </span><br><span class="line">root ALL=(ALL:ALL) ALL </span><br><span class="line">%sudo ALL=(ALL:ALL) ALL </span><br><span class="line">xxx ALL=(ALL:ALL) NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p><strong>不过可以直接通过sudo -l来查看sudo的限制：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -l</span><br><span class="line"></span><br><span class="line">Matching Defaults entries <span class="keyword">for</span> xxx on this host: </span><br><span class="line">env_reset, mail_badpass,</span><br><span class="line">secure_path=/usr/local/sbin\:/usr/local/bin\:/usr/sbin\:/usr/bin\:/sbin\:/bin </span><br><span class="line"></span><br><span class="line">User xxx may run the following commands on this host:</span><br><span class="line"> (ALL : ALL) NOPASSWD: ALL</span><br></pre></td></tr></table></figure><p><strong>注意看第一行的选项 Defaults env_reset 表示默认会将环境变量重置，这样你定义的变量在sudo环境就会失效，获取不到。<br>另外有的发行版还有一个Defaults env_keep&#x3D;”“的选项，用于保留部分环境变量不被重置，需要保留的变量就写入双引号中。</strong></p><p>为什么sudo echo $var能获取到变量值？<br>既然利用sudo执行会重置环境变量，那么为什么还能echo获取到相应的变量呢？<br><strong>这是由于shell命令行的替换&amp;重组功能，在输入命令，按下回车时，shell会先依据分隔符将命令行切割成字段，对每个字段查找有没有变量或命令替换，再替换完成后，重组成新的命令，再去执行。<br>所以，命令实际执行是：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo <span class="built_in">echo</span> <span class="variable">$var</span>                   <span class="comment"># $var =&gt; aaa </span></span><br><span class="line">(sudo <span class="built_in">echo</span> aaa)                    <span class="comment"># 完成命令替换&amp;重组 </span></span><br><span class="line">(<span class="built_in">echo</span> aaa)                         <span class="comment"># sudo环境中执行 </span></span><br><span class="line">aaa</span><br></pre></td></tr></table></figure><p>因此，sudo环境重置后，并不用去引用$var这个变量，而是直接echo aaa。</p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p><strong>sudo -E<br>简单来说，就是加上-E选项后，用户可以在sudo执行时保留当前用户已存在的环境变量，不会被sudo重置，另外，如果用户对于指定的环境变量没有权限，则会报错。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -E bash test.sh <span class="comment"># 加上-E参数后就可以获取到变量 aaa</span></span><br></pre></td></tr></table></figure><p><strong>2. 修改sudo配置文件</strong><br>在内部测试机器中，安全性要求不高，总是需要加上-E参数来执行脚本，这个安全设定也不是很方便，可以通过visudo命令来修改配置为保留原有的环境变量，具体修改如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$sudo</span> visudo </span><br><span class="line"><span class="comment"># Defaults env_reset                  # 注释掉原有配置 </span></span><br><span class="line"><span class="comment"># Defaults env_keep=”…”               # 注释掉指定的变量保持</span></span><br></pre></td></tr></table></figure><p>Defaults !env_reset # 修改为不重置环境<br><strong>3. 手动添加变量</strong><br>手动在脚本中设置所需的变量，这样看起来比较麻烦，或者在执行sudo脚本前先将所需要的变量写入到要执行的脚本开头.</p><p><em>命令<br>对于自己安装的软件在sudo提示找不到的命令（没加sudo可以找到），在这个后面添加命令所在的路径</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">secure_path=/usr/local/sbin\:/usr/local/bin\:/usr/sbin\:/usr/bin\:/sbin\:/bin </span><br><span class="line">1</span><br><span class="line">secure_path=/usr/local/sbin\:/usr/local/bin\:/usr/sbin\:/usr/bin\:/sbin\:/bin:自定义路径 </span><br></pre></td></tr></table></figure><h1 id="visudo编辑模式和退出"><a href="#visudo编辑模式和退出" class="headerlink" title="visudo编辑模式和退出"></a>visudo编辑模式和退出</h1><p><strong>执行</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$sudo</span> visudo</span><br></pre></td></tr></table></figure><p>找到如下授权，注释</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%admin ALL=(ALL) ALL</span><br></pre></td></tr></table></figure><p>修改为新的授权，意思是不需要密码执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%admin ALL=(ALL) NOPASSWD: ALL</span><br></pre></td></tr></table></figure><p><strong>保存并退出：</strong><br><em>保存</em><br>执行“Ctrl+O”*</p>]]></content>
    
    
    <summary type="html">spark将数据加载到hbase--bulkload方式</summary>
    
    
    
    <category term="linux" scheme="https://s-luping.github.io/luping/categories/linux/"/>
    
    
    <category term="ubuntu shell" scheme="https://s-luping.github.io/luping/tags/ubuntu-shell/"/>
    
  </entry>
  
  <entry>
    <title>Hive网站日志采集统计分析</title>
    <link href="https://s-luping.github.io/luping/2021/10/27/Hive%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/"/>
    <id>https://s-luping.github.io/luping/2021/10/27/Hive%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/</id>
    <published>2021-10-26T22:12:06.000Z</published>
    <updated>2022-03-13T12:15:32.341Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本次实践的目的是结合之前所学flume、hadoop、hive几个主要技术，完成一个小案例。<br>目标：<br>统计出独立ip数量<br>统计一个ip使用的header数量<br>访问最多的url链接 每个ip常访问的url<br>单日每小时访问量折线图</p><h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><p><strong>安装并配置好flume、hadoop、hive</strong><br><a href="https://blog.csdn.net/kun666666/article/details/121311179">hive安装配置</a><br><a href="https://blog.csdn.net/kun666666/article/details/120390257">hadoop安装配置</a><br><strong>数据源</strong> nginx日志文件access.log<br><img src="https://img-blog.csdnimg.cn/9ab8ab2468954e369d3d4e7457f6e20d.png" alt="access.log"><br>需修改nginx日志输出格式<br>满足如下格式，可减免数据清洗步骤</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.88.122.105  09:15:04  GET /js/pagination.js HTTP/1.1</span><br><span class="line">304 0 <span class="string">&quot;http://10.88.105.20:8063/stockrecommand.html&quot;</span> <span class="string">&quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/7.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)&quot;</span> </span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/4298f69df2ed48b39a38602976775269.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>准备一台服务器运行web服务，使用nginx做代理，使用flume采集nginx生产的日志上传至hdfs；</p><h2 id="修改hdfs默认配置"><a href="#修改hdfs默认配置" class="headerlink" title="修改hdfs默认配置"></a>修改hdfs默认配置</h2><p>因为我的网站每天只产生很少的数据量远小于hdfs默认的（128M）块大小因此为避免空间浪费需修改默认块大小</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#修改默认块大小</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10240000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">propery</span>&gt;</span></span><br><span class="line">#修改检查块大小 满足块大小整除检查块大小</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.bytes-per-checksum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="yran资源管理器配置"><a href="#yran资源管理器配置" class="headerlink" title="yran资源管理器配置"></a>yran资源管理器配置</h2><p>在用hive操作时使用的内存会大于默认分配的资源因此需修改</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#使用物理内存大小</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="flume采集方案"><a href="#flume采集方案" class="headerlink" title="flume采集方案"></a>flume采集方案</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers/flume/conf/</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources = source1</span><br><span class="line">agent1.sinks = sink1</span><br><span class="line">agent1.channels = channel1</span><br><span class="line"></span><br><span class="line">agent1.sources.source1.type = exec</span><br><span class="line">agent1.sources.source1.command = tail -F /www/wwwlogs/139.198.168.168.log</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.hdfs.path =hdfs://master:9000/weblog/year=20%y/month=%m/day=%d</span><br><span class="line">agent1.sinks.sink1.hdfs.filePrefix = mylog</span><br><span class="line">agent1.sinks.sink1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.sink1.hdfs.writeFormat = Text</span><br><span class="line"># 当文件滚动  生成新文件</span><br><span class="line"># 配置文件滚动方式（文件大小10M） #8M</span><br><span class="line">agent1.sinks.sink1.hdfs.rollSize = 8000000</span><br><span class="line">agent1.sinks.sink1.hdfs.rollCount = 0</span><br><span class="line">agent1.sinks.sink1.hdfs.rollInterval = 0</span><br><span class="line"># 指的是正在写的hdfs文件多长时间不更新就关闭文件</span><br><span class="line">agent1.sinks.sink1.hdfs.idleTimeout = 5</span><br><span class="line">agent1.sinks.sink1.hdfs.minBlockReplicas = 1</span><br><span class="line"># 向hdfs上刷新的event的个数</span><br><span class="line"># 这三者之间的关系:batchsize &lt;=transactionCapacity&lt;=capacity</span><br><span class="line"># 就是sink会一次从channel中取多少个event去发送，而这个发送是要最终以事务的形式去发送的</span><br><span class="line">agent1.sinks.sink1.hdfs.batchSize = 10</span><br><span class="line"></span><br><span class="line"># 我们打算对时间戳根据分钟以每10分钟为单位进行四舍五入。</span><br><span class="line"># agent1.sinks.sink1.hdfs.round = true</span><br><span class="line"># agent1.sinks.sink1.hdfs.roundValue = 24</span><br><span class="line"># agent1.sinks.sink1.hdfs.roundUnit = hour</span><br><span class="line"> </span><br><span class="line">agent1.sinks.sink1.hdfs.useLocalTimeStamp = true</span><br><span class="line"> </span><br><span class="line">#使用通道在内存中缓冲事件</span><br><span class="line">agent1.channels.channel1.type = memory</span><br><span class="line">agent1.channels.channel1.keep-alive = 120</span><br><span class="line"># capacity是指整个队列的缓存最大容量</span><br><span class="line">agent1.channels.channel1.capacity = 1500</span><br><span class="line"># transactionCapacity则是指事务event的最大容量，即每次传输的event最大为多少</span><br><span class="line">agent1.channels.channel1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line">#将源和接收器绑定到通道</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line">agent1.sinks.sink1.channel = channel1</span><br><span class="line">agent1.sinks.sink1.type = hdfs</span><br></pre></td></tr></table></figure><p><strong>启动flume采集</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line"><span class="comment">#执行参数解析</span></span><br><span class="line"><span class="built_in">nohup</span> flume-ng agent --conf /export/servers/flume-1.9.0/conf --conf-file flume-spark-push.properties -name a1  &gt;&gt; /export/data/flume/loglisence.log &amp;</span><br><span class="line"><span class="comment">#1、flume-ng  agent   运行一个Flume Agent</span></span><br><span class="line"><span class="comment">#2、--conf  指定配置文件路径</span></span><br><span class="line"><span class="comment">#3、--conf-file 收集方案文件</span></span><br><span class="line"><span class="comment">#4、-name  a1  Agent的名称 即上面的前缀 agent1</span></span><br></pre></td></tr></table></figure><p>查看收集的记录<br><img src="https://img-blog.csdnimg.cn/aa299b92e81e414eb521638420c354d4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h1 id="HIVE操作"><a href="#HIVE操作" class="headerlink" title="HIVE操作"></a>HIVE操作</h1><p>首先在hive创建日志表，</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mlog(ip string,</span><br><span class="line">mtime string,</span><br><span class="line">url string,</span><br><span class="line">respCode <span class="type">int</span>,</span><br><span class="line">header string)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">year</span> string,<span class="keyword">month</span> string,<span class="keyword">day</span> string)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">location <span class="string">&#x27;mylog/&#x27;</span>;</span><br></pre></td></tr></table></figure><p>然后在hive shell commend repair一下mlog表 识别表分区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msck repair table mlog;</span><br></pre></td></tr></table></figure><p>将分区数据添加到metastore<br><img src="https://img-blog.csdnimg.cn/4ecb8ac70df74102a3d64b95c688e182.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="日志总行数"><a href="#日志总行数" class="headerlink" title="日志总行数"></a>日志总行数</h2><p>全部记录 5866479</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> log;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/f22834d76a25483285ae71f59c297207.png" alt="在这里插入图片描述"></p><h2 id="独立ip数量-2387"><a href="#独立ip数量-2387" class="headerlink" title="独立ip数量 2387"></a>独立ip数量 2387</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="keyword">distinct</span>(ip)) <span class="keyword">from</span> log;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/ef8dfdae43b84680ad6ec1e4e0511dec.png" alt="在这里插入图片描述"></p><h2 id="一个ip使用header数量"><a href="#一个ip使用header数量" class="headerlink" title="一个ip使用header数量"></a>一个ip使用header数量</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ip,<span class="built_in">count</span>(<span class="keyword">distinct</span>(header)) c  <span class="keyword">from</span> log <span class="keyword">group</span> <span class="keyword">by</span> ip <span class="keyword">order</span> <span class="keyword">by</span> c <span class="keyword">desc</span>；# limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/8a445506022a418387bce1e70f66dee7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="访问最多的url"><a href="#访问最多的url" class="headerlink" title="访问最多的url"></a>访问最多的url</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> url,<span class="built_in">count</span>(url) c <span class="keyword">from</span> log <span class="keyword">group</span> <span class="keyword">by</span> url <span class="keyword">order</span> <span class="keyword">by</span> c <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/cffea8a44fae471f9c4bc1acff954da0.png" alt="在这里插入图片描述"></p><h2 id="每个ip访问最多的url"><a href="#每个ip访问最多的url" class="headerlink" title="每个ip访问最多的url"></a>每个ip访问最多的url</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ip,url,<span class="built_in">count</span>(url) c <span class="keyword">from</span> log <span class="keyword">group</span> <span class="keyword">by</span> ip,url <span class="keyword">order</span> <span class="keyword">by</span> c <span class="keyword">desc</span>； #limit <span class="number">300</span>;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/b48d0ae4a5ed4963b51af1be391a59c7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="每小时访问量-指定分区"><a href="#每小时访问量-指定分区" class="headerlink" title="每小时访问量 指定分区"></a>每小时访问量 指定分区</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#不区分ip</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">substring</span>(mtime,<span class="number">0</span>,<span class="number">2</span>),<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> log <span class="keyword">where</span> <span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2021&#x27;</span> <span class="keyword">and</span> <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;11&#x27;</span> <span class="keyword">and</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;07&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="built_in">substring</span>(mtime,<span class="number">0</span>,<span class="number">2</span>) limit <span class="number">10</span>;</span><br><span class="line">#每小时独立ip访问量</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">substring</span>(mtime,<span class="number">0</span>,<span class="number">2</span>),<span class="built_in">count</span>(<span class="keyword">distinct</span>(ip)) <span class="keyword">from</span> log <span class="keyword">where</span> <span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2021&#x27;</span> <span class="keyword">and</span> <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;11&#x27;</span> <span class="keyword">and</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;07&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="built_in">substring</span>(mtime,<span class="number">0</span>,<span class="number">2</span>) limit <span class="number">10</span>;</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/d21a0df9ae9440a68e754c02e522bed2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/a81dce7881b645c09c0df8bb045cd95f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p><strong>编写shell脚本将每天的小时访问量数据导出到mysql</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive -e <span class="string">&quot;create table qph as select substring(mtime,0,2),count(distinct(ip)) from log where year=&#x27;2021&#x27; and month=&#x27;11&#x27; and day=&#x27;07&#x27; group by substring(mtime,0,2) limit 10;&quot;</span></span><br><span class="line"></span><br><span class="line">sqoop <span class="built_in">export</span> --connect jdbc:mysql://localhost:3306/loginfo</span><br><span class="line">--uername hadoop --password <span class="built_in">pwd</span> --table daily </span><br><span class="line">--fields-terminated-by <span class="string">&#x27;\001&#x27;</span> --export-dir <span class="string">&#x27;user/hive/warehouse/log/qph&#x27;</span></span><br><span class="line"></span><br><span class="line">hive -e <span class="string">&quot;drop table qph;&quot;</span></span><br></pre></td></tr></table></figure><h2 id="添加定时任务"><a href="#添加定时任务" class="headerlink" title="添加定时任务"></a>添加定时任务</h2><p>crontab -e<br>加入以下内容<br>00 00 * * * &#x2F;bin&#x2F;sh &#x2F;user&#x2F;local&#x2F;src&#x2F;mysh&#x2F;daily.sh<br>每天凌晨0点执行昨天的日志统计任务，储存到mysql。</p><h1 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h1><p>6.4样例<br><img src="https://img-blog.csdnimg.cn/20210426221101345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210426221124587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>7.总结<br>实现了分析网站访问日志，统计出关键的信息(浏览量、注册数量、独立IP访问数量、跳出率)，统计各模块的访问数量，一天内每个时刻的四项指标访问量。可供网站决策者，也就是我自己，分析数据做出对热度较高的版块加关注，对热度低的板块做优化等。<br>缺陷与不足是，将网站和后台数据处理一并部署在一台云服务器上了，采用的单机，性能限制很大，做MapReduce清洗时服务器负载较大，特别容易挂掉。<br>遇到的问题与困难是，各项指标的sql和hql语句使用并不熟悉，导致导出的可使用的数据并不多。</p>]]></content>
    
    
    <summary type="html">Hive网站日志采集统计分析</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="hive sql" scheme="https://s-luping.github.io/luping/tags/hive-sql/"/>
    
  </entry>
  
  <entry>
    <title>Hive新浪微博日志查询分析</title>
    <link href="https://s-luping.github.io/luping/2021/10/11/Hive%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E2%BD%87%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>https://s-luping.github.io/luping/2021/10/11/Hive%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E2%BD%87%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</id>
    <published>2021-10-10T22:12:06.000Z</published>
    <updated>2022-03-13T12:47:45.767Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;beCommentWeiboId&quot;:&quot;&quot;,&quot;beForwardWeiboId&quot;:&quot;&quot;,&quot;catchTime&quot;:&quot;1387159495&quot;,&quot;co mmentCount&quot;:&quot;1419&quot;,&quot;content&quot;:&quot;分享图</span><br><span class="line">片&quot;,&quot;createTime&quot;:&quot;1386981067&quot;,&quot;info1&quot;:&quot;&quot;,&quot;info2&quot;:&quot;&quot;,&quot;info3&quot;:&quot;&quot;,&quot;mlevel&quot;:&quot;&quot; ,&quot;musicurl&quot;:[],&quot;pic_list&quot;: [&quot;http://ww3.sinaimg.cn/thumbnail/40d61044jw1ebixhnsiknj20qo0qognx.jpg&quot;],&quot; praiseCount&quot;:&quot;5265&quot;,&quot;reportCount&quot;:&quot;1285&quot;,&quot;source&quot;:&quot;iPad客户</span><br><span class="line">端&quot;,&quot;userId&quot;:&quot;1087770692&quot;,&quot;videourl&quot;: [],&quot;weiboId&quot;:&quot;3655325888057474&quot;,&quot;weiboUrl&quot;:&quot;http://weibo.com/1087770692/An dhixO7g&quot;&#125;] 2 [&#123;&quot;beCommentWeiboId&quot;:&quot;&quot;,&quot;beForwardWeiboId&quot;:&quot;&quot;,&quot;catchTime&quot;:&quot;1387159495&quot;,&quot;co mmentCount&quot;:&quot;91&quot;,&quot;content&quot;:&quot;行走：#去远方发现自己#@费勇主编，跨界明星联合执笔，</span><br><span class="line">分享他们观行思趣的心发现、他们的成长与心路历程，当当网限量赠送出品人@陈坤抄诵印刷版</span><br><span class="line">《心经》，赠完不再加印哦！详情请戳：</span><br><span class="line">http://t.cn/8k622Sj&quot;,&quot;createTime&quot;:&quot;1386925242&quot;,&quot;info1&quot;:&quot;&quot;,&quot;info2&quot;:&quot;&quot;,&quot;info 3&quot;:&quot;&quot;,&quot;mlevel&quot;:&quot;&quot;,&quot;musicurl&quot;:[],&quot;pic_list&quot;: [&quot;http://ww4.sinaimg.cn/thumbnail/b2336177jw1ebi6j4twk7j20m80tkgra.jpg&quot;],&quot; praiseCount&quot;:&quot;1&quot;,&quot;reportCount&quot;:&quot;721&quot;,&quot;source&quot;:&quot;&quot;,&quot;userId&quot;:&quot;2989711735&quot;,&quot;vi deourl&quot;: [],&quot;weiboId&quot;:&quot;3655091741442099&quot;,&quot;weiboUrl&quot;:&quot;http://weibo.com/2989711735/An 7bE639F&quot;&#125;]</span><br></pre></td></tr></table></figure><p>beCommentWeiboId 是否评论<br>beForwardWeiboId 是否是转发微博<br>catchTime 抓取时间<br>commentCount 评论次数<br>content 内容<br>createTime 创建时间<br>info1 信息字段1<br>info2信息字段2<br>info3信息字段3<br>mlevel no sure musicurl ⾳乐链接<br>pic_list 照⽚列表（可以有多个）<br>praiseCount 点赞⼈数<br>reportCount 转发⼈数<br>source 数据来源<br>userId ⽤户id<br>videourl 视频链接<br>weiboId 微博id<br>weiboUrl 微博⽹址</p><p>在hadoop创建目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfs -<span class="built_in">mkdir</span> weibo</span><br><span class="line">hadoop fs -put ./weibo/*  </span><br><span class="line">hadoop fs -<span class="built_in">ls</span> /weibo</span><br></pre></td></tr></table></figure><p>hive创建库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database <span class="keyword">if</span> not exists weibo;</span><br><span class="line">use weibo; </span><br><span class="line">create external table weibo(json string) location <span class="string">&#x27;/weibo&#x27;</span>;</span><br><span class="line">select * from weibo <span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure><p>处理json格式数据使用到get_json_object()和json_tuple(),其中两者都只认最外层是花括号 <strong>{ }</strong> 才能正常解析.最外层是时  <strong>[ ]</strong> 不能解析,<br>当最外层时 <strong>[]</strong> 时可使用substring方法去掉最外层 <strong>[ ]</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select get_json_object(substring(json,2,length(json)-1),<span class="string">&#x27;$.userId&#x27;</span>) from weibo <span class="built_in">limit</span> 1;</span><br></pre></td></tr></table></figure><h1 id="查询需求"><a href="#查询需求" class="headerlink" title="查询需求"></a>查询需求</h1><h2 id="微博总量和独立用户数"><a href="#微博总量和独立用户数" class="headerlink" title="微博总量和独立用户数"></a>微博总量和独立用户数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#总量</span></span><br><span class="line">select count(*) from weibo;</span><br><span class="line"><span class="comment">#独立用户数</span></span><br><span class="line">select count(distinct(get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>))) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-1) as j from weibo) a;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/ff32a818bf074450ac8b80feaf65a2c6.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/0ec5eac5eea9486791464924def4f964.png" alt="在这里插入图片描述"></p><h2 id="用户所有微博被转发的总数，输出前3个用户"><a href="#用户所有微博被转发的总数，输出前3个用户" class="headerlink" title="用户所有微博被转发的总数，输出前3个用户"></a>用户所有微博被转发的总数，输出前3个用户</h2><p>使用json_tuple提取多个字段</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select b.id,<span class="built_in">sum</span>(b.cnt) as bsum </span><br><span class="line">from </span><br><span class="line">(select </span><br><span class="line">json_tuple(a.j,<span class="string">&#x27;userId&#x27;</span>,<span class="string">&#x27;reportCount&#x27;</span>) as (<span class="built_in">id</span>,cnt) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-1) as j from weibo) a) </span><br><span class="line">b </span><br><span class="line">group by b.id</span><br><span class="line">order by bsum desc</span><br><span class="line"><span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/3500581b3fc0441eba9f08671732b73f.png" alt="在这里插入图片描述"></p><h2 id="被转发次数最多的前3条微博，输出用户id"><a href="#被转发次数最多的前3条微博，输出用户id" class="headerlink" title="被转发次数最多的前3条微博，输出用户id"></a>被转发次数最多的前3条微博，输出用户id</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>) as <span class="built_in">id</span>,</span><br><span class="line">cast(get_json_object(a.j,<span class="string">&#x27;$.reportCount&#x27;</span>) as INT) as cnt </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-1) as j from weibo) a </span><br><span class="line">order by cnt desc </span><br><span class="line"><span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/b5267acdcb4d405c8941d4976cd0798b.png" alt="在这里插入图片描述"></p><h2 id="每个用户发布的微博总数，存储到临时表"><a href="#每个用户发布的微博总数，存储到临时表" class="headerlink" title="每个用户发布的微博总数，存储到临时表"></a>每个用户发布的微博总数，存储到临时表</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table weibo_uid_wbcnt( </span><br><span class="line">userid string, wbcnt int ) </span><br><span class="line">row format delimited </span><br><span class="line">fields terminated by <span class="string">&#x27;\t&#x27;</span>; </span><br><span class="line">insert overwrite table weibo_uid_wbcnt select get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>),count(1) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line">group by get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>); </span><br><span class="line"></span><br><span class="line">select * from weibo_uid_wbcnt <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/60bc41496fe548b4a1adb1c687e809a0.png" alt="在这里插入图片描述"></p><h2 id="统计带图片的微博数"><a href="#统计带图片的微博数" class="headerlink" title="统计带图片的微博数"></a>统计带图片的微博数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select count(1) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line"><span class="built_in">where</span> get_json_object(a.j,<span class="string">&#x27;$.pic_list&#x27;</span>) like <span class="string">&#x27;%http%&#x27;</span>; </span><br></pre></td></tr></table></figure><h2 id="统计使用iphone发微博的独立用户数"><a href="#统计使用iphone发微博的独立用户数" class="headerlink" title="统计使用iphone发微博的独立用户数"></a>统计使用iphone发微博的独立用户数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>)) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line"><span class="built_in">where</span> lower(get_json_object(a.j,<span class="string">&#x27;$.source&#x27;</span>)) like <span class="string">&#x27;%iphone%&#x27;</span>;</span><br></pre></td></tr></table></figure><h2 id="微博中评论次数小于1000的用户id和数据来源，放入视图"><a href="#微博中评论次数小于1000的用户id和数据来源，放入视图" class="headerlink" title="微博中评论次数小于1000的用户id和数据来源，放入视图"></a>微博中评论次数小于1000的用户id和数据来源，放入视图</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create view weibo_view as </span><br><span class="line">select get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>) as <span class="built_in">id</span>,get_json_object(a.j,<span class="string">&#x27;$.source&#x27;</span>) as <span class="built_in">source</span> </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line"><span class="built_in">where</span> get_json_object(a.j,<span class="string">&#x27;$.commentCount&#x27;</span>)&lt;1000; </span><br><span class="line">select * from weibo_view <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure><h2 id="统计上条视图中数据来源“ipad客户端”的用户数目"><a href="#统计上条视图中数据来源“ipad客户端”的用户数目" class="headerlink" title="统计上条视图中数据来源“ipad客户端”的用户数目"></a>统计上条视图中数据来源“ipad客户端”的用户数目</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct <span class="built_in">id</span>) as cnt from weibo_view <span class="built_in">where</span> <span class="built_in">source</span>=<span class="string">&#x27;iPad客户端&#x27;</span>;</span><br></pre></td></tr></table></figure><h2 id="将微博的点赞数和转发数求和，降序，取前10条。"><a href="#将微博的点赞数和转发数求和，降序，取前10条。" class="headerlink" title="将微博的点赞数和转发数求和，降序，取前10条。"></a>将微博的点赞数和转发数求和，降序，取前10条。</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class DemoTest1 extends UDF &#123; </span><br><span class="line">public Integer evaluate(Integer num1,Integer num2)&#123; </span><br><span class="line">try&#123; </span><br><span class="line"><span class="built_in">return</span> num1+num2; </span><br><span class="line">&#125;catch (Exception e)&#123; </span><br><span class="line"><span class="built_in">return</span> null; &#125; </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br><span class="line">create temporary <span class="keyword">function</span> wb as <span class="string">&#x27;DemoTest1&#x27;</span>;</span><br><span class="line"></span><br><span class="line">select wb(cast(get_json_object(a.j,<span class="string">&#x27;$.praiseCount&#x27;</span>) as int),cast(get_json_object(a.j,<span class="string">&#x27;$.reportCount&#x27;</span>) as int)) as cnt </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line">order by cnt desc <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure><h2 id="⽤户微博内容中出现iphone关键词的最⼤次数"><a href="#⽤户微博内容中出现iphone关键词的最⼤次数" class="headerlink" title="⽤户微博内容中出现iphone关键词的最⼤次数"></a>⽤户微博内容中出现iphone关键词的最⼤次数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public class DemoTest2 extends UDF &#123; </span><br><span class="line">public int evaluate(String content,String word)&#123; </span><br><span class="line">int count = 0; </span><br><span class="line"><span class="keyword">if</span>(content != null&amp;&amp;content.length()&gt;0)&#123; </span><br><span class="line">String[] array = content.split(word); </span><br><span class="line">count = array.length-1; </span><br><span class="line">&#125; </span><br><span class="line"><span class="built_in">return</span> count; </span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br><span class="line">create temporary <span class="keyword">function</span> wcount as <span class="string">&#x27;DemoTest2&#x27;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">select b.id,max(b.cnt) as cn </span><br><span class="line">from </span><br><span class="line">(select get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>) as <span class="built_in">id</span>,wcount(get_json_object(a.j,<span class="string">&#x27;$.content&#x27;</span>),<span class="string">&#x27;iphone&#x27;</span>) as cnt </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a) b </span><br><span class="line">group by b.id </span><br><span class="line">order by cn desc <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Hive新浪微博日志查询分析</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="hive sql UDF" scheme="https://s-luping.github.io/luping/tags/hive-sql-UDF/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习记录</title>
    <link href="https://s-luping.github.io/luping/2021/10/03/Hive%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://s-luping.github.io/luping/2021/10/03/Hive%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2021-10-03T14:19:17.000Z</published>
    <updated>2022-03-13T12:15:32.364Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h1><p>下载<br><img src="https://img-blog.csdnimg.cn/3482595573ea451a88b20588af509df7.png" alt="在这里插入图片描述"><br>解压<br><img src="https://img-blog.csdnimg.cn/3c26bdff244f4f3c9011948b42fffbcf.png" alt="在这里插入图片描述"><br>重命名<br><img src="https://img-blog.csdnimg.cn/887d815531bd484d8180a3cac53ab2f5.png" alt="在这里插入图片描述"><br>添加环境变量<br>vi &#x2F;etc&#x2F;proflie<br><img src="https://img-blog.csdnimg.cn/6da4eb90a8ed4c129bede74d841c273d.png" alt="在这里插入图片描述"><br>使环境变量生效<br>source &#x2F;etc&#x2F;profile<br>修改配置文件<br>cp hive-env.sh.template hive-env.sh<br><img src="https://img-blog.csdnimg.cn/db764ab0fdf64f6d889b9969b71d90fd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="Hive-Metastore配置"><a href="#Hive-Metastore配置" class="headerlink" title="Hive Metastore配置"></a>Hive Metastore配置</h2><p>将自带的derby数据库替换为mysql数据库 可使多用户连接<br>参考文章<a href="https://my.oschina.net/u/4292373/blog/3497563">https://my.oschina.net/u/4292373/blog/3497563</a><br>登录mysql创建新用户<br><img src="https://img-blog.csdnimg.cn/d9a09ed5764a44af8b20fe6c0bb780b4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><strong>授权</strong><br><img src="https://img-blog.csdnimg.cn/a9c3fb812c494426a71ea415b2a0873e.png" alt="在这里插入图片描述"><br><strong>刷新权限</strong><br><img src="https://img-blog.csdnimg.cn/96b3c492b1254f4583cd073e1ea1ed3e.png" alt="在这里插入图片描述"></p><h2 id="新增hive-site-xml文件"><a href="#新增hive-site-xml文件" class="headerlink" title="新增hive-site.xml文件"></a>新增hive-site.xml文件</h2><p><strong>记坑</strong><br>hive-default.xml.template 的开头就写明了 WARNING!!!对该文件的任何更改都将被Hive忽略<br>其实hive-site.xml是用户定义的配置文件，hive在启动的时候会读取两个文件一个是hive-default.xml.template 还有一个就是hive-site.xml<br>在复制的hive-site.xml里保存你写的配置项，然后将其他的删掉，hive-site.xml只能写你自己的配置项，其他删掉<br>原文链接：<a href="https://blog.csdn.net/qq_43506520/article/details/83346463">https://blog.csdn.net/qq_43506520/article/details/83346463</a></p><p>cp hive-default.xml.template hive-site.xml<br>vi hive-site.xml  在hive-site.xml文件只保存如下配置</p><p><img src="https://img-blog.csdnimg.cn/0ce0cccb18e04df295ab6a2eafc2039b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p><strong>Jdbc安装驱动</strong></p><p>把连接MySQL的JDBC驱动包复制到Hive的lib目录下 下载地址：<a href="https://dev.mysql.com/downloads/connector/j/">https://dev.mysql.com/downloads/connector/j/</a> 驱动包名为：mysql-connector-java-5.1.46-bin.jar</p><p><strong>初始化数据库</strong></p><p>schematool -dbType mysql -initSchema</p><p><img src="https://img-blog.csdnimg.cn/6f840193fa5842b2b848b8a750b28412.png" alt="在这里插入图片描述"></p><p>若失败 错误类型和参考如下</p><p><a href="https://blog.csdn.net/lsr40/article/details/78026125">https://blog.csdn.net/lsr40/article/details/78026125</a></p><p><a href="https://blog.csdn.net/brotherdong90/article/details/49661731/">https://blog.csdn.net/brotherdong90/article/details/49661731/</a></p><p><strong>开启metastore</strong></p><p>hive –service metastore #开启元数据服务 防火墙开启9083</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>在dfs上的路径<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://localhost:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>开启hiveserver2 远程连接服务</strong></p><p>hiveserver2 start #</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--配置远程访问hive的用户密码--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.client.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.client.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root123<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 校验在metastore中存储的信息的版本和hive的jar包中的版本一致性--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>启动出现临时文件夹位置未定义问题</strong></p><p>解决方案参考如下文章：<br><a href="https://www.cnblogs.com/qxyy/articles/5247933.html">https://www.cnblogs.com/qxyy/articles/5247933.html</a></p><p>查看系统存储的hive运行日志</p><p>在.&#x2F;conf&#x2F;hive-log4j2.properties文件中记录系统日志位置，默认&#x2F;tmp&#x2F;user&#x2F;hive.log</p><h1 id="数据定义"><a href="#数据定义" class="headerlink" title="数据定义"></a>数据定义</h1><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#创建</span><br><span class="line"><span class="keyword">Create</span> database  if <span class="keyword">not</span> <span class="keyword">exists</span> emp；</span><br><span class="line">#查看</span><br><span class="line"><span class="keyword">Show</span> databases；</span><br><span class="line">#描述</span><br><span class="line"><span class="keyword">Describe</span> formatted emp；</span><br><span class="line">#使用</span><br><span class="line">Use emp；</span><br><span class="line">#修改</span><br><span class="line"><span class="keyword">Alter</span> database <span class="keyword">set</span> dbproperty;</span><br></pre></td></tr></table></figure><h2 id="数据表"><a href="#数据表" class="headerlink" title="数据表"></a>数据表</h2><p>创建普通内部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">table</span> employee(eid <span class="type">int</span>,ename string,egender tinyint,esalary <span class="type">float</span>);</span><br></pre></td></tr></table></figure><p>创建外部表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Create</span> <span class="keyword">external</span> <span class="keyword">table</span> emp(eid <span class="type">int</span>,ename string,egender tinyint,esalary <span class="type">float</span>);</span><br></pre></td></tr></table></figure><p>内部表外部表转换</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> table_name <span class="keyword">set</span> tablepropertiles(‘<span class="keyword">external</span>’<span class="operator">=</span>’<span class="literal">true</span>’<span class="operator">|</span><span class="literal">false</span>);</span><br></pre></td></tr></table></figure><h2 id="分隔符"><a href="#分隔符" class="headerlink" title="分隔符"></a>分隔符</h2><p><strong>列分隔符</strong> <strong>行分隔符</strong> <strong>Array分隔符</strong> <strong>Map分隔符</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> sales_info_new(</span><br><span class="line">sku_id string comment <span class="string">&#x27;商品id&#x27;</span>,</span><br><span class="line">sku_name string comment <span class="string">&#x27;商品名称&#x27;</span>,</span><br><span class="line">state_map map<span class="operator">&lt;</span>string,string<span class="operator">&gt;</span> comment <span class="string">&#x27;商品状态信息&#x27;</span>,</span><br><span class="line">id_array <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span> comment <span class="string">&#x27;商品相关id列表&#x27;</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span>(</span><br><span class="line">dt  string comment <span class="string">&#x27;年-月-日&#x27;</span></span><br><span class="line">)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">  fields terminated <span class="keyword">by</span> <span class="string">&#x27;|&#x27;</span></span><br><span class="line">  collection items terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">  map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span>;      </span><br></pre></td></tr></table></figure><p>导入数据样例</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">123|华为Mate10|id:1111,token:2222,user_name:zhangsan1|1235,345</span><br><span class="line">456|华为Mate30|id:1113,token:2224,user_name:zhangsan3|89,635</span><br><span class="line">789|小米5|id:1114,token:2225,user_name:zhangsan4|452,63</span><br><span class="line">1235|小米6|id:1115,token:2226,user_name:zhangsan5|785,36</span><br><span class="line">4562|OPPO Findx|id:1116,token:2227,user_name:zhangsan6|7875,3563</span><br></pre></td></tr></table></figure><p><strong>文本：不用写双引号，花括号，程序会自动添加双引号和花括号！ 加了会出错</strong></p><h2 id="分区分桶"><a href="#分区分桶" class="headerlink" title="分区分桶"></a>分区分桶</h2><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>为什么分：</p><p>使用分区技术，避免hive全表扫描，提升查询效率</p><p>如何分：</p><p>整个表的数据在存储时划分到多个子目录，从而在查询时可以指定查询条件（子目录以分区变量的值来命名）eg:year&#x3D;‘2018’</p><p>分区需注意什么：</p><p>PARTIONED BY(colName dataType)<br>hive的分区字段使用的是表外字段。而mysql使用的是表内字段。</p><p>1、hive的分区名区分大小写 不能使用中文</p><p>2、hive的分区本质是在表目录下面创建目录，但是该分区字段是一个伪列，不真实存在于数据中</p><p>3、一张表可以有一个或者多个分区，分区下面也可以有一个或者多个分区</p><p><strong>导入分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/xxx&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> part1 <span class="keyword">partition</span>(country<span class="operator">=</span><span class="string">&#x27;China&#x27;</span>); #要指定分区</span><br></pre></td></tr></table></figure><p><strong>#修改分区的存储路径：(hdfs的路径必须是全路径)</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> part1 <span class="keyword">partition</span>(country<span class="operator">=</span><span class="string">&#x27;Vietnam&#x27;</span>) <span class="keyword">set</span> location ‘hdfs:<span class="operator">/</span><span class="operator">/</span>hadoop01:<span class="number">9000</span><span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>brz.db<span class="operator">/</span>part1<span class="operator">/</span>country<span class="operator">=</span>Vietnam’</span><br></pre></td></tr></table></figure><p><strong>二级分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> part2(</span><br><span class="line">uid <span class="type">int</span>,</span><br><span class="line">uname string,</span><br><span class="line">uage <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">year</span> string,<span class="keyword">month</span> string)</span><br><span class="line"><span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"># 导入多分区</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/usr/local/xxx&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> part1 <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2018&#x27;</span>,<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;09&#x27;</span>); </span><br></pre></td></tr></table></figure><p><strong>增加分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> part1 <span class="keyword">add</span> <span class="keyword">partition</span>(country<span class="operator">=</span><span class="string">&#x27;india&#x27;</span>) <span class="keyword">partition</span>(country<span class="operator">=</span><span class="string">&#x27;korea&#x27;</span>) <span class="keyword">partition</span>(country<span class="operator">=</span><span class="string">&#x27;America&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>动态分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#动态分区的属性：</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span>;<span class="operator">/</span><span class="operator">/</span>(<span class="literal">true</span><span class="operator">/</span><span class="literal">false</span>)</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>strict;<span class="operator">/</span><span class="operator">/</span>(strict<span class="operator">/</span>nonstrict) #至少有一个静态的值</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partitions<span class="operator">=</span><span class="number">1000</span>;<span class="operator">/</span><span class="operator">/</span>(分区最大数)</span><br><span class="line"><span class="keyword">set</span> hive.exec.max.dynamic.partitions.pernode<span class="operator">=</span><span class="number">100</span></span><br><span class="line">#创建动态分区表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> dt_part1(</span><br><span class="line">uid <span class="type">int</span>,</span><br><span class="line">uname string,</span><br><span class="line">uage <span class="type">int</span></span><br><span class="line">)</span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (<span class="keyword">year</span> string,<span class="keyword">month</span> string)</span><br><span class="line"><span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">;</span><br><span class="line">#加载数据：（使用 <span class="keyword">insert</span> <span class="keyword">into</span>方式加载数据）</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> dy_part1 <span class="keyword">partition</span>(<span class="keyword">year</span>,<span class="keyword">month</span>) <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> part_tmp ;</span><br></pre></td></tr></table></figure><h3 id="分桶"><a href="#分桶" class="headerlink" title="分桶"></a>分桶</h3><p>在分区下分桶，分桶使用表内字段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法格式</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test</span><br><span class="line">(<span class="operator">&lt;</span>col_name<span class="operator">&gt;</span> <span class="operator">&lt;</span>data_type<span class="operator">&gt;</span> [, <span class="operator">&lt;</span>col_name<span class="operator">&gt;</span> <span class="operator">&lt;</span>data_type<span class="operator">&gt;</span> ...])]</span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> ...]</span><br><span class="line">CLUSTERED <span class="keyword">BY</span> (<span class="operator">&lt;</span>col_name<span class="operator">&gt;</span>)</span><br><span class="line">[SORTED <span class="keyword">BY</span> (<span class="operator">&lt;</span>col_name<span class="operator">&gt;</span> [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>] [, <span class="operator">&lt;</span>col_name<span class="operator">&gt;</span> [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>]...])]</span><br><span class="line"><span class="keyword">INTO</span> <span class="operator">&lt;</span>num_buckets<span class="operator">&gt;</span> BUCKETS</span><br></pre></td></tr></table></figure><p>CLUSTERED BY (<col_name>) 以哪一列进行分桶 选择一列来分桶</p><p>SORTED BY (<col_name> [ASC|DESC] 对分桶内的数据进行排序</p><p>INTO <num_buckets> BUCKETS 分成几个桶</p><p>##列信息更改<br>修改名称</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Alter</span> <span class="keyword">table</span> emp change eid id string;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/18affe192e0e401688465b35b82ee73c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>增加列<br><img src="https://img-blog.csdnimg.cn/5dc7baa42431479784722bda4f2cf314.png" alt="在这里插入图片描述"></p><h1 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#装载数据</span><br><span class="line">Load data <span class="keyword">to</span> <span class="keyword">table</span> inpath </span><br><span class="line">#插入数据</span><br><span class="line"><span class="keyword">Insert</span> <span class="keyword">into</span> <span class="keyword">table</span> emp <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="number">2021</span>,<span class="keyword">month</span><span class="operator">=</span><span class="number">10</span>) <span class="keyword">select</span> id,name <span class="keyword">from</span> ept;</span><br><span class="line">#导出数据</span><br><span class="line">#到hdfs</span><br><span class="line">Export <span class="keyword">table</span> ept <span class="keyword">to</span> ‘<span class="operator">/</span>hom<span class="operator">/</span>emp’;</span><br><span class="line">#<span class="keyword">Insert</span> 导出</span><br><span class="line"><span class="keyword">Insert</span> overwrite <span class="keyword">local</span> directory ‘path’ <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;</span><br><span class="line">#到本地</span><br><span class="line">Hfds dfs <span class="operator">-</span><span class="keyword">get</span> localpath</span><br><span class="line">#Hive shell 命令导出</span><br><span class="line">Hive <span class="operator">-</span>e ‘<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp;’ <span class="operator">&gt;</span> localpath</span><br><span class="line">#导入数据</span><br><span class="line">Import <span class="keyword">table</span> emp  <span class="keyword">from</span> path;</span><br><span class="line">#HQL查询</span><br><span class="line"><span class="keyword">Case</span> <span class="keyword">when</span> </span><br><span class="line"><span class="keyword">Select</span> name,salary, <span class="keyword">case</span></span><br><span class="line">Wehn salary <span class="operator">&lt;</span><span class="number">5000</span>  <span class="keyword">then</span> ‘low’</span><br><span class="line"><span class="keyword">When</span> salary <span class="operator">&gt;=</span><span class="number">5000</span> <span class="keyword">and</span> salary <span class="operator">&lt;</span><span class="number">7000</span> <span class="keyword">then</span> ‘middle’</span><br><span class="line">Whne salary <span class="operator">&gt;=</span><span class="number">7000</span> <span class="keyword">then</span> salary <span class="operator">&lt;</span> <span class="number">10000</span> <span class="keyword">then</span> ‘high’</span><br><span class="line"><span class="keyword">Else</span> ‘vary high’</span><br><span class="line"><span class="keyword">End</span> <span class="keyword">as</span> bracket <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure><p>##Like和rlike<br>使用Like运算符可以进行模糊查询，通配符”%”代表0个或多个字符，”_”代表1个字符。</p><p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p><h2 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h2><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个队列结果进行分组，然后对每个组执行聚合操作</p><h2 id="HAVING"><a href="#HAVING" class="headerlink" title="HAVING"></a>HAVING</h2><p>在 SQL 中增加 HAVING 子句原因是，WHERE 关键字无法与合计函数一起使用。</p><p>Having 与where不同</p><p>（1）where是对表中数据的筛选，having是对分组统计结果的筛选</p><p>（2）Where后不能写分组函数，而having后可以使用分组函数。</p><p>（3）Having只用于group by分组统计语句。</p><p>SELECT Customer,SUM(OrderPrice) FROM Orders<br>GROUP BY Customer<br>HAVING SUM(OrderPrice)&lt;2000</p><h2 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h2><h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><p>内连接（INNER JOIN）中，只有进⾏连接的两个表中都存在与连接条件相匹配的数据 时，记录才会被筛选出来</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a.empno,a.ename,b.dname <span class="keyword">FROM</span> emp a <span class="keyword">JOIN</span> dept b <span class="keyword">ON</span> a.deptno<span class="operator">=</span>b.deptno;</span><br></pre></td></tr></table></figure><h3 id="左连接"><a href="#左连接" class="headerlink" title="左连接"></a>左连接</h3><p>左外连接（LEFT OUTER JOIN）中，JOIN操作符左边表中符合WHERE⼦句的所有记 录将会出现在查询结果中。右边表中如果没有符合ON后⾯连接条件的记录时，从右边表 指定选择的列的值将会是NULL。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a.empno,a.ename,b.dname <span class="keyword">FROM</span> emp a <span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> dept b <span class="keyword">ON</span> a.deptno<span class="operator">=</span><span class="operator">=</span>b.deptno;</span><br></pre></td></tr></table></figure><h3 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h3><p><img src="https://img-blog.csdnimg.cn/49006f6c6d35416db52f662fdaab8791.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/5f62acabaa6444549dee737ae1d72303.png" alt="在这里插入图片描述"></p><h3 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h3><p>连接 n个表，⾄少需要n-1个连接条件。例如：连接三个表，⾄少需要两个连接条件。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a.ename,b.dname,c.zip <span class="keyword">FROM</span> emp a <span class="keyword">JOIN</span> dept b <span class="keyword">ON</span> a.deptno<span class="operator">=</span>b.deptno <span class="keyword">JOIN</span> location c <span class="keyword">ON</span> b.loc<span class="operator">=</span>c.loc;</span><br></pre></td></tr></table></figure><p>注意：为什么不是表b和表c先进⾏连接操作呢？这是因为Hive总是按照从左到右的顺序 执⾏的。</p><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>###ORDER BY<br>ORDER BY⽤于对全局查询结果进⾏排序，也就是说会有⼀个所有的数据都通过⼀个 reducer进⾏处理的过程。 </p><h3 id="SORT-BY"><a href="#SORT-BY" class="headerlink" title="SORT BY"></a>SORT BY</h3><p>Hive增加了⼀个可供选择的⽅式，即SORT BY，其只会在每个reducer中对数据进⾏排 序，即执⾏⼀个局部排序过程。这会保证每个reducer的输出数据都是有序的（但并⾮ 全局有序）。<br>ORDER BY 和SORT BY的区别是当reducer的个数⼤于1时，两种操作的输出结果是不 同的，SORT BY是reducer内的局部排序。<br>###DISTRIBUTE BY和SORT BY<br>如果我们想对同⼀部⻔中的员⼯进⾏排序处理，那么我们可以使⽤DISTRIBUTE BY来保 证具有相同部⻔编号的员⼯被分到同⼀个reducer中去，然后使⽤SORT BY来按照我们 的期望对数据进⾏排序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> emp DISTRIBUTE <span class="keyword">BY</span> deptno SORT <span class="keyword">BY</span> empno <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><p>###CLUSTER BY<br>当distribute by和sorts by字段相同时，可以使⽤cluster by⽅式。 ⽤cluster b除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序 排序，不能指定排序规则为ASC或者DESC。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp cluster <span class="keyword">by</span> deptno; </span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> emp distribute <span class="keyword">by</span> deptno sort <span class="keyword">by</span> deptno;</span><br></pre></td></tr></table></figure><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><p>Hive会在适当的时候对数值型数据类型进⾏隐式类型转换，有些时候需要显示类型转换 时可以使⽤关键字cast。</p><p>显示类型转换函数的语法是: </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cast</span>(<span class="keyword">value</span> <span class="keyword">AS</span> TYPE)</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> employees CHANGE <span class="keyword">COLUMN</span> salary salary STRING; </span><br><span class="line"><span class="keyword">SELECT</span> name,salary <span class="keyword">FROM</span> employees <span class="keyword">WHERE</span> <span class="built_in">cast</span>(salary <span class="keyword">AS</span> <span class="type">FLOAT</span>) <span class="operator">&lt;</span> <span class="number">100000.0</span>;</span><br></pre></td></tr></table></figure><h2 id="空字段赋值"><a href="#空字段赋值" class="headerlink" title="空字段赋值"></a>空字段赋值</h2><p>NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果 string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为 NULL ，则返回NULL。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> nvl(comm,<span class="number">-1</span>) <span class="keyword">from</span> emp;</span><br></pre></td></tr></table></figure><h1 id="Hive-合并小文件"><a href="#Hive-合并小文件" class="headerlink" title="Hive 合并小文件"></a>Hive 合并小文件</h1><p>当hive中数据都是由小文件组成时，需要将这些小文件合并为一个大的文件</p><p>步骤</p><p>创建临时表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test <span class="keyword">like</span> table1;</span><br></pre></td></tr></table></figure><p>在当前会话设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.dynamic.parition=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.exec.dynamic.parition.mode=nostrict;</span><br><span class="line"><span class="built_in">set</span> hive.exec.max.dynamic.parition=100000;</span><br><span class="line"><span class="built_in">set</span> hive.merge.smallfiles.avgsize=128000000; <span class="comment">#128M</span></span><br><span class="line"><span class="built_in">set</span> hive.merge.size.per.task=128000000;</span><br></pre></td></tr></table></figure><p>将原表数据合并到临时表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> test <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> <span class="keyword">table</span>;</span><br></pre></td></tr></table></figure><p>创建备份目录将原表数据放进备份目录，将临时表数据迁移至原表目录</p><h1 id="hive数据倾斜问题"><a href="#hive数据倾斜问题" class="headerlink" title="hive数据倾斜问题"></a>hive数据倾斜问题</h1><p>核心解决方案时多段聚合，将第一次聚合时每个key加上随机数，对数据打散，在进行二次聚合。</p><h2 id="group-by-distinct"><a href="#group-by-distinct" class="headerlink" title="group by distinct"></a>group by distinct</h2><p>进行group by 时形成 key val_list,当某些key重复数据较多时，就会产生数据倾斜</p><p>核心解决方案时多段聚合，将第一次聚合时每个key加上随机数，对数据打散，在进行二次聚合。</p><h2 id="join导致的"><a href="#join导致的" class="headerlink" title="join导致的"></a>join导致的</h2><p><img src="/luping/2021/10/03/Hive%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_2.png" alt="img_2.png"><br>找出on的字段重复</p><h1 id="hive-sql-如何转换为mapreduce任务"><a href="#hive-sql-如何转换为mapreduce任务" class="headerlink" title="hive sql 如何转换为mapreduce任务"></a>hive sql 如何转换为mapreduce任务</h1><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> userid <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">group</span> <span class="keyword">by</span> userid;</span><br></pre></td></tr></table></figure><p>group by 的字段组合作为 map任务输出key的值，同时作为reduce得输入，在map阶段之后的分区、排序时reduce将相同的key放到一起来处理<br><img src="/luping/2021/10/03/Hive%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_3.png" alt="img_3.png"></p>]]></content>
    
    
    <summary type="html">Hive学习记录</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="Hive" scheme="https://s-luping.github.io/luping/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>spark job的几种提交流程</title>
    <link href="https://s-luping.github.io/luping/2021/10/02/spark%20job%E7%9A%84%E5%87%A0%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"/>
    <id>https://s-luping.github.io/luping/2021/10/02/spark%20job%E7%9A%84%E5%87%A0%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</id>
    <published>2021-10-01T20:43:38.000Z</published>
    <updated>2022-03-13T12:27:25.080Z</updated>
    
    <content type="html"><![CDATA[<h1 id="standalone"><a href="#standalone" class="headerlink" title="standalone"></a>standalone</h1><p>集群启动后worker向master注册信息，<br>通过spark-submit提交任务时，在任务提交节点或Client启动driver，<br>在driver创建并初始化sparkContext对象，包含DAGScheduler和TaskScheduler，TaskScheduler与Master节点通讯申请注册Application，Master节点接收到Application的注册请求后，通过资源调度算法，在自己的集群的worker上启动Executor进程；启动的Executor也会反向注册到TaskScheduler上<br>DAGScheduler：负责把Spark作业转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；<br>所有task运行完成后，SparkContext向Master注销，释放资源；<br><img src="https://img-blog.csdnimg.cn/94cb178cb14e4ccf982cd5420e795086.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><a href="https://blog.csdn.net/bokzmm/article/details/79476409">参考文章</a></p><h1 id="spark-on-yarn"><a href="#spark-on-yarn" class="headerlink" title="spark on yarn"></a>spark on yarn</h1><p>配置<br>在client节点配置中spark-env.sh添加Hadoop_HOME的配置目录即可提交yarn 任务，具体步骤如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br></pre></td></tr></table></figure><p>注意client只需要有Spark的安装包即可提交任务，不需要其他配置（比如slaves）!!!</p><h2 id="client"><a href="#client" class="headerlink" title="client"></a>client</h2><p><img src="https://img-blog.csdnimg.cn/e6dd5707482141ec8537ab908cb4c20e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>SparkContext在Client创建并实例化<br>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler<br>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster<br>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源<br>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver<br>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task<br>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#当前目录</span></span><br><span class="line"><span class="comment">#/export/servers/spark-2.2.3/</span></span><br><span class="line">spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn --deploy-mode client \</span><br><span class="line">--conf spark.driver.host=192.168.88.12 \</span><br><span class="line">examples/jars/spark-examples_2.11-2.2.3.jar 10</span><br></pre></td></tr></table></figure><p>–class 是全限定类名<br>–master yarn 是指使用yarn管理<br>–deploy-mode <strong>client</strong>*<em>cluster</em>* client可以在提交任务的机器查看结果<br>cluster只能在yarn上看结果<br>–conf spark.driver.host&#x3D;192.168.88.12 driver监听的主机名或者IP地址。就是提交任务机器的地址，这用于和executors以及独立的master通信接收结果<br>examples&#x2F;jars&#x2F;spark-examples_2.11-2.2.3.jar 运行的jar包<br>当我们测试一个demo如下图会为spark任务自动机器CPU、内存等资源<br><img src="https://img-blog.csdnimg.cn/9bf8d66eaae74d81af5a8471050826ef.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h2><p>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler<br>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster<br>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源<br>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver<br>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task<br>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p><h2 id="Yarn-client和Yarn-cluster的区别："><a href="#Yarn-client和Yarn-cluster的区别：" class="headerlink" title="Yarn-client和Yarn-cluster的区别："></a>Yarn-client和Yarn-cluster的区别：</h2><p>yarn-cluster模式下，Dirver运行在ApplicationMaster中，负责申请资源并监控task运行状态和重试失败的task，当用户提交了作业之后就可以关掉client，作业会继续在yarn中运行；<br>yarn-client模式下，Dirver运行在本地客户端，client不能离开。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;standalone&quot;&gt;&lt;a href=&quot;#standalone&quot; class=&quot;headerlink&quot; title=&quot;standalone&quot;&gt;&lt;/a&gt;standalone&lt;/h1&gt;&lt;p&gt;集群启动后worker向master注册信息，&lt;br&gt;通过spark-sub</summary>
      
    
    
    
    
    <category term="spark" scheme="https://s-luping.github.io/luping/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习记录</title>
    <link href="https://s-luping.github.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://s-luping.github.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2021-09-13T19:38:03.000Z</published>
    <updated>2022-03-14T01:16:50.597Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spark介绍"><a href="#spark介绍" class="headerlink" title="spark介绍"></a>spark介绍</h1><p>Spark是加州大学伯克利分校AMP实验室（Algorithms, Machines, and People Lab）开发的通用内存并行计算框架<br>Spark使用Scala语言进行实现，它是一种面向对象、函数式编程语言，能够像操作本地集合对象一样轻松地操作分布式数据集，具有以下特点:<br>1.运行速度快：Spark拥有DAG执行引擎，支持在内存中对数据进行迭代计算。官方提供的数据表明，如果数据由磁盘读取，速度是Hadoop MapReduce的10倍以上，如果数据从内存中读取，速度可以高达100多倍。<br>2.易用性好：Spark不仅支持Scala编写应用程序，而且支持Java和Python等语言进行编写，特别是Scala是一种高效、可拓展的语言，能够用简洁的代码处理较为复杂的处理工作。<br>3.通用性强：Spark生态圈即BDAS（伯克利数据分析栈）包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX等组件，这些组件分别处理Spark Core提供内存计算框架、SparkStreaming的实时处理应用、Spark SQL的即席查询、MLlib或MLbase的机器学习和GraphX的图处理。<br>4.随处运行：Spark具有很强的适应性，能够读取HDFS、Cassandra、HBase、S3和Techyon为持久层读写原生数据，能够以Mesos、YARN和自身携带的Standalone作为资源管理器调度job，来完成Spark应用程序的计算</p><h2 id="Spark与Hadoop差异"><a href="#Spark与Hadoop差异" class="headerlink" title="Spark与Hadoop差异"></a>Spark与Hadoop差异</h2><p>Spark是在借鉴了MapReduce之上发展而来的，继承了其分布式并行计算的优点并改进了MapReduce明显的缺陷，具体如下:<br>首先，Spark把中间数据放到内存中，迭代运算效率高。MapReduce中计算结果需要落地，保存到磁盘上，这样势必会影响整体速度，而Spark支持DAG图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，提高了处理效率。<br>其次，Spark容错性高。Spark引进了弹性分布式数据集RDD (Resilient Distributed Dataset) 的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，如果数据集一部分丢失，则可以根据“血统”（即充许基于数据衍生过程）对它们进行重建。另外在RDD计算时可以通过CheckPoint来实现容错，而CheckPoint有两种方式：CheckPoint Data，和Logging The Updates，用户可以控制采用哪种方式来实现容错。<br>最后，Spark更加通用。不像Hadoop只提供了Map和Reduce两种操作，Spark提供的数据集操作类型有很多种，大致分为：Transformations和Actions两大类。Transformations包括Map、Filter、FlatMap、Sample、GroupByKey、ReduceByKey、Union、Join、Cogroup、MapValues、Sort和PartionBy等多种操作类型，同时还提供Count, Actions包括Collect、Reduce、Lookup和Save等操作。另外各个处理节点之间的通信模型不再像Hadoop只有Shuffle一种模式，用户可以命名、物化，控制中间结果的存储、分区等。<br><a href="https://blog.csdn.net/kxiaozhuk/article/details/82699175">原文链接</a></p><h1 id="spark安装"><a href="#spark安装" class="headerlink" title="spark安装"></a>spark安装</h1><h2 id="下载解压"><a href="#下载解压" class="headerlink" title="下载解压"></a>下载解压</h2><p>下载安装包 解压到本地软件安装目录<br><a href="https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8.tgz">spark-2.4.8.tgz</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers</span><br><span class="line">wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8.tgz</span><br><span class="line">tar xvf spark-2.4.8.tgz .</span><br></pre></td></tr></table></figure><h2 id="添加系统环境变量"><a href="#添加系统环境变量" class="headerlink" title="添加系统环境变量"></a>添加系统环境变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/servers/spark-2.4.8</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h2 id="spark-shell"><a href="#spark-shell" class="headerlink" title="spark-shell"></a>spark-shell</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p><img src="/luping/img_6.png" alt="img_5.png"></p><h1 id="spark任务提交执行"><a href="#spark任务提交执行" class="headerlink" title="spark任务提交执行"></a>spark任务提交执行</h1><h2 id="standalone-spark自主管理的集群模式"><a href="#standalone-spark自主管理的集群模式" class="headerlink" title="standalone spark自主管理的集群模式"></a>standalone spark自主管理的集群模式</h2><p><strong>要配置spark安装目录下的slaves文件</strong>添加本地注意域名映射<br><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_7.png" alt="img_7.png"><br>通过spark-submit提交任务时，在任务提交节点或Client启动driver，<br>在driver创建并初始化sparkContext对象包含DAGScheduler和TaskScheduler，<br>与master通信申请资源，master指派worker为其启动executor<br>生成job阶段，遇到行动算子生成一个job<br>DAGScheduler负责把Sparkjob转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；<br>TaskScheduler与Master节点通讯申请注册Application，Master节点接收到Application的注册请求后，通过资源调度算法，在自己的集群的worker上启动Executor进程；启动的Executor也会反向注册到TaskScheduler上<br>所有task运行完成后，SparkContext向Master注销，释放资源；<br>Stage阶段划分<br>根据宽依赖窄依赖划分阶段，判断宽依赖和窄依赖的依据是是否进行shuffle操作，不需要shuffle的窄依赖分到一个阶段中间的RDD转换操作无需落地，而宽依赖需要shuffle的过程数据需要落地磁盘</p><h2 id="spark-on-yarn-提交到hadoop的yarn集群执行"><a href="#spark-on-yarn-提交到hadoop的yarn集群执行" class="headerlink" title="spark on yarn 提交到hadoop的yarn集群执行"></a>spark on yarn 提交到hadoop的yarn集群执行</h2><p><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_8.png" alt="img_8.png"><br>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler<br>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster<br>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源<br>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver<br>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task<br>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p><h1 id="spark的模块"><a href="#spark的模块" class="headerlink" title="spark的模块"></a>spark的模块</h1><h2 id="spark-Core"><a href="#spark-Core" class="headerlink" title="spark Core"></a>spark Core</h2><p>###RDD<br>Spark提供的主要抽象是弹性分布式数据集(RDD),它是跨集群节点分区的元素集合,可以并行操作.<br>RDD特点:</p><ul><li>1.它是在集群节点上的不可变的、已分区的集合对象;</li><li>2.通过并行转换的方式来创建(如 Map、 filter、join 等);</li><li>3.失败自动重建;</li><li>4.可以控制存储级别(内存、磁盘等)来进行重用;</li><li>5.必须是可序列化的;</li><li>6.是静态类型的(只读)。</li></ul><h3 id="RDD操作函数"><a href="#RDD操作函数" class="headerlink" title="RDD操作函数"></a>RDD操作函数</h3><p>RDD的操作函数主要分为2种类型行动算子(Transformation)和转换算子(Action).<br>可以对RDD进行函数操作,当你对一个RDD进行了操作,那么结果将会是一个新的RDD<br><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_9.png" alt="img_9.png"><br>Transformation操作不是马上提交Spark集群执行,Spark在遇到 Transformation操作时只会记录需要这样的操作,并不会去执行,需要等到有Action 操作的时候才会真正启动计算过程进行计算.<br>针对每个 Action,Spark 会生成一个Job, 从数据的创建开始,经过 Transformation, 结尾是 Action 操作.<br>这些操作对应形成一个有向无环图(DAG),形成 DAG 的先决条件是最后的函数操作是一个Action.</p><h3 id="DAG-stage-划分依据"><a href="#DAG-stage-划分依据" class="headerlink" title="DAG stage 划分依据"></a>DAG stage 划分依据</h3><p><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_10.png" alt="img_10.png"><br>spark dagscheduler将任务划分stage,shuffle是划分DAG中stage 的标识,同时影响Spark执行速度的关键步骤.<br>RDD 的 Transformation 函数中,又分为窄依赖(narrow dependency)和宽依赖(wide dependency)的操作.<br>窄依赖跟宽依赖的区别是是否发生 shuffle(洗牌) 操作.宽依赖会发生 shuffle 操作.<br>窄依赖是子 RDD的各个分片(partition)不依赖于其他分片,能够独立计算得到结果,<br>宽依赖指子 RDD 的各个分片会依赖于父RDD 的多个分片,所以会造成父 RDD 的各个分片在集群中重新分片</p><h3 id="shuffle优化"><a href="#shuffle优化" class="headerlink" title="shuffle优化"></a>shuffle优化</h3><p>shuffle涉及网络传输和磁盘io,非常消耗资源 因此需要对shuffle优化<br><strong>一是如果可以避免shuffle则不选择涉及shuffle的算子</strong><br>rdd.groupByKey().mapValues(_ .sum) 与 rdd.reduceByKey(_ + _) 执行的结果是一样的，但是前者需要把全部的数据通过网络传递一遍，而后者只需要根据每个 key 局部的 partition 累积结果，在 shuffle 的之后把局部的累积值相加后得到结果.<br><strong>缓存机制 cache persist</strong><br>Spark中对于一个RDD执行多次算子(函数操作)的默认原理是这样的:每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。<br>对于这种情况,可对多次使用的RDD进行持久化。<br>cache 是使用的默认缓存选项,一般默认为Memoryonly(内存中缓存),<br>persist 则可以在缓存的时候选择任意一种缓存类型.事实上,cache内部调用的是默认的persist.persist可选择的方式很多缓存到磁盘或是内存磁盘组合缓存等</p><h1 id="spark常用算子"><a href="#spark常用算子" class="headerlink" title="spark常用算子"></a>spark常用算子</h1><h2 id="转换算子-Transformations"><a href="#转换算子-Transformations" class="headerlink" title="转换算子(Transformations)"></a>转换算子(Transformations)</h2><table><thead><tr><th>Transformations</th><th>Description</th></tr></thead><tbody><tr><td>map(func)</td><td>通过函数func传递源的每个元素，返回一个新的分布式数据集。</td></tr><tr><td>filter(func)</td><td>过滤数据，通过选择func返回true的源元素返回一个新的数据集。</td></tr><tr><td>flatMap(func)</td><td>与map类似，但是每个输入项都可以映射到0个或更多的输出项(因此func应该返回一个Seq而不是单个项)。展平 多个集合 汇总成一个集合</td></tr><tr><td>mapPartitions(func)</td><td>与map类似，但在RDD的每个分区(块)上分别运行，因此在类型为T的RDD上运行时，func必须是Iterator &#x3D;&gt; Iterator</td></tr><tr><td>mapPartitionsWithIndex(func)</td><td>与mapPartitions类似，但也为func提供了一个表示分区索引的整数值，因此func必须是类型(Int, Iterator) &#x3D;&gt; Iterator时，类型为T的RDD。</td></tr><tr><td>sample(withReplacement, fraction, seed)</td><td>使用给定的随机数生成器种子，对数据的一小部分进行抽样，无论是否进行替换。</td></tr><tr><td>union(otherDataset)</td><td>合并，返回一个新数据集，其中包含源数据集中的元素和参数的并集。</td></tr><tr><td>intersection(otherDataset)</td><td>交集，返回一个新的RDD，其中包含源数据集中的元素和参数的交集。</td></tr><tr><td>distinct([numPartitions]))</td><td>去重，返回包含源数据集的不同元素的新数据集。</td></tr><tr><td>groupByKey([numPartitions])</td><td>当对一个(K, V)对的数据集调用时，返回一个(K，可迭代)对的数据集。注意:如果您要对每个键进行分组以执行聚合(比如求和或平均)，那么使用reduceByKey或aggregateByKey将产生更好的性能。注意:默认情况下，输出中的并行级别取决于父RDD的分区数量。您可以传递一个可选的numPartitions参数来设置不同数量的任务。</td></tr><tr><td>reduceByKey(func, [numPartitions])</td><td>在（K，V）对的数据集上调用时，返回一个（K，V）对的数据集，其中每个键的值使用给定的reduce函数func进行聚合，该函数的类型必须是（V，V）&#x3D;&gt;V。与groupByKey一样，reduce任务的数量可以通过可选的第二个参数进行配置。</td></tr><tr><td>aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])</td><td>当对一个(K, V)对的数据集调用时，返回一个(K, U)对的数据集，其中每个键的值使用给定的combine函数和一个中立的“零”值进行聚合。允许不同于输入值类型的聚合值类型，同时避免不必要的分配。与groupByKey类似，reduce任务的数量可以通过第二个可选参数进行配置。</td></tr><tr><td>sortByKey([ascending], [numPartitions])</td><td>当对一个(K, V)对的数据集(K, V)调用时，K实现有序，返回一个(K, V)对的数据集，按键序升序或降序排序，如布尔升序参数中指定的那样。</td></tr><tr><td>join(otherDataset, [numPartitions])</td><td>当对类型(K, V)和(K, W)的数据集调用时，返回一个(K， (V, W))对的数据集，其中包含每个键的所有元素对。通过leftOuterJoin、right touterjoin和fullOuterJoin来支持外部连接。</td></tr><tr><td>cogroup(otherDataset, [numPartitions])</td><td>当对类型(K, V)和(K, W)的数据集调用时，返回一个元组(K， (Iterable， Iterable))的数据集。这个操作也称为groupWith。</td></tr><tr><td>cartesian(otherDataset)</td><td>当对T和U类型的数据集调用时，返回一个(T, U)对的数据集(所有元素对)。</td></tr><tr><td>pipe(command, [envVars])</td><td>通过shell命令(例如Perl或bash脚本)管道传输RDD的每个分区。RDD元素被写入到进程的stdin中，并以字符串的RDD形式返回到它的stdout中的行输出。</td></tr><tr><td>coalesce(numPartitions)</td><td>将RDD中的分区数减少到numPartitions。用于筛选大型数据集后更有效地运行操作。</td></tr><tr><td>repartition(numPartitions)</td><td>随机重组RDD中的数据，创建更多或更少的分区，并在这些分区之间进行平衡。这总是在网络上对所有数据进行无序处理。</td></tr><tr><td>repartitionAndSortWithinPartitions(partitioner)</td><td>根据给定的分区器重新分区RDD，并在每个结果分区中按关键字对记录进行排序。这比在每个分区内调用重新分区然后进行排序更有效，因为它可以将排序向下推到无序处理机制中。</td></tr></tbody></table><h2 id="行动算子-Actions"><a href="#行动算子-Actions" class="headerlink" title="行动算子(Actions)"></a>行动算子(Actions)</h2><p>行动算子从功能上来说作为一个触发器，会触发提交整个作业并开始执行。从代码上来说，它与转换算子的最大不同之处在于：转换算子返回的还是 RDD，行动算子返回的是非 RDD 类型的值，如整数，或者根本没有返回值。</p><table><thead><tr><th>Actions</th><th>Description</th></tr></thead><tbody><tr><td>reduce(func)</td><td>使用函数func（接受两个参数并返回一个）聚合数据集的元素。函数应该是可交换的和相联的，从而可以并行计算</td></tr><tr><td>collect()</td><td>在驱动程序中将数据集的所有元素作为数组返回。这通常在过滤器或其他返回足够小的数据子集的操作之后有用。</td></tr><tr><td>count()</td><td>返回数据集中元素的数量。</td></tr><tr><td>first()</td><td>返回数据集的第一个元素(类似于take(1))。</td></tr><tr><td>take(n)</td><td>返回一个包含数据集前n个元素的数组。</td></tr><tr><td>takeSample(withReplacement, num, [seed])</td><td>返回数据集num元素的随机样本数组，可选地预先指定随机数生成器种子，是否进行替换。</td></tr><tr><td>takeOrdered(n, [ordering])</td><td>使用自然顺序或自定义比较器返回RDD的前n个元素。</td></tr><tr><td>saveAsTextFile(path)</td><td>将数据集的元素作为文本文件(或一组文本文件)写入本地文件系统、HDFS或任何其他hadoop支持的文件系统的给定目录中。Spark将对每个元素调用toString，将其转换为文件中的一行文本。</td></tr><tr><td>saveAsSequenceFile(path)(Java and Scala)</td><td>在本地文件系统、HDFS或任何其他Hadoop支持的文件系统的给定路径中，将数据集的元素作为Hadoop序列文件编写。这在实现Hadoop可写接口的键值对RDDs上可用。在Scala中，它还可以用于隐式转换为可写的类型(Spark包括基本类型的转换，如Int、Double、String等)。</td></tr><tr><td>saveAsObjectFile(path)(Java and Scala)</td><td>使用Java序列化以简单的格式编写数据集的元素，然后可以使用SparkContext.objectFile()加载这些元素。</td></tr><tr><td>countByKey()</td><td>只在类型(K, V)的RDDs上可用。返回一个(K, Int)对的hashmap，并记录每个键的计数。</td></tr><tr><td>foreach(func)</td><td>对数据集的每个元素运行函数func。这通常是为了避免副作用，如更新累加器或与外部存储系统交互。注意：在foreach（）之外修改除累加器以外的变量可能会导致未定义的行为。有关更多详细信息，请参见理解闭包。</td></tr></tbody></table><h1 id="spark-SQL"><a href="#spark-SQL" class="headerlink" title="spark SQL"></a>spark SQL</h1><p>SparkSession中所有功能的入口点是SparkSession类.使用:SparkSessionSparkSession.builder()创建SparkSession对象</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Spark SQL example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">spark学习记录</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="spark" scheme="https://s-luping.github.io/luping/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习记录</title>
    <link href="https://s-luping.github.io/luping/2021/04/02/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://s-luping.github.io/luping/2021/04/02/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2021-04-01T20:15:14.000Z</published>
    <updated>2022-03-13T12:12:29.654Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop模块"><a href="#Hadoop模块" class="headerlink" title="Hadoop模块"></a>Hadoop模块</h1><p><strong>common 公共模块</strong> <strong>HDFS 文件存储</strong> <strong>YARN 资源管理</strong> <strong>MapReduce 计算框架</strong></p><h1 id="Hadoop集群安装部署"><a href="#Hadoop集群安装部署" class="headerlink" title="Hadoop集群安装部署"></a>Hadoop集群安装部署</h1><h2 id="虚拟机配置-linux网络配置"><a href="#虚拟机配置-linux网络配置" class="headerlink" title="虚拟机配置 linux网络配置"></a>虚拟机配置 linux网络配置</h2><p>1.修改主机名称 &#x2F;etc&#x2F;hostname<br>将克隆的2、3主机分别改名为hadoop02、hadoop03<br>2.主机名和ip映射配置 此处设置IP时注意</p><p><img src="https://img-blog.csdnimg.cn/f988601e2c4547d3ab810d33bd6230b6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>Host文件配置 三台虚拟机之间通信名称代替ip</p><p><img src="https://img-blog.csdnimg.cn/9292a3493936478692a62b93ea359383.png" alt="在这里插入图片描述"></p><p>2.网络参数配置 配置静态ip</p><p><img src="https://img-blog.csdnimg.cn/7ad6752ab0764ced83d872b97417331e.png" alt="在这里插入图片描述"></p><p>3.测试网卡配置</p><p>若修改vmware默认初始网段,出现无法ping通外网在上图虚拟网络编辑器还原默认配置,使用还原后的网段即可.</p><p><img src="https://img-blog.csdnimg.cn/3dc896681a3445719df76d9cc8ba164c.png" alt="在这里插入图片描述"></p><h2 id="SSH服务配置-免密登录"><a href="#SSH服务配置-免密登录" class="headerlink" title="SSH服务配置  免密登录"></a>SSH服务配置  免密登录</h2><p>1.生成私匙和公匙</p><p><img src="https://img-blog.csdnimg.cn/4478c537a6084964bc7d0914215f9258.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>将共匙 加入authorized_keys 文件（如没有touch创建该文件）  实现自我登录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/3770934d7cba49108c128659616aa0f5.png" alt="在这里插入图片描述"></p><p>复制共匙到hadoop02和hadoop03 实现免密登录</p><p><img src="https://img-blog.csdnimg.cn/1d385a50bb2f421e8cb664460b2739e5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="防火墙配置"><a href="#防火墙配置" class="headerlink" title="防火墙配置"></a>防火墙配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=9000/tcp --permanent</span><br><span class="line">firewall-cmd --zone=public --add-port=50075/tcp --permanent</span><br><span class="line">firewall-cmd --zone=public --add-port=8088/tcp --permanent</span><br></pre></td></tr></table></figure><p>hdfs 9000 50070 50010</p><p>yarn 8030 8031 8032 8088</p><p>journalnode 8485</p><p>zookeeper 2181 2888 3888 </p><p>放开端口需重载防火墙配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure><p>查看一下开放的端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --list-ports</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/d93363b7c7014241a918ec9641a558b1.png" alt="在这里插入图片描述"></p><p>常用的端口如下</p><p><img src="https://img-blog.csdnimg.cn/9cc4cbe16e244f6a980367b5a2632a3c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="Jdk安装"><a href="#Jdk安装" class="headerlink" title="Jdk安装"></a>Jdk安装</h2><p>上传</p><p><img src="https://img-blog.csdnimg.cn/531d1c9393f04875b182856bc815b471.png" alt="在这里插入图片描述"></p><p>解压</p><p><img src="https://img-blog.csdnimg.cn/38d4a2e9ddbe4ba48d97be12869bbaa1.png" alt="在这里插入图片描述"></p><p>重命名</p><p><img src="https://img-blog.csdnimg.cn/e6043ba8114b4bc8b7f872981008e3d3.png" alt="在这里插入图片描述"></p><h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><p><img src="https://img-blog.csdnimg.cn/ba1403763ce2410794874c1027e7d680.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>验证安装完成</p><p><img src="https://img-blog.csdnimg.cn/d6a70a5e9fee4de6a0d24534c60f8e4a.png" alt="在这里插入图片描述"></p><h2 id="Hadoop安装和集群配置"><a href="#Hadoop安装和集群配置" class="headerlink" title="Hadoop安装和集群配置"></a>Hadoop安装和集群配置</h2><p>上传</p><p><img src="https://img-blog.csdnimg.cn/6347d7b094a243678d1bac8176d2013e.png" alt="在这里插入图片描述"></p><p>解压</p><p><img src="https://img-blog.csdnimg.cn/6ad19bd2f4834b70b5ee2b0bb0a4f3e7.png" alt="在这里插入图片描述"></p><h3 id="配置环境变量-1"><a href="#配置环境变量-1" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p><img src="https://img-blog.csdnimg.cn/11e9c32c3c8b438694c2412ae5444d2e.png" alt="在这里插入图片描述"></p><p>需要注意的是配置环境变量的时候若有两个path 一定要记得两个都要在前面加＄符号</p><p>验证安装成功</p><p><img src="https://img-blog.csdnimg.cn/d345fbb76c354736aeff6b0b22aad7ae.png" alt="在这里插入图片描述"></p><h3 id="主节点配置文件"><a href="#主节点配置文件" class="headerlink" title="主节点配置文件"></a>主节点配置文件</h3><p><strong>core-site.xml</strong></p><p><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/core-default.xml">官网core-default</a><br><img src="https://img-blog.csdnimg.cn/0d565e3ea58f4ca8aa8f546ba7bece24.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>高可用ha配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/export/data/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>ha001:2181,ha002:2181,ha003:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--ipc超时重试次数和间隔--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>20000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>hdfs-site.xml</strong></p><p><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">官网hdfs-default</a><br><img src="https://img-blog.csdnimg.cn/5ce8d16531714c4ea0345ba142c9b7b6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>高可用ha配置 </p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span>href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--三台节点--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--元数据信息位置--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/export/data/hadoop/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--数据位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/export/data/hadoop/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--开启WEB-HDFS--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--ha集群名称--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--两台namenode名称--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--nn1的通信地址--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--RPC通信地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>ha001:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--http通信地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>ha001:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--nn2的通信地址--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--RPC通信地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>ha002:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--http通信地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>ha002:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--NAMENODE的元数据在journalnode的存放位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://ha001:8485;ha002:8485;ha003:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--指定journal在本地磁盘的存放位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/export/data/hadoop/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--开启namenode失败自动切换--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--隔离机制自动切换时登录第二台namenode --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">         sshfence</span><br><span class="line">         shell(/bin/true)</span><br><span class="line">      <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> </span><br><span class="line">  <span class="comment">&lt;!--journal连接配置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.qjournal.start-segment.timeout.ms<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>20000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.qjournal.select-input-streams.timeout.ms<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>20000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.qjournal.write-txns.timeout.ms<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>20000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>Mapred-site.xml</strong></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>6144<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx3072m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx6144m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">官网mapred-site</a><br><strong>yarn-site.xml</strong><br>资源管理器负责配置调控CPU，内存，磁盘等分配和使用<br>分为节点资源管理和任务资源分配<br>单节点推荐内存设置如下(主要为系统预留)<br><img src="https://img-blog.csdnimg.cn/98939f3c674840b482478cf33bf6750e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>每个container则根据任务需要设置最大最小分配资源</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--是否将对容器实施物理内存限制--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--是否将对容器实施虚拟内存限制--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--物理核心和虚拟核心比率--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--将逻辑处理器（例如超线程）视为核心--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.count-logical-processors-as-cores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--scheduler给每个容器可分配资源--&gt;</span></span><br><span class="line">  <span class="comment">&lt;!------表示每个container的最大物理内存-------&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>3072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>256<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!------表示每个container的最大CPU核心数-------&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--单个节点可用资源--&gt;</span></span><br><span class="line">  <span class="comment">&lt;!------表示该节点可使用的物理内存-------&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>2400<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!------表示该节点可使用的CPU核心数-------&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">&lt;!--resourcemanager地址--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>ha001<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>ha002<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定zookeeper集群地址--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>ha001:2181,ha002:2181,ha003:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">&lt;!--开启resourcemanager高可用--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定cluster ID--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>yrc<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yran常用参数参考</a><br><strong>slaves</strong><br><img src="https://img-blog.csdnimg.cn/1fa9f9b0100540bda5394c2b6f304878.png" alt="在这里插入图片描述"></p><p>将主节点内容分发到子节点  分发profile配置</p><p><img src="https://img-blog.csdnimg.cn/7613aa9ad98248f686a9f33fb22984cb.png" alt="在这里插入图片描述"></p><h2 id="zookeeper安装并配置"><a href="#zookeeper安装并配置" class="headerlink" title="zookeeper安装并配置"></a>zookeeper安装并配置</h2><p>1.下载并解压<br><img src="https://img-blog.csdnimg.cn/c7622c1677c942d497ff734bf56a282a.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2f20dd10022d4e69ae9a5968d286f755.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="配置zoo-cfg"><a href="#配置zoo-cfg" class="headerlink" title="配置zoo.cfg"></a>配置zoo.cfg</h3><p><img src="https://img-blog.csdnimg.cn/bc7ef5dadb7c4bad8e8ac474509cdda2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="添加myid文件"><a href="#添加myid文件" class="headerlink" title="添加myid文件"></a>添加myid文件</h3><p><img src="https://img-blog.csdnimg.cn/d2cdaa9d8d1e4dba882ce3341b408b10.png" alt="在这里插入图片描述"></p><p>Ha-01输入值1，ha-02输入值为2，ha-03输入值为3</p><h2 id="Hadoop集群启动测试"><a href="#Hadoop集群启动测试" class="headerlink" title="Hadoop集群启动测试"></a>Hadoop集群启动测试</h2><p>1.启动各个节点的zookeeper服务 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/f4f4ebf36370404482edfc9c4ef1a7fc.png" alt="在这里插入图片描述"></p><p>2.启动集群监控namenode的管理日志journalNode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemons.sh start journalnode </span><br><span class="line"><span class="comment"># 说明：该命令不推荐使用，系统会提示使用新的命令</span></span><br><span class="line">hdfs –daemon start journalnode</span><br></pre></td></tr></table></figure><p>可以不用单独启动，在启动hadoop集群的时候会自动启动（如果配置了的journalnode情况）</p><p><img src="https://img-blog.csdnimg.cn/8e80e7cbcea643c3a465f566b7ed2899.png" alt="在这里插入图片描述"></p><p>3.在node-01上格式化namenode，并分发到node-02</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop namenode –format </span><br><span class="line"><span class="comment"># 提示用：hdfs namenode -format</span></span><br><span class="line"><span class="comment"># 注意：只分发master和backupmaster </span></span><br><span class="line">scp –r /export/data/hadoop node-02:/export/data</span><br></pre></td></tr></table></figure><p>若初始化时出现下面错误</p><p><img src="https://img-blog.csdnimg.cn/50bb38befb3f48dbafd5470c3dbf320a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>错误原因：<br>我们在执行start-dfs.sh的时候，默认启动顺序是namenode&gt;datanode&gt;journalnode&gt;zkfc，如果journalnode和namenode不在一台机器启动的话，很容易因为网络延迟问题导致namenode无法连接journalnode，无法实现选举，最后导致刚刚启动的namenode会突然挂掉。虽然namenode启动时有重试机制等待journalnode的启动，但是由于重试次数限制，可能网络情况不好，导致重试次数用完了，也没有启动成功。</p><p>解决方法：</p><p>方法①：手动启动namenode，避免了网络延迟等待journalnode的步骤，一旦两个namenode连入journalnode，实现了选举，则不会出现失败情况。</p><p>方法②：先启动journalnode然后再运行start-dfs.sh。</p><p>方法③：把namenode对journalnode的容错次数或时间调成更大的值，保证能够对正常的启动延迟、网络延迟能容错。在hdfs-site.xml中修改ipc参数，namenode对journalnode检测的重试次数，默认为10次，每次1000ms，故网络情况差需要增加。具体修改信息为：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">description</span>&gt;</span>Indicates the number of retries a client will make to establish</span><br><span class="line">      a server connection.</span><br><span class="line">     <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Indicates the number of milliseconds a client will wait for</span><br><span class="line">  before retrying to establish a server connection.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/weixin_43482612/article/details/109556284">原文链接</a></p><p>4.在node-01上格式化ZKFC  这个命令必须自己敲出来不能复制</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc –formatZK</span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>结果 3台均正常启动dfs 和yarn<br><img src="https://img-blog.csdnimg.cn/19e1db8e52974713bc0b352402008be6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>如果有漏掉的机器没有启动 则可以用 在漏掉的机器上执行启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode </span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line">yarn-daemon.sh start secondarymanager</span><br><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><h1 id="hadoop-job执行流程"><a href="#hadoop-job执行流程" class="headerlink" title="hadoop job执行流程"></a>hadoop job执行流程</h1><p><img src="https://img-blog.csdnimg.cn/5f68d2ac44e94ae8864dedd6ff6b4118.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>dataInput–&gt;split–&gt;Mapper–&gt;Combine–&gt;(产出临时数据–&gt;Partition–&gt;Sort–&gt;Reducer–&gt;最终数据。</p><h2 id="Mapper阶段"><a href="#Mapper阶段" class="headerlink" title="Mapper阶段"></a>Mapper阶段</h2><p>Mapper的数量由输入的大小和个数决定。在默认情况下，最终input占据了多少block，就应该启动多少个Mapper。500M的数据分成四个block（128M*4）就是4个mapper。</p><h2 id="分区-排序-溢写-文件合并"><a href="#分区-排序-溢写-文件合并" class="headerlink" title="分区 排序 溢写 文件合并"></a>分区 排序 溢写 文件合并</h2><p>partition默认分区  分区器是HashPartitioner  对numReduceTasks取模，模数相同分到同一分区，对key进行排序，当内存缓冲区达到阈值进行溢写到磁盘，产生的多个小文件将合并为一个大文件。分区对应reduce；<a href="https://blog.csdn.net/qq_35699475/article/details/75582072">参考文章</a><br><img src="https://img-blog.csdnimg.cn/842a74b920fd4ae8b1ea97dd47ead6b0.png" alt="在这里插入图片描述"></p><h2 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h2><p>Reducer将与一个key关联的一组中间数值集归为一个更小的数值集。它的数据来源可能是多个mapper的某个分区，过程就要进行shuffle，然后对收集到的key进行合并；<br>reduce的数量可以直接在程序设置job.setNumReduceTasks属性设置</p><h1 id="HDFS的Checkpoint"><a href="#HDFS的Checkpoint" class="headerlink" title="HDFS的Checkpoint"></a>HDFS的Checkpoint</h1><p>Checkpoint（检查点）：因为数据库系统或者像HDFS这样的分布式文件系统，对文件数据的修改不是直接写回到磁盘的，很多操作是先缓存到内存的Buffer中，当遇到一个检查点Checkpoint时，系统会强制将内存中的数据写回磁盘，当然此时才会记录日志，从而产生持久的修改状态。</p><p>在介绍Checkpoint之前，先来看看Namenode上面有些什么数据：</p><p><strong>edits</strong> HDFS操作的日志记录，没此对HDFS进行修改操作后，都会往edits中记录一条日志；</p><p><strong>fsimage</strong> HDFS中命名空间、数据块分布、文件属性等信息都存放在fsimage中；</p><p>edits是在每次修改HDFS时都会插入记录，那么fsimage则在整个HDFS运行期间不会产生变化，用HDFS官方文档的说法就是：NameNode merges fsimage and edits files only during start up。也就是说，只有在每次启动Namenode时，才会把edits中的操作增加到fsimage中，并且把edits清空。所以fsimage总是记录启动Namenode时的状态，而edits在每次启动时也是空的，它只记录本次启动后的操作日志。</p><h2 id="为什么需要checkpoint？"><a href="#为什么需要checkpoint？" class="headerlink" title="为什么需要checkpoint？"></a>为什么需要checkpoint？</h2><p>按照fsimage和edits的工作机制，在一次启动后，edits的文件可能会增长到很大，这样在下次启动Namenode时需要花费很长时间来恢复；<br>另一方面，如果在HDFS运行过程中发生Namenode的故障，那么edits中的记录就会丢失。所以，我们需要利用Checkpoint即使将修改操作持久化。</p><h2 id="checkpoint触发条件"><a href="#checkpoint触发条件" class="headerlink" title="checkpoint触发条件"></a>checkpoint触发条件</h2><p>在配置文件中的参数：</p><p>时间维度，默认一小时触发一次工作流程 dfs.namenode.checkpoint.period ：3600</p><p>次数维度，默认100万次触发一次工作流程 dfs.namenode.checkpoint.txns ： 1000000</p><p>大小维度，默认64M触发一次工作流程 fs.checkpoint.size：67108864。</p><p>也就说触发HDFS中Checkpoint的机制有三种，一是时间、次数和日志的大小</p><h2 id="checkpoint做了什么"><a href="#checkpoint做了什么" class="headerlink" title="checkpoint做了什么"></a>checkpoint做了什么</h2><p>Chekpoint主要干的事情是，将Namenode中的edits和fsimage文件拷贝到Second Namenode上，然后将edits中的操作与fsimage文件merge以后形成一个新的fsimage，这样不仅完成了对现有Namenode数据的备份，而且还产生了持久化操作的fsimage。</p><p>最后一步，Second Namenode需要把merge后的fsimage文件upload到Namenode上面，完成Namenode中fsimage的更新。<br>以上提到的文件都可以在hadoop系统的data目录下找到。<br><a href="https://blog.csdn.net/angelofmersy/article/details/40959039">原文</a></p><h3 id="shared-edits-dir-日志文件位置设置"><a href="#shared-edits-dir-日志文件位置设置" class="headerlink" title="shared.edits.dir 日志文件位置设置"></a>shared.edits.dir 日志文件位置设置</h3><p>当集群为高可用集群时standbynamenode会读取该目录下edits文件并与fsimage合并为新的fsimage</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--NAMENODE的元数据在journalnode的存放位置--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://ha001:8485;ha002:8485;ha003:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定journalnode在本地磁盘的存放位置--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/export/data/hadoop/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="fsimage-fsimage文件位置"><a href="#fsimage-fsimage文件位置" class="headerlink" title="fsimage fsimage文件位置"></a>fsimage fsimage文件位置</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--元数据信息位置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/export/data/hadoop/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="利用副本机制故障还原"><a href="#利用副本机制故障还原" class="headerlink" title="利用副本机制故障还原"></a>利用副本机制故障还原</h2><p>1.删掉Active NameNode的FSimage和Edits_Log模拟数据丢失<br>记录NN存储 和Edits_Log的路径</p><p>2.将Standby NameNode的FSimage和Edits_Log复制到NN的FSimage和Edits_Log对应的目录下</p><p>3.启动挂掉的NameNode<br><a href="https://blog.csdn.net/weixin_44704605/article/details/110946336">原文</a></p><h1 id="HDFS中的fsck命令"><a href="#HDFS中的fsck命令" class="headerlink" title="HDFS中的fsck命令"></a>HDFS中的fsck命令</h1><p>查看文件目录的健康信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /weblog</span><br></pre></td></tr></table></figure><p>查看文件中损坏的块 (-list-corruptfileblocks)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /weblog -list-corruptfileblocks</span><br></pre></td></tr></table></figure><h2 id="损坏文件的处理"><a href="#损坏文件的处理" class="headerlink" title="损坏文件的处理"></a>损坏文件的处理</h2><p>将损坏的文件移动至&#x2F;lost+found目录 (-move)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /user/hadoop-twq/cmd -move</span><br></pre></td></tr></table></figure><p>删除有损坏数据块的文件 (-delete)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /user/hadoop-twq/cmd -delete</span><br></pre></td></tr></table></figure><h2 id="打印文件的Block报告-blocks"><a href="#打印文件的Block报告-blocks" class="headerlink" title="打印文件的Block报告(-blocks)"></a>打印文件的Block报告(-blocks)</h2><p>执行下面的命令，可以查看一个指定文件的所有的Block详细信息，需要和-files一起使用：　</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /user/hadoop-twq/cmd/big_file.txt -files -blocks</span><br></pre></td></tr></table></figure><p>如果，我们在上面的命令再加上-locations的话，就是表示还需要打印每一个数据块的位置信息，如下：<br><img src="https://img-blog.csdnimg.cn/2cf4f6f42cd04325b61664e0d2ef924c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h1 id="hdfs-haadmin-命令"><a href="#hdfs-haadmin-命令" class="headerlink" title="hdfs haadmin 命令"></a>hdfs haadmin 命令</h1><h2 id="transitionToActive"><a href="#transitionToActive" class="headerlink" title="-transitionToActive "></a>-transitionToActive <namenodeid></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line"><span class="comment">#nn1 -&gt; active</span></span><br><span class="line">hdfs haadmin -transitionToActive -forcemanual nn1</span><br><span class="line"><span class="comment">#nn2 -&gt; standby</span></span><br><span class="line">hdfs haadmin -transitionToStandby -forcemanual nn1</span><br></pre></td></tr></table></figure><h2 id="getServiceState"><a href="#getServiceState" class="headerlink" title="-getServiceState "></a>-getServiceState <serviceId></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line">hdfs haadmin -getServiceState nn1</span><br></pre></td></tr></table></figure><h2 id="checkHealth"><a href="#checkHealth" class="headerlink" title="-checkHealth "></a>-checkHealth <serviceId></h2>]]></content>
    
    
    <summary type="html">Hadoop学习记录</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="Hadoop" scheme="https://s-luping.github.io/luping/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>练习从数据采集、分析到展示的过程</title>
    <link href="https://s-luping.github.io/luping/2021/03/26/%E7%BB%83%E4%B9%A0%E4%B8%80%E4%B8%AA%E4%BB%8E%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%88%86%E6%9E%90%E5%88%B0%E5%B1%95%E7%A4%BA%E7%9A%84%E8%BF%87%E7%A8%8B/"/>
    <id>https://s-luping.github.io/luping/2021/03/26/%E7%BB%83%E4%B9%A0%E4%B8%80%E4%B8%AA%E4%BB%8E%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%88%86%E6%9E%90%E5%88%B0%E5%B1%95%E7%A4%BA%E7%9A%84%E8%BF%87%E7%A8%8B/</id>
    <published>2021-03-26T15:50:52.000Z</published>
    <updated>2022-03-13T12:25:25.714Z</updated>
    
    <content type="html"><![CDATA[<h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p>学了Python爬虫 文本分析 又看到学校上热搜 就写了个舆论监测的东西<br>#结果展示<br><strong>首页数据总览</strong><br><img src="https://img-blog.csdnimg.cn/20210313222235204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>热度排行榜TOP10</strong><br><img src="https://img-blog.csdnimg.cn/20210315233432230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>趋势观察</strong><br><img src="https://img-blog.csdnimg.cn/20210313222449253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210313222511515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>词云展示</strong><br><img src="https://img-blog.csdnimg.cn/20210313222612656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>lda话题分析</strong><br><img src="https://img-blog.csdnimg.cn/20210313222708167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h1><p><strong>数据采集、数据分析、数据展示</strong></p><p>信息来源都是面向公众的媒体平台，像微博、贴吧、知乎、微信这些，主要搜集关于某个主题文本信息。<br>爬下来的信息做了些初步的统计信息，和一些简单分析如上图。</p><h2 id="采集数据"><a href="#采集数据" class="headerlink" title="采集数据"></a>采集数据</h2><p>用python写的爬虫</p><h3 id="爬虫结构"><a href="#爬虫结构" class="headerlink" title="爬虫结构"></a>爬虫结构</h3><p><img src="https://img-blog.csdnimg.cn/20210313231506641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>用到的库</p><p><img src="https://img-blog.csdnimg.cn/20210313224218588.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>这几个平台的都有相应的反扒措施，但是我爬的都是大家都能看到的不违法，而且我用一台服务器每隔四个小时爬取一次，不会造成多大影响<br>于是我看了其他一些反反扒的文章抓到了数据</p><h2 id="模块介绍"><a href="#模块介绍" class="headerlink" title="模块介绍"></a>模块介绍</h2><p><strong>download模块</strong></p><p>通用下载模块，对网页内容下载</p><p><strong>parser模块</strong></p><p>请求到的信息格式微博、知乎是json格式，只需json.loads下来取某个key的values即可，微信、贴吧等是html网页源码格式，我使用的是BeautifulSoup库，soup.find_all()很顺手，使用lxml库的etree的xpath语法虽更简单，但是有时因为一个元素去改整个得到list很是麻烦</p><p><strong>dataoutput</strong></p><p>数据储存，我用的是mysql</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 数据统一格式</span><br><span class="line">mdata = <span class="punctuation">&#123;</span></span><br><span class="line">    &#x27;plantform&#x27;<span class="punctuation">:</span> &#x27;weibo&#x27;<span class="punctuation">,</span> </span><br><span class="line">    &#x27;mtimestamp&#x27;<span class="punctuation">:</span> create_time<span class="punctuation">,</span> </span><br><span class="line">    &#x27;hotnum&#x27;<span class="punctuation">:</span> hotnum<span class="punctuation">,</span> </span><br><span class="line">    &#x27;url&#x27;<span class="punctuation">:</span> url<span class="punctuation">,</span> </span><br><span class="line">    &#x27;title&#x27;<span class="punctuation">:</span> content<span class="punctuation">,</span></span><br><span class="line">    &#x27;comments&#x27;<span class="punctuation">:</span> comments</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p><strong>spidermain</strong></p><p>调度模块，将前面三个模块综合起来</p><p><strong>爬虫这里遇到的问题：如何设计增量爬取，如何保证数据质量</strong></p><p><strong>解决方案</strong></p><p><strong>增量爬取</strong></p><p>每次爬取平台的信息前先查询数据库里最新的一条数据时间，作为参数只请求或保留该时间之后的数据即可</p><p><strong>解决数据质量问题</strong></p><p>我想要的是比如关于我们学校的人物、事件的讨论 就要对爬取的数据处理 比某些微博超话题下的微博内容就是些语气词 或是如 生气 开心这些分析价值不大的给过滤掉 一般小于三字的内容都可以过滤</p><p>时间储存统一使用时间戳，</p><p><strong>代码量</strong>：爬虫部分大概900行代码</p><p>本地调试完成，就可上传Linux服务器，设置定时任务，每天抓取，做为数据分析的基础</p><h2 id="数据处理分析"><a href="#数据处理分析" class="headerlink" title="数据处理分析"></a>数据处理分析</h2><p>爬虫写好并稳定运行后 就开始处理爬取到的数据 数据如下图</p><p><img src="https://img-blog.csdnimg.cn/2021031413532941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="原始数据"></p><p><strong>数据统计</strong> </p><p><strong>分组统计</strong>，</p><p>统计每个平台每天抓取的信息数量，web页面实时展示每个平台当天抓取的信息数量，定时每天凌晨将昨天统计数据写入mysql表，记录为每天各平台历史抓取信息数量，用于观察信息发布趋势变化</p><p><img src="https://img-blog.csdnimg.cn/2021031601040112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>词云</strong> </p><p>查询一段时间内的所有数据，将title、comment字段下的内容查询先使用分词工具jieba分词、去停用词等预处理，得到每条记录的keywords将这些记录丢进模型计算每个词TFIDF值做出词云，可观察这段时间讨论的内容的关键词。<br>其实这个跟下面的lda差不多</p><p><strong>情感分析</strong> </p><p>一是使用snownlp，但是结果很难让人满意。<br>二是使用情感词典(玻森情感词典)，如下，会有11万词的评分，使用使就是遍历该词典，结果得分一般是加和计算。</p><p><img src="https://img-blog.csdnimg.cn/20210314150320310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="珀森词典"><br>我对比后选择了后者。</p><p><strong>话题聚类</strong>  </p><p>将信息进行主题聚类，发现讨论的话题，<br>这里我使用的是lda主题模型，目标是找到每一篇文档的主题分布和每一个主题中词的分布，根据每个主题词的分布，我们加以概括这个主题，该主题下的文章内容我称之为一个讨论话题。<br>再处理文本之前先进行文本预处理，切词、去停用词，标记文档，参数调整，再训练模型即可，过程原理大家感兴趣可以找相关文档看。</p><p><strong>代码量</strong> </p><p>统计分析部分大概500行代码</p><h2 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h2><p>用Python flask 做的web，结合echarts生成一些图，更直观的查看信息发布趋势</p><p><img src="https://img-blog.csdnimg.cn/20210316011330473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>代码量</strong> 该部分不算前端代码只有100行。</p><p><strong>总记代码量</strong> 不到2000行</p><p>每一部分的东西都是之前几个学期学的东西，零零散散组了起来。</p>]]></content>
    
    
    <summary type="html">练习从数据采集、分析到展示的过程</summary>
    
    
    
    <category term="python" scheme="https://s-luping.github.io/luping/categories/python/"/>
    
    
    <category term="python 爬虫 舆论监测" scheme="https://s-luping.github.io/luping/tags/python-%E7%88%AC%E8%99%AB-%E8%88%86%E8%AE%BA%E7%9B%91%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>东方财富股吧标题爬取分析</title>
    <link href="https://s-luping.github.io/luping/2021/03/10/%E4%B8%9C%E6%96%B9%E8%B4%A2%E5%AF%8C%E8%82%A1%E5%90%A7%E6%A0%87%E9%A2%98%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90/"/>
    <id>https://s-luping.github.io/luping/2021/03/10/%E4%B8%9C%E6%96%B9%E8%B4%A2%E5%AF%8C%E8%82%A1%E5%90%A7%E6%A0%87%E9%A2%98%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90/</id>
    <published>2021-03-10T01:46:21.000Z</published>
    <updated>2022-03-13T12:25:25.733Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20210326003708831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>45个股吧，140万条数据库记录<br>日期从2018-03-01至2021-03-01共36个月的股吧帖子，<br>爬取股吧名称、阅读、评论、标题、作者和发帖时间，<br>并分析总体情绪</p><h2 id="亮点回顾"><a href="#亮点回顾" class="headerlink" title="亮点回顾"></a>亮点回顾</h2><p>时间问题<br>获取的时间未加年份，解决方法，观察发现发帖日期月份逐级递减，按获取顺序下一个时间月份在同一年内小于等于上一个月份，设一个变量m储存月份，始值设为12，与获取的最新月份new_m比较，若new_m&gt;m，使当前年份减一；再令m&#x3D;new_m。<br>数据去重问题<br>有时候爬取会因各种问题中断，当你再次续爬时数据会重复，于是我加了一个用于去重的myid<br>myid &#x3D; item[‘username’] + str(item[‘mdate’])[3:-4] + title[:100]<br>思想是，时间地点人物组合，即{<strong>谁</strong>}在{<strong>什么时间</strong>}{<strong>干了什么</strong>}地点没加，但也使每条记录内容保证唯一，大概率去重。<br>考虑过用每个news的url做主键去重，但是一下url是有重复的<br>创建的数据表语句如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> info_guba</span><br><span class="line">(</span><br><span class="line">    id           <span class="type">int</span> auto_increment</span><br><span class="line">        <span class="keyword">primary</span> key,</span><br><span class="line">    myid         <span class="type">varchar</span>(<span class="number">255</span>) <span class="keyword">collate</span> utf8mb4_croatian_ci <span class="keyword">not</span> <span class="keyword">null</span>,</span><br><span class="line">    scans        <span class="type">int</span>(<span class="number">8</span>)                                   <span class="keyword">null</span>,</span><br><span class="line">    comments     <span class="type">int</span>(<span class="number">6</span>)                                   <span class="keyword">null</span>,</span><br><span class="line">    titles       text <span class="keyword">collate</span> utf8mb4_croatian_ci         <span class="keyword">null</span>,</span><br><span class="line">    usernames    <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">collate</span> utf8mb4_croatian_ci  <span class="keyword">null</span>,</span><br><span class="line">    mdates       <span class="type">int</span>(<span class="number">15</span>)                                  <span class="keyword">null</span>,</span><br><span class="line">    f_scores     <span class="type">float</span>(<span class="number">12</span>, <span class="number">10</span>) <span class="keyword">default</span> <span class="number">0.0000000000</span>       <span class="keyword">null</span>,</span><br><span class="line">    polarity     <span class="type">int</span>(<span class="number">1</span>)                                   <span class="keyword">null</span>,</span><br><span class="line">    company_name <span class="type">varchar</span>(<span class="number">80</span>) <span class="keyword">collate</span> utf8mb4_croatian_ci  <span class="keyword">null</span>,</span><br><span class="line">    industry     <span class="type">varchar</span>(<span class="number">80</span>) <span class="keyword">collate</span> utf8mb4_croatian_ci  <span class="keyword">null</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>跨年份取月份对应时间戳问题<br>2018-03至2021-03期间每个月份的情绪指数，要取每个月的一号和下个月的一号时间戳，使能完成区间取值，<br>我的方法<br>将38个月份数值储存在一个列表，遍历列表月份，如果月份没到倒数第二位，判断月份是否为12月，是则变量年份减一，拿日期转换时间戳得到时间段较大值，再判断，该月份的下一个月份是不是12月，是则年份减一，日期转换时间戳得到较小值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">months = [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>,</span><br><span class="line">          <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, </span><br><span class="line">          <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, </span><br><span class="line">          <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line"><span class="keyword">for</span> num, m <span class="keyword">in</span> <span class="built_in">enumerate</span>(months):</span><br><span class="line">    <span class="keyword">if</span> num != <span class="number">36</span>:</span><br><span class="line">        <span class="keyword">if</span> m == <span class="number">12</span>:</span><br><span class="line">        year -= <span class="number">1</span></span><br><span class="line">    max_timestamp = self.date_to_timestamp(year, m)</span><br><span class="line">    <span class="keyword">if</span> months[num + <span class="number">1</span>] == <span class="number">12</span>:</span><br><span class="line">        year02 = year - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        year02 = year</span><br><span class="line">    month = <span class="built_in">str</span>(year02) + <span class="string">&#x27;-&#x27;</span> + <span class="built_in">str</span>(months[num + <span class="number">1</span>])</span><br><span class="line">    min_timestamp = self.date_to_timestamp(year02, months[num + <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>爬虫部分代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /usr/bin/python </span></span><br><span class="line"><span class="comment"># --*-- coding:UTF-8 --*--</span></span><br><span class="line"><span class="comment"># Date 2021/3/18 16:35</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">from</span> snownlp <span class="keyword">import</span> SnowNLP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">guba</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, host, db, user, passwd</span>):</span><br><span class="line">        self.headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, &#x27;</span></span><br><span class="line">                                      <span class="string">&#x27;like Gecko) Chrome/89.0.4389.90 Safari/537.36 Edg/89.0.774.54&#x27;</span>&#125;</span><br><span class="line">        self.host = host</span><br><span class="line">        self.db = db</span><br><span class="line">        self.user = user</span><br><span class="line">        self.passwd = passwd</span><br><span class="line">        self.dataoutput = DataOutput()</span><br><span class="line">        self.ip_num = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取代理</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_new_ip</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.ip_num &lt;= <span class="number">1000</span>:</span><br><span class="line">            ip_port = requests.get(</span><br><span class="line">                <span class="string">&#x27;获取代理的api&#x27;</span>,</span><br><span class="line">                timeout=<span class="number">6</span>)</span><br><span class="line">            ip = ip_port.text.replace(<span class="string">&#x27;\r\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            proxyip = &#123;<span class="string">&quot;http&quot;</span>: <span class="string">&quot;http://&quot;</span> + ip,</span><br><span class="line">                       <span class="string">&quot;https&quot;</span>: <span class="string">&quot;https://&quot;</span> + ip&#125;</span><br><span class="line">            self.ip_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> proxyip</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 移除换行符</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rm_special_letters</span>(<span class="params">self, old_list</span>):</span><br><span class="line">        new_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> old_list:</span><br><span class="line">            i = i.replace(<span class="string">&#x27;\r\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            i = i.replace(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            new_list.append(i)</span><br><span class="line">        <span class="keyword">return</span> new_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将日期格式转换为时间戳</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">date_to_timestamp</span>(<span class="params">self, year, timestr</span>):</span><br><span class="line">        mdate = <span class="built_in">str</span>(year) + <span class="string">&#x27;-&#x27;</span> + timestr</span><br><span class="line">        time_array = time.strptime(mdate, <span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">        news_timestamp = time.mktime(time_array)</span><br><span class="line">        <span class="keyword">return</span> news_timestamp</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取每日热帖  阅读量 评论量 标题 用户 发帖时间</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dangu_pinglun</span>(<span class="params">self, url, company_name, industry</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param 所属板块:</span></span><br><span class="line"><span class="string">        :param 公司名称:</span></span><br><span class="line"><span class="string">        :type url: 股吧首页链接</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">global</span> mtimestamp</span><br><span class="line">        mtimestamp = time.time()</span><br><span class="line">        page = <span class="number">1</span></span><br><span class="line">        year = <span class="number">2021</span></span><br><span class="line">        latest_mounth = <span class="number">12</span></span><br><span class="line">        proxyip = self.get_new_ip()</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            datalist = []</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">if</span> page % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="comment">#每50页换一次代理ip 防止反扒</span></span><br><span class="line">                    <span class="built_in">print</span>(company_name, page, <span class="string">&quot;----&quot;</span> + <span class="built_in">str</span>(time.time()))</span><br><span class="line">                    proxyip = self.get_new_ip()</span><br><span class="line">                <span class="comment">#拼接url</span></span><br><span class="line">                murl = url + <span class="built_in">str</span>(page) + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line">                resp = requests.get(murl, headers=self.headers, proxies=proxyip, timeout=<span class="number">10</span>)</span><br><span class="line">                <span class="comment"># print(resp.text)</span></span><br><span class="line">                htmltree = etree.HTML(resp.text)</span><br><span class="line">                yuedu_count = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l1 a1&quot;]/text()&#x27;</span>)</span><br><span class="line">                yuedu_count = self.rm_special_letters(yuedu_count)[<span class="number">1</span>:]</span><br><span class="line">                pinglun_count = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l2 a2&quot;]/text()&#x27;</span>)</span><br><span class="line">                pinglun_count = self.rm_special_letters(pinglun_count)[<span class="number">1</span>:]</span><br><span class="line">                title_list = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l3 a3&quot;]/a/@title&#x27;</span>)</span><br><span class="line">                username_list = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l4 a4&quot;]/a//text()&#x27;</span>)</span><br><span class="line">                last_time_list = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l5 a5&quot;]/text()&#x27;</span>)[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 此处将评论列表保存到字典 交给dataoutput储存</span></span><br><span class="line">                <span class="keyword">for</span> num, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(pinglun_count):</span><br><span class="line">                    <span class="comment"># 当阅读数量含有汉字时</span></span><br><span class="line">                    <span class="keyword">if</span> re.search(<span class="string">&#x27;[\u4e00-\u9fa5]&#x27;</span>, yuedu_count[num]):</span><br><span class="line">                        yuedu_count[num] = <span class="number">20000</span></span><br><span class="line">                    <span class="keyword">if</span> re.search(<span class="string">&#x27;[\u4e00-\u9fa5]&#x27;</span>, pinglun_count[num]):</span><br><span class="line">                        pinglun_count[num] = <span class="number">20000</span></span><br><span class="line">                    <span class="comment"># 截取时间具体提到天 去掉时分时间</span></span><br><span class="line">                    lastdate = last_time_list[num].split(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">                    <span class="comment">#发帖时间递减 ，当下层月份大于上边时年份减一</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">int</span>(lastdate.split(<span class="string">&#x27;-&#x27;</span>)[<span class="number">0</span>]) &gt; latest_mounth:</span><br><span class="line">                        year -= <span class="number">1</span></span><br><span class="line">                    mtimestamp = self.date_to_timestamp(year, lastdate)</span><br><span class="line">                    info_dict = &#123;<span class="string">&#x27;scan&#x27;</span>: yuedu_count[num],</span><br><span class="line">                                 <span class="string">&#x27;comment_num&#x27;</span>: pinglun_count[num],</span><br><span class="line">                                 <span class="string">&#x27;title&#x27;</span>: title_list[num],</span><br><span class="line">                                 <span class="string">&#x27;username&#x27;</span>: username_list[num],</span><br><span class="line">                                 <span class="string">&#x27;mdate&#x27;</span>: mtimestamp,</span><br><span class="line">                                 <span class="string">&#x27;company&#x27;</span>: company_name,</span><br><span class="line">                                 <span class="string">&#x27;industry&#x27;</span>: industry&#125;</span><br><span class="line">                    datalist.append(info_dict)</span><br><span class="line">                    latest_mounth = <span class="built_in">int</span>(lastdate.split(<span class="string">&#x27;-&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">                page += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 将存库语句写到这里是为了个别字节数据储存终端而导致总程序终段</span></span><br><span class="line">                self.dataoutput.write_to_mysql(host=self.host, db=self.db, user=self.user, passwd=self.passwd,</span><br><span class="line">                                               datalist=datalist)</span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(industry, company_name, page, <span class="string">&quot;---&quot;</span> + <span class="built_in">str</span>(time.time()))</span><br><span class="line">                <span class="built_in">print</span>(<span class="built_in">str</span>(e))</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;HTTPConnectionPool&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                    proxyip = self.get_new_ip()</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;index out of range&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                    page += <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> <span class="string">&#x27;day is out of range for month&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                    page += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 此处判断总时间是否到达最大时间  即到2018年3月终止 爬取下一个</span></span><br><span class="line">            <span class="keyword">if</span> mtimestamp &lt;= <span class="number">1521475200</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;时间到&#x27;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataOutput</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__tablename = <span class="string">&#x27;info_guba&#x27;</span></span><br><span class="line">        self.__tablekeys = <span class="string">&#x27;(myid,scans,comments,titles,usernames,mdates,f_scores,company_name,industry)&#x27;</span></span><br><span class="line"><span class="comment">#删除特殊字符 以防引起mysql异常</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rm_special_letter</span>(<span class="params">self, line</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="string">&quot;\&#x27;&quot;</span>, <span class="string">&quot;\&quot;&quot;</span>, <span class="string">&quot;#&quot;</span>, <span class="string">&quot;\\&quot;</span>]:</span><br><span class="line">            line = line.replace(i, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> line</span><br><span class="line">        </span><br><span class="line"><span class="string">&quot;&quot;&quot;借助snownlp</span></span><br><span class="line"><span class="string">    分析news的情绪分为3级 0：积极  1：中立  2：消极&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feeling</span>(<span class="params">self, line</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            res = SnowNLP(line)</span><br><span class="line">            f_score = res.sentiments</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            f_score = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> f_score</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__rm_stopwords</span>(<span class="params">self, wordlist</span>):</span><br><span class="line">        new_wordlist = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tool_files/stopwords.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> r:</span><br><span class="line">            stopwords = r.read()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> wordlist:</span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">in</span> stopwords:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_wordlist.append(i)</span><br><span class="line">            <span class="keyword">return</span> new_wordlist</span><br><span class="line">            </span><br><span class="line"><span class="string">&quot;&quot;&quot;使用玻森情感词典 计算情绪指数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feeling2</span>(<span class="params">self, line</span>):</span><br><span class="line">        path = <span class="string">&quot;tool_files/sentiment_score.txt&quot;</span></span><br><span class="line">        df = pd.read_table(path, sep=<span class="string">&quot; &quot;</span>, names=[<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;score_snownlp&#x27;</span>])</span><br><span class="line">        key = df[<span class="string">&#x27;key&#x27;</span>].values.tolist()</span><br><span class="line">        score = df[<span class="string">&#x27;score_snownlp&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">getscore</span>(<span class="params">line</span>):</span><br><span class="line">            segs = jieba.lcut(line)  <span class="comment"># 分词</span></span><br><span class="line">            jieba.load_userdict(<span class="string">&#x27;tool_files/userdict.txt&#x27;</span>)</span><br><span class="line">            segs = self.__rm_stopwords(segs)</span><br><span class="line">            score_list = [score[key.index(x)] <span class="keyword">for</span> x <span class="keyword">in</span> segs <span class="keyword">if</span> (x <span class="keyword">in</span> key)]</span><br><span class="line">            <span class="comment"># 修改后的sentiment_score.txt 得分有的为字符串格式不能直接使用sum求和</span></span><br><span class="line">            <span class="comment"># print(score_list)</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(score_list) != <span class="number">0</span>:</span><br><span class="line">                sums = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> score_list:</span><br><span class="line">                    sums = sums + <span class="built_in">float</span>(i)</span><br><span class="line">                <span class="keyword">return</span> sums / <span class="built_in">len</span>(score_list)  <span class="comment"># 计算得分</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        last_score = getscore(line)</span><br><span class="line">        <span class="keyword">if</span> last_score == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">round</span>(last_score, <span class="number">5</span>)</span><br><span class="line"><span class="comment">#数据去重</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__mysql_data_rechecking</span>(<span class="params">self, item, ids_inmysql</span>):</span><br><span class="line">        id_inmysqls = [myid[<span class="number">0</span>] <span class="keyword">for</span> myid <span class="keyword">in</span> ids_inmysql]</span><br><span class="line">        title = self.rm_special_letter(item[<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line">        myid = item[<span class="string">&#x27;username&#x27;</span>] + <span class="built_in">str</span>(item[<span class="string">&#x27;mdate&#x27;</span>])[<span class="number">3</span>:-<span class="number">4</span>] + title[:<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">if</span> myid <span class="keyword">not</span> <span class="keyword">in</span> id_inmysqls:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;newrecord&#x27;</span>, title, myid</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;数据已存在&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">write_to_mysql</span>(<span class="params">self, datalist, host, db, user, passwd</span>):</span><br><span class="line">        <span class="comment"># mysql连接初始化连接</span></span><br><span class="line">        db = pymysql.connect(host=host, user=user, password=passwd, database=db)</span><br><span class="line">        <span class="comment"># 使用 cursor() 方法创建一个游标对象cursor</span></span><br><span class="line">        cursor = db.cursor()</span><br><span class="line">        <span class="comment"># 查询表中 plantform title username 数据拼接字符串用于去重</span></span><br><span class="line">        quchong_sql = <span class="string">&#x27;SELECT myid FROM &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.__tablename)</span><br><span class="line">        cursor.execute(quchong_sql)</span><br><span class="line">        myids = cursor.fetchall()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> datalist:</span><br><span class="line">            data = self.__mysql_data_rechecking(item, myids)</span><br><span class="line">            <span class="keyword">if</span> data[<span class="number">0</span>] == <span class="string">&#x27;newrecord&#x27;</span>:</span><br><span class="line">                title, myid = data[<span class="number">1</span>], data[<span class="number">2</span>]</span><br><span class="line">                <span class="comment"># feeling = self.feeling(title)</span></span><br><span class="line">                feeling = <span class="number">0</span></span><br><span class="line">                <span class="comment"># SQL插入语句</span></span><br><span class="line">                sql = <span class="string">&quot;INSERT INTO &#123;TABLENAME&#125;&#123;keys&#125;&quot;</span> \</span><br><span class="line">                      <span class="string">&quot;VALUES (&#x27;&#123;v0&#125;&#x27;,&#x27;&#123;v1&#125;&#x27;,&#x27;&#123;v2&#125;&#x27;,&#x27;&#123;v3&#125;&#x27;,&#x27;&#123;v4&#125;&#x27;,&#x27;&#123;v5&#125;&#x27;,&#x27;&#123;v6&#125;&#x27;,&#x27;&#123;v7&#125;&#x27;,&#x27;&#123;v8&#125;&#x27;)&quot;</span>.<span class="built_in">format</span> \</span><br><span class="line">                    (TABLENAME=self.__tablename,</span><br><span class="line">                     keys=self.__tablekeys,</span><br><span class="line">                     v0=myid,</span><br><span class="line">                     v1=item[<span class="string">&#x27;scan&#x27;</span>],</span><br><span class="line">                     v2=item[<span class="string">&#x27;comment_num&#x27;</span>],</span><br><span class="line">                     v3=title,</span><br><span class="line">                     v4=item[<span class="string">&#x27;username&#x27;</span>],</span><br><span class="line">                     v5=item[<span class="string">&#x27;mdate&#x27;</span>],</span><br><span class="line">                     v6=feeling,</span><br><span class="line">                     v7=item[<span class="string">&#x27;company&#x27;</span>],</span><br><span class="line">                     v8=item[<span class="string">&#x27;industry&#x27;</span>])</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="comment"># 执行sql语句</span></span><br><span class="line">                    cursor.execute(sql)</span><br><span class="line">                    <span class="comment"># 执行sql语句</span></span><br><span class="line">                    db.commit()</span><br><span class="line">                <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;PRIMARY&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">&#x27;查重失败&#x27;</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="built_in">print</span>(item)</span><br><span class="line">                        <span class="built_in">print</span>(<span class="built_in">str</span>(e) + <span class="string">&quot;---&quot;</span> + <span class="built_in">str</span>(time.time()))</span><br><span class="line">                        <span class="comment"># 发生错误时回滚</span></span><br><span class="line">                        db.rollback()</span><br><span class="line">                        <span class="comment"># raise e</span></span><br><span class="line">        <span class="comment"># 关闭数据库连接</span></span><br><span class="line">        db.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data01 = &#123;</span><br><span class="line">    <span class="string">&#x27;批发和零售业&#x27;</span>: [<span class="string">&#x27;大参林 603233&#x27;</span>, <span class="string">&#x27;广百股份 002187&#x27;</span>, <span class="string">&#x27;来伊份 603777&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;制造业&#x27;</span>: [<span class="string">&#x27;中国中车 601766&#x27;</span>, <span class="string">&#x27;永兴材料 002756&#x27;</span>, <span class="string">&#x27;海思科 002653&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;房地产业&#x27;</span>: [<span class="string">&#x27;格力地产 600185&#x27;</span>, <span class="string">&#x27;绿景控股 000502&#x27;</span>, <span class="string">&#x27;万科Ａ 000002&#x27;</span>],</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;租赁和商务服务业&#x27;</span>: [<span class="string">&#x27;深圳华强 000062&#x27;</span>, <span class="string">&#x27;渤海租赁 000415&#x27;</span>, <span class="string">&#x27;轻纺城 600790&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;采矿业&#x27;</span>: [<span class="string">&#x27;兴业矿业 000426&#x27;</span>, <span class="string">&#x27;冀中能源 000937&#x27;</span>, <span class="string">&#x27;中国石化 600028&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;交通运输、仓储和邮政业&#x27;</span>: [<span class="string">&#x27;中远海控 601919&#x27;</span>, <span class="string">&#x27;宜昌交运 002627&#x27;</span>, <span class="string">&#x27;大众交通 600611&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;信息传输、软件和信息技术服务业&#x27;</span>: [<span class="string">&#x27;恒生电子 600570&#x27;</span>, <span class="string">&#x27;中国联通 600050&#x27;</span>, <span class="string">&#x27;恒华科技 300365&#x27;</span>],</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;教育&#x27;</span>: [<span class="string">&#x27;好未来 ustal&#x27;</span>, <span class="string">&#x27;中公教育 002607&#x27;</span>, <span class="string">&#x27;紫光学大 000526&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;卫生和社会工作业&#x27;</span>: [<span class="string">&#x27;通策医疗 600763&#x27;</span>, <span class="string">&#x27;迪安诊断 300244&#x27;</span>, <span class="string">&#x27;爱尔眼科 300015&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;文化、体育和娱乐业&#x27;</span>: [<span class="string">&#x27;凤凰传媒 601928&#x27;</span>, <span class="string">&#x27;新华传媒 600825&#x27;</span>, <span class="string">&#x27;长江传媒 600757&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;金融业&#x27;</span>: [<span class="string">&#x27;民生银行 600016&#x27;</span>, <span class="string">&#x27;中国平安 601318&#x27;</span>, <span class="string">&#x27;国信证券 002736&#x27;</span>],</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;建筑业&#x27;</span>: [<span class="string">&#x27;棕榈股份 002431&#x27;</span>, <span class="string">&#x27;上海建工 600170&#x27;</span>, <span class="string">&#x27;隧道股份 600820&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;电力、热力、燃气及水的生产和供应业&#x27;</span>: [<span class="string">&#x27;滨海能源 000695&#x27;</span>, <span class="string">&#x27;太阳能 000591&#x27;</span>, <span class="string">&#x27;上海电力 600021&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;水利、环境和公共设施管理业&#x27;</span>: [<span class="string">&#x27;远达环保 600292&#x27;</span>, <span class="string">&#x27;碧水源 300070&#x27;</span>, <span class="string">&#x27;启迪环境 000826&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    gb = guba(host=<span class="string">&#x27;localhost&#x27;</span>, db=<span class="string">&#x27;guba&#x27;</span>, user=<span class="string">&#x27;root&#x27;</span>, passwd=<span class="string">&#x27;root&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">for</span> num, company <span class="keyword">in</span> <span class="built_in">enumerate</span>(data[item]):</span><br><span class="line">            stock_code = company.split(<span class="string">&#x27; &#x27;</span>)[<span class="number">1</span>]</span><br><span class="line">            name = company.split(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">            url = <span class="string">&#x27;http://guba.eastmoney.com/list,&#x27;</span> + <span class="built_in">str</span>(stock_code) + <span class="string">&#x27;,f_&#x27;</span></span><br><span class="line">            gb.dangu_pinglun(url, name, item)</span><br></pre></td></tr></table></figure><p>情绪分析指标计算部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /usr/bin/python </span></span><br><span class="line"><span class="comment"># --*-- coding:UTF-8 --*--</span></span><br><span class="line"><span class="comment"># Date 2021/3/23 21:12</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> dataoutput <span class="keyword">import</span> DataOutput</span><br><span class="line"></span><br><span class="line">d = DataOutput()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Analyse</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.host = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">        self.db = <span class="string">&#x27;guba&#x27;</span></span><br><span class="line">        self.user = <span class="string">&#x27;root&#x27;</span></span><br><span class="line">        self.passwd = <span class="string">&#x27;root&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该部分计算每个title的情绪得分</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_score_and_polarity</span>(<span class="params">self</span>):</span><br><span class="line">        db = pymysql.connect(host=self.host, user=self.user, password=self.passwd, database=self.db)</span><br><span class="line">        cursor = db.cursor()</span><br><span class="line">        sql01 = <span class="string">&#x27;select titles,id from info_guba&#x27;</span></span><br><span class="line">        cursor.execute(sql01)</span><br><span class="line">        res = cursor.fetchall()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> res:</span><br><span class="line">            <span class="comment"># 使用snownlp计算情绪值</span></span><br><span class="line">            score_snownlp = d.feeling(i[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># 评出情感极性 将情绪得分&gt;0.6的评论当作积极评论，小于0.4的评论当作消极评论。</span></span><br><span class="line">            <span class="keyword">if</span> score_snownlp &lt;= <span class="number">0.6</span>:</span><br><span class="line">                <span class="keyword">if</span> score_snownlp &gt; <span class="number">0.4</span>:</span><br><span class="line">                    p = <span class="number">0</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    p = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                p = <span class="number">1</span></span><br><span class="line">            sql02 = <span class="string">&quot;update info_guba set f_scores=&#123;0&#125;,polarity=&#123;1&#125; where id=&#123;2&#125;&quot;</span>.<span class="built_in">format</span>(score_snownlp, p, i[<span class="number">1</span>])</span><br><span class="line">            cursor.execute(sql02)</span><br><span class="line">            db.commit()</span><br><span class="line">            <span class="built_in">print</span>(i[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># db.commit()</span></span><br><span class="line">        db.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算单股吧一天内所有帖子情绪值加和求平均</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_score_of_day</span>(<span class="params">self</span>):</span><br><span class="line">        db = pymysql.connect(host=self.host, user=self.user, password=self.passwd, database=self.db)</span><br><span class="line">        cursor = db.cursor()</span><br><span class="line">        sql03 = <span class="string">&quot;select company_name from info_guba group by company_name&quot;</span></span><br><span class="line">        cursor.execute(sql03)</span><br><span class="line">        com_names = cursor.fetchall()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> com_names:</span><br><span class="line">            <span class="comment"># 该语句查询每只股票散户每天发贴的情绪</span></span><br><span class="line">            sql04 = <span class="string">&quot;select round(sum(f_scores*log10(scans+comments))/count(f_scores),10),mdates from info_guba &quot;</span> \</span><br><span class="line">                    <span class="string">&quot;where company_name=&#x27;&#123;0&#125;&#x27; and usernames not like &#x27;%资讯%&#x27; group by &quot;</span> \</span><br><span class="line">                    <span class="string">&quot;mdates order by mdates&quot;</span>.<span class="built_in">format</span>(i[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">            cursor.execute(sql04)</span><br><span class="line">            res = cursor.fetchall()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> res:</span><br><span class="line">                score = j[<span class="number">0</span>]</span><br><span class="line">                day = j[<span class="number">1</span>]</span><br><span class="line">                com_name = i[<span class="number">0</span>]</span><br><span class="line">                sql05 = <span class="string">&quot;insert into score_of_day(score,daytimestamp,company_name) &quot;</span> \</span><br><span class="line">                        <span class="string">&quot;values (&#123;0&#125;,&#123;1&#125;,&#x27;&#123;2&#125;&#x27;)&quot;</span>.<span class="built_in">format</span>(score, day, com_name)</span><br><span class="line">                <span class="comment"># print(sql03)</span></span><br><span class="line">                cursor.execute(sql05)</span><br><span class="line">                db.commit()</span><br><span class="line">        db.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将日期格式转换为时间戳</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">date_to_timestamp</span>(<span class="params">self, year, month</span>):</span><br><span class="line">        mdate = <span class="built_in">str</span>(year) + <span class="string">&#x27;-&#x27;</span> + <span class="built_in">str</span>(month)</span><br><span class="line">        time_array = time.strptime(mdate, <span class="string">&quot;%Y-%m&quot;</span>)</span><br><span class="line">        news_timestamp = time.mktime(time_array)</span><br><span class="line">        <span class="keyword">return</span> news_timestamp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">month_score_of_all</span>(<span class="params">self</span>):</span><br><span class="line">        db = pymysql.connect(host=self.host, user=self.user, password=self.passwd, database=self.db)</span><br><span class="line">        cursor = db.cursor()</span><br><span class="line">        year = <span class="number">2021</span></span><br><span class="line">        months = [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>,</span><br><span class="line">                  <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">for</span> num, m <span class="keyword">in</span> <span class="built_in">enumerate</span>(months):</span><br><span class="line">            <span class="keyword">if</span> num != <span class="number">37</span>:</span><br><span class="line">                <span class="keyword">if</span> m == <span class="number">12</span>:</span><br><span class="line">                    year -= <span class="number">1</span></span><br><span class="line">                max_timestamp = self.date_to_timestamp(year, m)</span><br><span class="line">                <span class="keyword">if</span> months[num + <span class="number">1</span>] == <span class="number">12</span>:</span><br><span class="line">                    year02 = year - <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    year02 = year</span><br><span class="line">                month = <span class="built_in">str</span>(year02) + <span class="string">&#x27;-&#x27;</span> + <span class="built_in">str</span>(months[num + <span class="number">1</span>])</span><br><span class="line">                min_timestamp = self.date_to_timestamp(year02, months[num + <span class="number">1</span>])</span><br><span class="line">                sql06 = <span class="string">&quot;select round(sum(polarity*f_scores*log10(scans+comments)),10) from info_guba &quot;</span> \</span><br><span class="line">                        <span class="string">&quot;where usernames not like &#x27;%资讯%&#x27; &quot;</span> \</span><br><span class="line">                        <span class="string">&quot;and mdates&gt;&#123;0&#125; and mdates&lt;&#123;1&#125;&quot;</span>.<span class="built_in">format</span> \</span><br><span class="line">                    (min_timestamp, max_timestamp)</span><br><span class="line">                cursor.execute(sql06)</span><br><span class="line">                res = cursor.fetchall()</span><br><span class="line">                <span class="built_in">print</span>(month, res)</span><br><span class="line">                <span class="comment">#break</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">东方财富股吧标题爬取分析</summary>
    
    
    
    <category term="python" scheme="https://s-luping.github.io/luping/categories/python/"/>
    
    
    <category term="python 爬虫" scheme="https://s-luping.github.io/luping/tags/python-%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>随机森林算法的Python实现</title>
    <link href="https://s-luping.github.io/luping/2021/03/02/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E7%9A%84Python%E5%AE%9E%E7%8E%B0/"/>
    <id>https://s-luping.github.io/luping/2021/03/02/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E7%9A%84Python%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-03-01T20:43:38.000Z</published>
    <updated>2022-03-13T12:25:25.723Z</updated>
    
    <content type="html"><![CDATA[<p><strong>随机森林主要应用于回归和分类。<br>它几乎可以将任何数据填进去，下文使用鸢尾花数据进行分类和预测</strong><br><strong>环境</strong> python3.8<br><strong>数据集</strong> 鸢尾花数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dataset</span>(<span class="params">self</span>):</span><br><span class="line">    iris = load_iris()</span><br><span class="line">    feature = pd.DataFrame(data=iris.data, columns=iris.feature_names)</span><br><span class="line">    target = pd.DataFrame(data=<span class="built_in">map</span>(<span class="keyword">lambda</span> item: iris.target_names[item],</span><br><span class="line">                                   iris.target), columns=&#123;<span class="string">&#x27;target_names&#x27;</span>&#125;)</span><br><span class="line">    feature_train, feature_test, target_train, target_test = \</span><br><span class="line">        train_test_split(feature, target, test_size=<span class="number">0.3</span>)</span><br><span class="line">    <span class="keyword">return</span> feature_train, feature_test, target_train, target_test</span><br></pre></td></tr></table></figure><p>实验思路：<br>首先训练10个基分类器，每个基分类器为一个决策树；在预测时对每个基分类器投票结果进行统计倒排，选取票数最多的结果；其中‎每棵树的生长情况如下：‎<br>‎如果培训集中的案例数为 N，则随机取样 N 案例 - 但从原始数据中‎‎替换‎‎。此示例将是种植树的培训集。‎<br>‎如果有 M 输入变量，则指定一个数字 m&lt;&lt;M，以便在每个节点中随机从 M 中选择 m 变量，并且这些 m 上的最佳拆分用于拆分节点。在森林生长过程中，m 值保持不变。‎<br>‎每棵树都尽可能的生长。没有修剪。<br><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#workings">查看随机森林官网描述</a><br><strong>fit训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, feature=<span class="literal">None</span>, label=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    训练模型，请记得将模型保存至self.models</span></span><br><span class="line"><span class="string">    :param feature: 训练集数据，类型为ndarray</span></span><br><span class="line"><span class="string">    :param label: 训练集标签，类型为ndarray</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># ************* Begin ************#</span></span><br><span class="line">    n = <span class="built_in">len</span>(feature)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_model):</span><br><span class="line">        <span class="comment"># 在训练集N随机选取n个样本  #frac=1 样本可重复取 （样本只包含特征数据）</span></span><br><span class="line">        randomSamples = feature.sample(n, replace=<span class="literal">True</span>, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 在所有特征M随机选取m个特征 特征无重复 0.75表示选取4*0.75=3个特征</span></span><br><span class="line">        randomFeatures = randomSamples.sample(frac=<span class="number">0.75</span>, replace=<span class="literal">False</span>, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 标记该模型选取的特征</span></span><br><span class="line">        tags = self.connect(randomFeatures.columns.tolist())</span><br><span class="line">        <span class="comment"># 根据样本筛选出索引与之相同的lable即target_name</span></span><br><span class="line">        <span class="comment"># 使用loc标签索引获取</span></span><br><span class="line">        randomLable = label.loc[randomSamples.index.tolist(),:]</span><br><span class="line">        <span class="comment"># for i,j in zip(randomFeatures.index.tolist(),,randomLable.index.tolist()):</span></span><br><span class="line">        <span class="comment">#print(i,j)</span></span><br><span class="line">        model = DecisionTreeClassifier()</span><br><span class="line">        model = model.fit(randomFeatures, randomLable)</span><br><span class="line">        self.models.append(&#123;tags: model&#125;)</span><br><span class="line">    <span class="comment"># ************* End **************#</span></span><br></pre></td></tr></table></figure><p><strong>预测</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, features, target</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param features: 测试集数据，类型为ndarray</span></span><br><span class="line"><span class="string">    :param target: 测试集实际lable，类型为ndarray</span></span><br><span class="line"><span class="string">    :return: 预测结果，类型为ndarray</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># ************* Begin ************#</span></span><br><span class="line">    result = []</span><br><span class="line">    vote = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> model <span class="keyword">in</span> self.models:</span><br><span class="line">        <span class="comment"># 获取模型使用的训练特征</span></span><br><span class="line">        modelFeatures = <span class="built_in">list</span>(model.keys())[<span class="number">0</span>].split(<span class="string">&#x27;000&#x27;</span>)[:-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 提取出模型预测需要的标签</span></span><br><span class="line">        test_data = features[modelFeatures]</span><br><span class="line">        <span class="comment"># 基分类器进行预测</span></span><br><span class="line">        r = <span class="built_in">list</span>(model.values())[<span class="number">0</span>].predict(test_data)</span><br><span class="line">        vote.append(r)</span><br><span class="line">    <span class="comment"># 将数组转换为矩阵 10行45列</span></span><br><span class="line">    vote = np.array(vote)</span><br><span class="line">    <span class="comment"># print(vote.shape) # print(vote)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features)):</span><br><span class="line">        <span class="comment"># 对每棵树的投票结果进行排序选取最大的</span></span><br><span class="line">        v = <span class="built_in">sorted</span>(Counter(vote[:, i]).items(),</span><br><span class="line">                   key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">       <span class="comment"># 查看投票情况和实际标签对比</span></span><br><span class="line">        <span class="built_in">print</span>(v, <span class="string">&quot;---&quot;</span>,<span class="built_in">list</span>(target)[i])</span><br><span class="line">        result.append(v[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">    <span class="comment"># ************* End **************#</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">connect</span>(<span class="params">self, ls</span>):</span><br><span class="line">    s = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ls:</span><br><span class="line">        s += i + <span class="string">&#x27;000&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><p><strong>主函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    Bcf = BaggingClassifier()</span><br><span class="line">    featureAndTarget = Bcf.dataset()</span><br><span class="line">    Bcf.fit(featureAndTarget[<span class="number">0</span>],featureAndTarget[<span class="number">2</span>])</span><br><span class="line">    res = Bcf.predict(features=featureAndTarget[<span class="number">1</span>], target=featureAndTarget[<span class="number">3</span>][<span class="string">&#x27;target_names&#x27;</span>])</span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">zip</span>(featureAndTarget[<span class="number">3</span>][<span class="string">&#x27;target_names&#x27;</span>], res):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        <span class="comment">#print(i + &#x27;\t&#x27; + j)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;准确率为&#x27;</span> + <span class="built_in">str</span>(right / <span class="built_in">len</span>(res) * <span class="number">100</span>) + <span class="string">&quot;%&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/0d10e92317b34ce696252af9ca60df82.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>可以看出准确率很高啦，可以调整特征数量m参数，准确率也会不同。</p>]]></content>
    
    
    <summary type="html">随机森林算法的Python实现</summary>
    
    
    
    <category term="python" scheme="https://s-luping.github.io/luping/categories/python/"/>
    
    
    <category term="python 随机森林" scheme="https://s-luping.github.io/luping/tags/python-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
  </entry>
  
</feed>
