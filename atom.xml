<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Lu Ping&#39;s blog</title>
  
  <subtitle>Welcome to my blog</subtitle>
  <link href="https://s-luping.github.io/luping/atom.xml" rel="self"/>
  
  <link href="https://s-luping.github.io/luping/"/>
  <updated>2022-05-06T15:30:45.619Z</updated>
  <id>https://s-luping.github.io/luping/</id>
  
  <author>
    <name>luping</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>spark分区</title>
    <link href="https://s-luping.github.io/luping/2022/05/07/spark%E5%88%86%E5%8C%BA/"/>
    <id>https://s-luping.github.io/luping/2022/05/07/spark%E5%88%86%E5%8C%BA/</id>
    <published>2022-05-07T06:55:17.000Z</published>
    <updated>2022-05-06T15:30:45.619Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><p>如下读取mysql表时默认是一个分区 查询结果只有一个task在跑</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> jdbcDF = spark.read.jdbc(url,tableName,prop)</span><br></pre></td></tr></table></figure><p>这种读取读大表（千万级）就会OOM<br>因此在读取时要用到spark分区</p><h1 id="理解spark分区"><a href="#理解spark分区" class="headerlink" title="理解spark分区"></a>理解spark分区</h1><p>就是将一个非常大的任务拆分为多个小任务，小任务数量最好满足整除Executor数量*Executor核心数量，这样可以提供资源利用率。<br>这就是在spark调优中，增大RDD分区数目，增大任务并行度的原因。</p><h1 id="spark分区-什么时候增加的，增加有什么用？"><a href="#spark分区-什么时候增加的，增加有什么用？" class="headerlink" title="spark分区 什么时候增加的，增加有什么用？"></a>spark分区 什么时候增加的，增加有什么用？</h1><p>接下来的描述，是针对于sparksql（也就是把数据加载成Dataset之后再处理）来说的。</p><p>1.增加分区数，可以增加并行度，当spark申请的cpu核心足够的情况下，可以同时跑不同分区的数据（因为一个分区的数据，只能由一个核心来跑，不能多个）</p><p>2.手动增加，使用repartition来将所有数据打散</p><p>3.自动增加，spark有个参数：spark.sql.shuffle.partitions，默认值为200。也就是说当触发shuffle逻辑的时候，数据会自动分为200个分区运行，但是在数据量大的情况下，每个分区的数据量太大，而且假设spark申请到了300个核心，但是因为分区数只有200，会导致只有200个核心在运行，另外100个核心在空转（虽然占用资源但是却不干活）。所以可以将该参数设置为500甚至更大，来增加分区数和并行度。</p><h1 id="repartition适用场景"><a href="#repartition适用场景" class="headerlink" title="repartition适用场景"></a>repartition适用场景</h1><p>RDD单个分区数据量比较大，或者单个分区处理比较慢，<br>数据倾斜</p><p>转载于<br>[<a href="https://blog.csdn.net/qq_22473611/article/details/107822168]">https://blog.csdn.net/qq_22473611/article/details/107822168]</a></p>]]></content>
    
    
    <summary type="html">在一次spark读取mysql千万级大表时的</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="spark" scheme="https://s-luping.github.io/luping/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>vue学习记录</title>
    <link href="https://s-luping.github.io/luping/2022/02/11/vue%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://s-luping.github.io/luping/2022/02/11/vue%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2022-02-10T20:43:38.000Z</published>
    <updated>2022-03-13T12:28:56.437Z</updated>
    
    <content type="html"><![CDATA[<h1 id="windows开发环境"><a href="#windows开发环境" class="headerlink" title="windows开发环境"></a>windows开发环境</h1><h2 id="nodejs环境安装"><a href="#nodejs环境安装" class="headerlink" title="nodejs环境安装"></a>nodejs环境安装</h2><p>安装时将 nodejs和npm添加到系统环境变量<br> 测试是否安装成功：nodejs里面会安装npm指令，显示版本号安装成功。<br><img src="https://img-blog.csdnimg.cn/1edf6d3c931c4483b67f2f418715f899.png" alt="在这里插入图片描述"></p><h2 id="配置nodejs-prefix（全局）和cache（缓存）路径"><a href="#配置nodejs-prefix（全局）和cache（缓存）路径" class="headerlink" title="配置nodejs prefix（全局）和cache（缓存）路径"></a>配置nodejs prefix（全局）和cache（缓存）路径</h2><p>先找到nodejs的安装目录（E:\nodejs）<br><strong>在nodejs安装路径下，新建global和cache两个文件夹</strong><br>命令进行修改设置</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm config <span class="built_in">set</span> cache <span class="string">&quot;E:\nodejs\cache&quot;</span></span><br><span class="line">npm config <span class="built_in">set</span> prefix <span class="string">&quot;E:\nodejs\global&quot;</span></span><br></pre></td></tr></table></figure><h2 id="设置环境变量"><a href="#设置环境变量" class="headerlink" title="设置环境变量"></a>设置环境变量</h2><p>添加node_global HOME<br><img src="https://img-blog.csdnimg.cn/0a0276ef45ea42bfa0b08126d457abcd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>添加到path<br><img src="https://img-blog.csdnimg.cn/4e82b88a358e47ca94afeb838edad825.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>安装webpack、vue-cli脚手架构建工具</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install webpack <span class="literal">-g</span></span><br><span class="line">npm install vue<span class="literal">-cli</span> <span class="literal">-g</span></span><br></pre></td></tr></table></figure><p>查看安装是否成功<br><img src="https://img-blog.csdnimg.cn/2a0bf21f741b4f08916d17a621a9cbdb.png" alt="在这里插入图片描述"></p><h1 id="创建一个vue项目"><a href="#创建一个vue项目" class="headerlink" title="创建一个vue项目"></a>创建一个vue项目</h1><p><strong>在dos命令行</strong><br><strong>在硬盘上找一个文件夹放工程用的： cd 目录路径</strong><br>安装vue脚手架输入：vue init webpack demo，注意这里的“demo” 是项目的名称可以说是随便的起名，但是需要主要的是“不能用中文”。<br>然后根据需要选择选项</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br><span class="line">npm run dev </span><br></pre></td></tr></table></figure><h1 id="vue目录结构"><a href="#vue目录结构" class="headerlink" title="vue目录结构"></a>vue目录结构</h1><p><img src="https://img-blog.csdnimg.cn/2375db529a1447699e468a922f435b32.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h3 id="build"><a href="#build" class="headerlink" title="build"></a>build</h3><p><img src="https://img-blog.csdnimg.cn/37ee7e4d8b164afa9de4805999086b19.png" alt="在这里插入图片描述"></p><h3 id="config"><a href="#config" class="headerlink" title="config"></a>config</h3><p><img src="https://img-blog.csdnimg.cn/688b8e7d0da5484e9a69a5707d57b9cc.png" alt="在这里插入图片描述"></p><h1 id="项目练习"><a href="#项目练习" class="headerlink" title="项目练习"></a>项目练习</h1><p>vue整合elementUI</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm i element<span class="literal">-ui</span> <span class="literal">-S</span></span><br></pre></td></tr></table></figure><p>main.js 添加<br>完整引入</p><figure class="highlight powershell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 完整引入 Element</span><br><span class="line">import Vue from <span class="string">&#x27;vue&#x27;</span></span><br><span class="line">import ElementUI from <span class="string">&#x27;element-ui&#x27;</span></span><br><span class="line">import locale from <span class="string">&#x27;element-ui/lib/locale/lang/en&#x27;</span></span><br><span class="line"></span><br><span class="line">Vue.use(ElementUI, &#123; locale &#125;)</span><br></pre></td></tr></table></figure><h1 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h1><h2 id="vue-axios-跨域访问"><a href="#vue-axios-跨域访问" class="headerlink" title="vue-axios 跨域访问"></a>vue-axios 跨域访问</h2><p>问题描述当前后端分离项目调试时，前端项目需要访问后端接口，使用axios请求时会出现跨域访问问题，<br>解决 ： 在config目录下修改index.js文件 如下部分<br><img src="https://img-blog.csdnimg.cn/1a92f7701a044160abe98c2b3c1ea781.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>使用vue的代理访问目标资源，然后改为自己的格式</p>]]></content>
    
    
    <summary type="html">vue学习记录</summary>
    
    
    
    <category term="web" scheme="https://s-luping.github.io/luping/categories/web/"/>
    
    
    <category term="vue" scheme="https://s-luping.github.io/luping/tags/vue/"/>
    
  </entry>
  
  <entry>
    <title>spark将数据加载到hbase--bulkload方式</title>
    <link href="https://s-luping.github.io/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/"/>
    <id>https://s-luping.github.io/luping/2021/11/23/spark%E5%B0%86%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%88%B0hbase--bulkload%E6%96%B9%E5%BC%8F/</id>
    <published>2021-11-22T21:30:25.000Z</published>
    <updated>2022-03-13T12:27:51.224Z</updated>
    
    <content type="html"><![CDATA[<p><strong>通过bulkload方式加载数据优点：与put方式相比</strong><br>1.导入过程不占用Region资源<br>2.能快速导入海量的数据<br>3.节省内存<br>应该是业界将数据载入hbase常用方式之一，因此有必要学习掌握</p><h1 id="实现步骤"><a href="#实现步骤" class="headerlink" title="实现步骤"></a>实现步骤</h1><h2 id="步骤一-读取数据生成rdd"><a href="#步骤一-读取数据生成rdd" class="headerlink" title="步骤一  读取数据生成rdd"></a>步骤一  读取数据生成rdd</h2><p>读入数据是面向行的表，一行有多个字段，需要转换成面向列的数据，构造<strong>keyValue</strong>对象，一定要注意<strong>key们要排序</strong>，比如<em>user:age</em>列要在<em>user:gender</em>列之前<br>需要设计行键保证行键唯一和避免数据都涌入一个region，如我的是按时间设计的，好几个月的数据，因此将数据按月预分区。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;file:///&quot;</span>+filePath)</span><br><span class="line">    .flatMap(x=&gt;getLineData(x,rowKeyBase,<span class="type">HBaseUtils</span>.<span class="type">LOG_FIELD_NAMES</span>))</span><br><span class="line">    .sortByKey()</span><br><span class="line"><span class="comment">//处理每一条记录生成keyvalue对象</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLineData</span></span>(line:<span class="type">String</span>,rowkey:<span class="type">String</span>,fieldNames: <span class="type">TreeMap</span>[<span class="type">String</span>, <span class="type">Int</span>]): <span class="type">List</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">KeyValue</span>)] =&#123;</span><br><span class="line">  <span class="keyword">val</span> length = fieldNames.size</span><br><span class="line">  <span class="keyword">val</span> values:<span class="type">Array</span>[<span class="type">String</span>] = line.split(<span class="string">&quot;\\\t&quot;</span>)</span><br><span class="line">  <span class="keyword">if</span> (<span class="literal">null</span> == values || values.length!=length) <span class="keyword">return</span> <span class="type">Nil</span></span><br><span class="line">  <span class="comment">//println(rowkey+values(1)+Random.nextInt(100000).toString)</span></span><br><span class="line">  <span class="keyword">val</span> rowKey = <span class="type">Bytes</span>.toBytes(rowkey+values(<span class="number">1</span>)+<span class="type">Random</span>.nextInt(<span class="number">1000</span>).toString)</span><br><span class="line">  <span class="keyword">val</span> writable = <span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>(rowKey)</span><br><span class="line">  <span class="keyword">val</span> columnFamily = <span class="type">Bytes</span>.toBytes(<span class="string">&quot;detail&quot;</span>)</span><br><span class="line">  fieldNames.toList.map&#123;</span><br><span class="line">    <span class="keyword">case</span> (fieldName, fieldIndex) =&gt;</span><br><span class="line">      <span class="comment">// KeyValue实例对象</span></span><br><span class="line">      <span class="keyword">val</span> keyValue = <span class="keyword">new</span> <span class="type">KeyValue</span>(</span><br><span class="line">        rowKey, <span class="comment">//</span></span><br><span class="line">        columnFamily, <span class="comment">//</span></span><br><span class="line">        <span class="type">Bytes</span>.toBytes(fieldName), <span class="comment">//</span></span><br><span class="line">        <span class="type">Bytes</span>.toBytes(values(fieldIndex)) <span class="comment">//</span></span><br><span class="line">      )</span><br><span class="line">      <span class="comment">// 返回</span></span><br><span class="line">      (writable, keyValue)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="步骤二-配置输出HFile文件"><a href="#步骤二-配置输出HFile文件" class="headerlink" title="步骤二  配置输出HFile文件"></a>步骤二  配置输出HFile文件</h2><h3 id="输出前检查"><a href="#输出前检查" class="headerlink" title="输出前检查"></a>输出前检查</h3><h4 id="检查HFile输出目录是否存在"><a href="#检查HFile输出目录是否存在" class="headerlink" title="检查HFile输出目录是否存在"></a>检查HFile输出目录是否存在</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO：构建Job，设置相关配置信息，主要为输出格式</span></span><br><span class="line"><span class="comment">// a. 读取配置信息</span></span><br><span class="line"><span class="keyword">val</span> hbaseConfig: <span class="type">Configuration</span> = <span class="type">HBaseUtils</span>.getHBaseConfiguration(<span class="string">&quot;hbase&quot;</span>,<span class="string">&quot;2181&quot;</span>)</span><br><span class="line"><span class="comment">//  Configuration parameter hbase.mapreduce.hfileoutputformat.table.name cannot be empty</span></span><br><span class="line">hbaseConfig.set(<span class="string">&quot;hbase.mapreduce.hfileoutputformat.table.name&quot;</span>, <span class="string">&quot;log&quot;</span>)</span><br><span class="line"><span class="comment">// b. 如果输出目录存在，删除</span></span><br><span class="line"><span class="keyword">val</span> dfs = <span class="type">FileSystem</span>.get(hbaseConfig)</span><br><span class="line"><span class="keyword">val</span> outputPath: <span class="type">Path</span> = <span class="keyword">new</span> <span class="type">Path</span>(<span class="string">&quot;hdfs://hbase:9000/hbase/log/&quot;</span>+rowKeyBase)</span><br><span class="line"><span class="keyword">if</span> (dfs.exists(outputPath)) &#123;</span><br><span class="line">  dfs.delete(outputPath, <span class="literal">true</span>)</span><br><span class="line">&#125;</span><br><span class="line">dfs.close()</span><br></pre></td></tr></table></figure><h4 id="配置HFileOutputFormat2"><a href="#配置HFileOutputFormat2" class="headerlink" title="配置HFileOutputFormat2"></a>配置HFileOutputFormat2</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO： 配置HFileOutputFormat2输出</span></span><br><span class="line"><span class="keyword">val</span> conn = <span class="type">ConnectionFactory</span>.createConnection(hbaseConfig)</span><br><span class="line"><span class="keyword">val</span> htableName = <span class="type">TableName</span>.valueOf(<span class="string">&quot;log&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> table: <span class="type">Table</span> = conn.getTable(htableName)</span><br><span class="line"><span class="type">HFileOutputFormat2</span>.configureIncrementalLoad(</span><br><span class="line">  <span class="type">Job</span>.getInstance(hbaseConfig), <span class="comment">//</span></span><br><span class="line">  table, <span class="comment">//</span></span><br><span class="line">  conn.getRegionLocator(htableName) <span class="comment">//</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="输出HFile文件"><a href="#输出HFile文件" class="headerlink" title="输出HFile文件"></a>输出HFile文件</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO： 3. 保存数据为HFile文件//先排序</span></span><br><span class="line">rdd.sortBy(x=&gt;(x._1, x._2.getKeyString), ascending = <span class="literal">true</span>)</span><br><span class="line">  .saveAsNewAPIHadoopFile(</span><br><span class="line">    <span class="string">&quot;hdfs://hbase:9000/hbase/log/&quot;</span>+rowKeyBase,</span><br><span class="line">    classOf[<span class="type">ImmutableBytesWritable</span>], <span class="comment">//</span></span><br><span class="line">    classOf[<span class="type">KeyValue</span>], <span class="comment">//</span></span><br><span class="line">    classOf[<span class="type">HFileOutputFormat2</span>], <span class="comment">//</span></span><br><span class="line">    hbaseConfig)</span><br></pre></td></tr></table></figure><h2 id="将HFile文件bulkload到hbase表分区当中"><a href="#将HFile文件bulkload到hbase表分区当中" class="headerlink" title="将HFile文件bulkload到hbase表分区当中"></a>将HFile文件bulkload到hbase表分区当中</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// TODO：4. 将输出HFile加载到HBase表中</span></span><br><span class="line"><span class="keyword">val</span> load = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(hbaseConfig)</span><br><span class="line">load.doBulkLoad(outputPath, conn.getAdmin, table,</span><br><span class="line">  conn.getRegionLocator(htableName))</span><br></pre></td></tr></table></figure><h1 id="出现的问题"><a href="#出现的问题" class="headerlink" title="出现的问题"></a>出现的问题</h1><p>写入权限<br>可以将HFile要输出的文件位置chmod 777 &#x2F;outputDir</p>]]></content>
    
    
    <summary type="html">spark将数据加载到hbase--bulkload方式</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="spark" scheme="https://s-luping.github.io/luping/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu sudo执行shell脚本环境变量失效</title>
    <link href="https://s-luping.github.io/luping/2021/11/02/sudo%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E5%91%BD%E4%BB%A4/"/>
    <id>https://s-luping.github.io/luping/2021/11/02/sudo%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E5%92%8C%E5%91%BD%E4%BB%A4/</id>
    <published>2021-11-01T20:43:38.000Z</published>
    <updated>2022-03-13T12:28:56.428Z</updated>
    
    <content type="html"><![CDATA[<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p><strong>普通用户下，设置并export一个变量，然后利用sudo执行echo命令，能得到变量的值，但是如果把echo命令写入脚本，然后再sudo执行脚本，就找不到变量，未能获取到值，如题情况如下：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> tesh.sh </span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$var</span> </span><br><span class="line">$ var=aaa </span><br><span class="line">$ <span class="built_in">export</span> var                       <span class="comment"># export 变量 </span></span><br><span class="line">$ sudo <span class="built_in">echo</span> <span class="variable">$var</span>                   <span class="comment"># sudo执行echo命令，返回变量值 </span></span><br><span class="line">aaa </span><br><span class="line">$ sudo bash test.sh                <span class="comment"># sudo执行脚本，不能获取变量值 </span></span><br><span class="line"></span><br><span class="line">$ bash test.sh                     <span class="comment"># 普通用户执行脚本，返回变量值 </span></span><br></pre></td></tr></table></figure><h1 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h1><p><strong>sudo运行时，会默认重置环境变量为安全的环境变量，也即，但前面设置的变量都会失效，只有少数配置文件中指定的环境变量能保存下来。</strong></p><p><strong>sudo的配置文件是 &#x2F;etc&#x2F;sudoers 需要root权限才能读取:</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sed ‘/^<span class="comment">#/d;/^$/d’ /etc/sudoers</span></span><br><span class="line"></span><br><span class="line">Defaults env_reset </span><br><span class="line">Defaults mail_badpass </span><br><span class="line">Defaults secure_path=”/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin” </span><br><span class="line">root ALL=(ALL:ALL) ALL </span><br><span class="line">%sudo ALL=(ALL:ALL) ALL </span><br><span class="line">xxx ALL=(ALL:ALL) NOPASSWD:ALL</span><br></pre></td></tr></table></figure><p><strong>不过可以直接通过sudo -l来查看sudo的限制：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -l</span><br><span class="line"></span><br><span class="line">Matching Defaults entries <span class="keyword">for</span> xxx on this host: </span><br><span class="line">env_reset, mail_badpass,</span><br><span class="line">secure_path=/usr/local/sbin\:/usr/local/bin\:/usr/sbin\:/usr/bin\:/sbin\:/bin </span><br><span class="line"></span><br><span class="line">User xxx may run the following commands on this host:</span><br><span class="line"> (ALL : ALL) NOPASSWD: ALL</span><br></pre></td></tr></table></figure><p><strong>注意看第一行的选项 Defaults env_reset 表示默认会将环境变量重置，这样你定义的变量在sudo环境就会失效，获取不到。<br>另外有的发行版还有一个Defaults env_keep&#x3D;”“的选项，用于保留部分环境变量不被重置，需要保留的变量就写入双引号中。</strong></p><p>为什么sudo echo $var能获取到变量值？<br>既然利用sudo执行会重置环境变量，那么为什么还能echo获取到相应的变量呢？<br><strong>这是由于shell命令行的替换&amp;重组功能，在输入命令，按下回车时，shell会先依据分隔符将命令行切割成字段，对每个字段查找有没有变量或命令替换，再替换完成后，重组成新的命令，再去执行。<br>所以，命令实际执行是：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo <span class="built_in">echo</span> <span class="variable">$var</span>                   <span class="comment"># $var =&gt; aaa </span></span><br><span class="line">(sudo <span class="built_in">echo</span> aaa)                    <span class="comment"># 完成命令替换&amp;重组 </span></span><br><span class="line">(<span class="built_in">echo</span> aaa)                         <span class="comment"># sudo环境中执行 </span></span><br><span class="line">aaa</span><br></pre></td></tr></table></figure><p>因此，sudo环境重置后，并不用去引用$var这个变量，而是直接echo aaa。</p><h1 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h1><p><strong>sudo -E<br>简单来说，就是加上-E选项后，用户可以在sudo执行时保留当前用户已存在的环境变量，不会被sudo重置，另外，如果用户对于指定的环境变量没有权限，则会报错。</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -E bash test.sh <span class="comment"># 加上-E参数后就可以获取到变量 aaa</span></span><br></pre></td></tr></table></figure><p><strong>2. 修改sudo配置文件</strong><br>在内部测试机器中，安全性要求不高，总是需要加上-E参数来执行脚本，这个安全设定也不是很方便，可以通过visudo命令来修改配置为保留原有的环境变量，具体修改如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$sudo</span> visudo </span><br><span class="line"><span class="comment"># Defaults env_reset                  # 注释掉原有配置 </span></span><br><span class="line"><span class="comment"># Defaults env_keep=”…”               # 注释掉指定的变量保持</span></span><br></pre></td></tr></table></figure><p>Defaults !env_reset # 修改为不重置环境<br><strong>3. 手动添加变量</strong><br>手动在脚本中设置所需的变量，这样看起来比较麻烦，或者在执行sudo脚本前先将所需要的变量写入到要执行的脚本开头.</p><p><em>命令<br>对于自己安装的软件在sudo提示找不到的命令（没加sudo可以找到），在这个后面添加命令所在的路径</em></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">secure_path=/usr/local/sbin\:/usr/local/bin\:/usr/sbin\:/usr/bin\:/sbin\:/bin </span><br><span class="line">1</span><br><span class="line">secure_path=/usr/local/sbin\:/usr/local/bin\:/usr/sbin\:/usr/bin\:/sbin\:/bin:自定义路径 </span><br></pre></td></tr></table></figure><h1 id="visudo编辑模式和退出"><a href="#visudo编辑模式和退出" class="headerlink" title="visudo编辑模式和退出"></a>visudo编辑模式和退出</h1><p><strong>执行</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$sudo</span> visudo</span><br></pre></td></tr></table></figure><p>找到如下授权，注释</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%admin ALL=(ALL) ALL</span><br></pre></td></tr></table></figure><p>修改为新的授权，意思是不需要密码执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">%admin ALL=(ALL) NOPASSWD: ALL</span><br></pre></td></tr></table></figure><p><strong>保存并退出：</strong><br><em>保存</em><br>执行“Ctrl+O”*</p>]]></content>
    
    
    <summary type="html">spark将数据加载到hbase--bulkload方式</summary>
    
    
    
    <category term="linux" scheme="https://s-luping.github.io/luping/categories/linux/"/>
    
    
    <category term="ubuntu shell" scheme="https://s-luping.github.io/luping/tags/ubuntu-shell/"/>
    
  </entry>
  
  <entry>
    <title>Hive网站日志采集统计分析</title>
    <link href="https://s-luping.github.io/luping/2021/10/27/Hive%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/"/>
    <id>https://s-luping.github.io/luping/2021/10/27/Hive%E7%BD%91%E7%AB%99%E6%97%A5%E5%BF%97%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90/</id>
    <published>2021-10-26T22:12:06.000Z</published>
    <updated>2022-03-13T12:15:32.341Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本次实践的目的是结合之前所学flume、hadoop、hive几个主要技术，完成一个小案例。<br>目标：<br>统计出独立ip数量<br>统计一个ip使用的header数量<br>访问最多的url链接 每个ip常访问的url<br>单日每小时访问量折线图</p><h1 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h1><p><strong>安装并配置好flume、hadoop、hive</strong><br><a href="https://blog.csdn.net/kun666666/article/details/121311179">hive安装配置</a><br><a href="https://blog.csdn.net/kun666666/article/details/120390257">hadoop安装配置</a><br><strong>数据源</strong> nginx日志文件access.log<br><img src="https://img-blog.csdnimg.cn/9ab8ab2468954e369d3d4e7457f6e20d.png" alt="access.log"><br>需修改nginx日志输出格式<br>满足如下格式，可减免数据清洗步骤</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">10.88.122.105  09:15:04  GET /js/pagination.js HTTP/1.1</span><br><span class="line">304 0 <span class="string">&quot;http://10.88.105.20:8063/stockrecommand.html&quot;</span> <span class="string">&quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/7.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)&quot;</span> </span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/4298f69df2ed48b39a38602976775269.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>准备一台服务器运行web服务，使用nginx做代理，使用flume采集nginx生产的日志上传至hdfs；</p><h2 id="修改hdfs默认配置"><a href="#修改hdfs默认配置" class="headerlink" title="修改hdfs默认配置"></a>修改hdfs默认配置</h2><p>因为我的网站每天只产生很少的数据量远小于hdfs默认的（128M）块大小因此为避免空间浪费需修改默认块大小</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#修改默认块大小</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>10240000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">propery</span>&gt;</span></span><br><span class="line">#修改检查块大小 满足块大小整除检查块大小</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.bytes-per-checksum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>512<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="yran资源管理器配置"><a href="#yran资源管理器配置" class="headerlink" title="yran资源管理器配置"></a>yran资源管理器配置</h2><p>在用hive操作时使用的内存会大于默认分配的资源因此需修改</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#使用物理内存大小</span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="flume采集方案"><a href="#flume采集方案" class="headerlink" title="flume采集方案"></a>flume采集方案</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers/flume/conf/</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources = source1</span><br><span class="line">agent1.sinks = sink1</span><br><span class="line">agent1.channels = channel1</span><br><span class="line"></span><br><span class="line">agent1.sources.source1.type = exec</span><br><span class="line">agent1.sources.source1.command = tail -F /www/wwwlogs/139.198.168.168.log</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line"></span><br><span class="line">agent1.sinks.sink1.hdfs.path =hdfs://master:9000/weblog/year=20%y/month=%m/day=%d</span><br><span class="line">agent1.sinks.sink1.hdfs.filePrefix = mylog</span><br><span class="line">agent1.sinks.sink1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.sink1.hdfs.writeFormat = Text</span><br><span class="line"># 当文件滚动  生成新文件</span><br><span class="line"># 配置文件滚动方式（文件大小10M） #8M</span><br><span class="line">agent1.sinks.sink1.hdfs.rollSize = 8000000</span><br><span class="line">agent1.sinks.sink1.hdfs.rollCount = 0</span><br><span class="line">agent1.sinks.sink1.hdfs.rollInterval = 0</span><br><span class="line"># 指的是正在写的hdfs文件多长时间不更新就关闭文件</span><br><span class="line">agent1.sinks.sink1.hdfs.idleTimeout = 5</span><br><span class="line">agent1.sinks.sink1.hdfs.minBlockReplicas = 1</span><br><span class="line"># 向hdfs上刷新的event的个数</span><br><span class="line"># 这三者之间的关系:batchsize &lt;=transactionCapacity&lt;=capacity</span><br><span class="line"># 就是sink会一次从channel中取多少个event去发送，而这个发送是要最终以事务的形式去发送的</span><br><span class="line">agent1.sinks.sink1.hdfs.batchSize = 10</span><br><span class="line"></span><br><span class="line"># 我们打算对时间戳根据分钟以每10分钟为单位进行四舍五入。</span><br><span class="line"># agent1.sinks.sink1.hdfs.round = true</span><br><span class="line"># agent1.sinks.sink1.hdfs.roundValue = 24</span><br><span class="line"># agent1.sinks.sink1.hdfs.roundUnit = hour</span><br><span class="line"> </span><br><span class="line">agent1.sinks.sink1.hdfs.useLocalTimeStamp = true</span><br><span class="line"> </span><br><span class="line">#使用通道在内存中缓冲事件</span><br><span class="line">agent1.channels.channel1.type = memory</span><br><span class="line">agent1.channels.channel1.keep-alive = 120</span><br><span class="line"># capacity是指整个队列的缓存最大容量</span><br><span class="line">agent1.channels.channel1.capacity = 1500</span><br><span class="line"># transactionCapacity则是指事务event的最大容量，即每次传输的event最大为多少</span><br><span class="line">agent1.channels.channel1.transactionCapacity = 100</span><br><span class="line"> </span><br><span class="line">#将源和接收器绑定到通道</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line">agent1.sinks.sink1.channel = channel1</span><br><span class="line">agent1.sinks.sink1.type = hdfs</span><br></pre></td></tr></table></figure><p><strong>启动flume采集</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line"><span class="comment">#执行参数解析</span></span><br><span class="line"><span class="built_in">nohup</span> flume-ng agent --conf /export/servers/flume-1.9.0/conf --conf-file flume-spark-push.properties -name a1  &gt;&gt; /export/data/flume/loglisence.log &amp;</span><br><span class="line"><span class="comment">#1、flume-ng  agent   运行一个Flume Agent</span></span><br><span class="line"><span class="comment">#2、--conf  指定配置文件路径</span></span><br><span class="line"><span class="comment">#3、--conf-file 收集方案文件</span></span><br><span class="line"><span class="comment">#4、-name  a1  Agent的名称 即上面的前缀 agent1</span></span><br></pre></td></tr></table></figure><p>查看收集的记录<br><img src="https://img-blog.csdnimg.cn/aa299b92e81e414eb521638420c354d4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h1 id="HIVE操作"><a href="#HIVE操作" class="headerlink" title="HIVE操作"></a>HIVE操作</h1><p>首先在hive创建日志表，</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> mlog(ip string,</span><br><span class="line">mtime string,</span><br><span class="line">url string,</span><br><span class="line">respCode <span class="type">int</span>,</span><br><span class="line">header string)</span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">year</span> string,<span class="keyword">month</span> string,<span class="keyword">day</span> string)</span><br><span class="line"><span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br><span class="line">location <span class="string">&#x27;mylog/&#x27;</span>;</span><br></pre></td></tr></table></figure><p>然后在hive shell commend repair一下mlog表 识别表分区</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">msck repair table mlog;</span><br></pre></td></tr></table></figure><p>将分区数据添加到metastore<br><img src="https://img-blog.csdnimg.cn/4ecb8ac70df74102a3d64b95c688e182.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="日志总行数"><a href="#日志总行数" class="headerlink" title="日志总行数"></a>日志总行数</h2><p>全部记录 5866479</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> log;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/f22834d76a25483285ae71f59c297207.png" alt="在这里插入图片描述"></p><h2 id="独立ip数量-2387"><a href="#独立ip数量-2387" class="headerlink" title="独立ip数量 2387"></a>独立ip数量 2387</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="keyword">distinct</span>(ip)) <span class="keyword">from</span> log;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/ef8dfdae43b84680ad6ec1e4e0511dec.png" alt="在这里插入图片描述"></p><h2 id="一个ip使用header数量"><a href="#一个ip使用header数量" class="headerlink" title="一个ip使用header数量"></a>一个ip使用header数量</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ip,<span class="built_in">count</span>(<span class="keyword">distinct</span>(header)) c  <span class="keyword">from</span> log <span class="keyword">group</span> <span class="keyword">by</span> ip <span class="keyword">order</span> <span class="keyword">by</span> c <span class="keyword">desc</span>；# limit <span class="number">10</span>;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/8a445506022a418387bce1e70f66dee7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="访问最多的url"><a href="#访问最多的url" class="headerlink" title="访问最多的url"></a>访问最多的url</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> url,<span class="built_in">count</span>(url) c <span class="keyword">from</span> log <span class="keyword">group</span> <span class="keyword">by</span> url <span class="keyword">order</span> <span class="keyword">by</span> c <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/cffea8a44fae471f9c4bc1acff954da0.png" alt="在这里插入图片描述"></p><h2 id="每个ip访问最多的url"><a href="#每个ip访问最多的url" class="headerlink" title="每个ip访问最多的url"></a>每个ip访问最多的url</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> ip,url,<span class="built_in">count</span>(url) c <span class="keyword">from</span> log <span class="keyword">group</span> <span class="keyword">by</span> ip,url <span class="keyword">order</span> <span class="keyword">by</span> c <span class="keyword">desc</span>； #limit <span class="number">300</span>;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/b48d0ae4a5ed4963b51af1be391a59c7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="每小时访问量-指定分区"><a href="#每小时访问量-指定分区" class="headerlink" title="每小时访问量 指定分区"></a>每小时访问量 指定分区</h2><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#不区分ip</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">substring</span>(mtime,<span class="number">0</span>,<span class="number">2</span>),<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> log <span class="keyword">where</span> <span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2021&#x27;</span> <span class="keyword">and</span> <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;11&#x27;</span> <span class="keyword">and</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;07&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="built_in">substring</span>(mtime,<span class="number">0</span>,<span class="number">2</span>) limit <span class="number">10</span>;</span><br><span class="line">#每小时独立ip访问量</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">substring</span>(mtime,<span class="number">0</span>,<span class="number">2</span>),<span class="built_in">count</span>(<span class="keyword">distinct</span>(ip)) <span class="keyword">from</span> log <span class="keyword">where</span> <span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2021&#x27;</span> <span class="keyword">and</span> <span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;11&#x27;</span> <span class="keyword">and</span> <span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;07&#x27;</span> <span class="keyword">group</span> <span class="keyword">by</span> <span class="built_in">substring</span>(mtime,<span class="number">0</span>,<span class="number">2</span>) limit <span class="number">10</span>;</span><br><span class="line">#</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/d21a0df9ae9440a68e754c02e522bed2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/a81dce7881b645c09c0df8bb045cd95f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p><strong>编写shell脚本将每天的小时访问量数据导出到mysql</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive -e <span class="string">&quot;create table qph as select substring(mtime,0,2),count(distinct(ip)) from log where year=&#x27;2021&#x27; and month=&#x27;11&#x27; and day=&#x27;07&#x27; group by substring(mtime,0,2) limit 10;&quot;</span></span><br><span class="line"></span><br><span class="line">sqoop <span class="built_in">export</span> --connect jdbc:mysql://localhost:3306/loginfo</span><br><span class="line">--uername hadoop --password <span class="built_in">pwd</span> --table daily </span><br><span class="line">--fields-terminated-by <span class="string">&#x27;\001&#x27;</span> --export-dir <span class="string">&#x27;user/hive/warehouse/log/qph&#x27;</span></span><br><span class="line"></span><br><span class="line">hive -e <span class="string">&quot;drop table qph;&quot;</span></span><br></pre></td></tr></table></figure><h2 id="添加定时任务"><a href="#添加定时任务" class="headerlink" title="添加定时任务"></a>添加定时任务</h2><p>crontab -e<br>加入以下内容<br>00 00 * * * &#x2F;bin&#x2F;sh &#x2F;user&#x2F;local&#x2F;src&#x2F;mysh&#x2F;daily.sh<br>每天凌晨0点执行昨天的日志统计任务，储存到mysql。</p><h1 id="可视化数据"><a href="#可视化数据" class="headerlink" title="可视化数据"></a>可视化数据</h1><p>6.4样例<br><img src="https://img-blog.csdnimg.cn/20210426221101345.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210426221124587.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>7.总结<br>实现了分析网站访问日志，统计出关键的信息(浏览量、注册数量、独立IP访问数量、跳出率)，统计各模块的访问数量，一天内每个时刻的四项指标访问量。可供网站决策者，也就是我自己，分析数据做出对热度较高的版块加关注，对热度低的板块做优化等。<br>缺陷与不足是，将网站和后台数据处理一并部署在一台云服务器上了，采用的单机，性能限制很大，做MapReduce清洗时服务器负载较大，特别容易挂掉。<br>遇到的问题与困难是，各项指标的sql和hql语句使用并不熟悉，导致导出的可使用的数据并不多。</p>]]></content>
    
    
    <summary type="html">Hive网站日志采集统计分析</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="hive sql" scheme="https://s-luping.github.io/luping/tags/hive-sql/"/>
    
  </entry>
  
  <entry>
    <title>Hive新浪微博日志查询分析</title>
    <link href="https://s-luping.github.io/luping/2021/10/11/Hive%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E2%BD%87%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>https://s-luping.github.io/luping/2021/10/11/Hive%E6%96%B0%E6%B5%AA%E5%BE%AE%E5%8D%9A%E2%BD%87%E5%BF%97%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</id>
    <published>2021-10-10T22:12:06.000Z</published>
    <updated>2022-03-13T12:47:45.767Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据描述"><a href="#数据描述" class="headerlink" title="数据描述"></a>数据描述</h1><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;beCommentWeiboId&quot;:&quot;&quot;,&quot;beForwardWeiboId&quot;:&quot;&quot;,&quot;catchTime&quot;:&quot;1387159495&quot;,&quot;co mmentCount&quot;:&quot;1419&quot;,&quot;content&quot;:&quot;分享图</span><br><span class="line">片&quot;,&quot;createTime&quot;:&quot;1386981067&quot;,&quot;info1&quot;:&quot;&quot;,&quot;info2&quot;:&quot;&quot;,&quot;info3&quot;:&quot;&quot;,&quot;mlevel&quot;:&quot;&quot; ,&quot;musicurl&quot;:[],&quot;pic_list&quot;: [&quot;http://ww3.sinaimg.cn/thumbnail/40d61044jw1ebixhnsiknj20qo0qognx.jpg&quot;],&quot; praiseCount&quot;:&quot;5265&quot;,&quot;reportCount&quot;:&quot;1285&quot;,&quot;source&quot;:&quot;iPad客户</span><br><span class="line">端&quot;,&quot;userId&quot;:&quot;1087770692&quot;,&quot;videourl&quot;: [],&quot;weiboId&quot;:&quot;3655325888057474&quot;,&quot;weiboUrl&quot;:&quot;http://weibo.com/1087770692/An dhixO7g&quot;&#125;] 2 [&#123;&quot;beCommentWeiboId&quot;:&quot;&quot;,&quot;beForwardWeiboId&quot;:&quot;&quot;,&quot;catchTime&quot;:&quot;1387159495&quot;,&quot;co mmentCount&quot;:&quot;91&quot;,&quot;content&quot;:&quot;行走：#去远方发现自己#@费勇主编，跨界明星联合执笔，</span><br><span class="line">分享他们观行思趣的心发现、他们的成长与心路历程，当当网限量赠送出品人@陈坤抄诵印刷版</span><br><span class="line">《心经》，赠完不再加印哦！详情请戳：</span><br><span class="line">http://t.cn/8k622Sj&quot;,&quot;createTime&quot;:&quot;1386925242&quot;,&quot;info1&quot;:&quot;&quot;,&quot;info2&quot;:&quot;&quot;,&quot;info 3&quot;:&quot;&quot;,&quot;mlevel&quot;:&quot;&quot;,&quot;musicurl&quot;:[],&quot;pic_list&quot;: [&quot;http://ww4.sinaimg.cn/thumbnail/b2336177jw1ebi6j4twk7j20m80tkgra.jpg&quot;],&quot; praiseCount&quot;:&quot;1&quot;,&quot;reportCount&quot;:&quot;721&quot;,&quot;source&quot;:&quot;&quot;,&quot;userId&quot;:&quot;2989711735&quot;,&quot;vi deourl&quot;: [],&quot;weiboId&quot;:&quot;3655091741442099&quot;,&quot;weiboUrl&quot;:&quot;http://weibo.com/2989711735/An 7bE639F&quot;&#125;]</span><br></pre></td></tr></table></figure><p>beCommentWeiboId 是否评论<br>beForwardWeiboId 是否是转发微博<br>catchTime 抓取时间<br>commentCount 评论次数<br>content 内容<br>createTime 创建时间<br>info1 信息字段1<br>info2信息字段2<br>info3信息字段3<br>mlevel no sure musicurl ⾳乐链接<br>pic_list 照⽚列表（可以有多个）<br>praiseCount 点赞⼈数<br>reportCount 转发⼈数<br>source 数据来源<br>userId ⽤户id<br>videourl 视频链接<br>weiboId 微博id<br>weiboUrl 微博⽹址</p><p>在hadoop创建目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfs -<span class="built_in">mkdir</span> weibo</span><br><span class="line">hadoop fs -put ./weibo/*  </span><br><span class="line">hadoop fs -<span class="built_in">ls</span> /weibo</span><br></pre></td></tr></table></figure><p>hive创建库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database <span class="keyword">if</span> not exists weibo;</span><br><span class="line">use weibo; </span><br><span class="line">create external table weibo(json string) location <span class="string">&#x27;/weibo&#x27;</span>;</span><br><span class="line">select * from weibo <span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure><p>处理json格式数据使用到get_json_object()和json_tuple(),其中两者都只认最外层是花括号 <strong>{ }</strong> 才能正常解析.最外层是时  <strong>[ ]</strong> 不能解析,<br>当最外层时 <strong>[]</strong> 时可使用substring方法去掉最外层 <strong>[ ]</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select get_json_object(substring(json,2,length(json)-1),<span class="string">&#x27;$.userId&#x27;</span>) from weibo <span class="built_in">limit</span> 1;</span><br></pre></td></tr></table></figure><h1 id="查询需求"><a href="#查询需求" class="headerlink" title="查询需求"></a>查询需求</h1><h2 id="微博总量和独立用户数"><a href="#微博总量和独立用户数" class="headerlink" title="微博总量和独立用户数"></a>微博总量和独立用户数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#总量</span></span><br><span class="line">select count(*) from weibo;</span><br><span class="line"><span class="comment">#独立用户数</span></span><br><span class="line">select count(distinct(get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>))) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-1) as j from weibo) a;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/ff32a818bf074450ac8b80feaf65a2c6.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/0ec5eac5eea9486791464924def4f964.png" alt="在这里插入图片描述"></p><h2 id="用户所有微博被转发的总数，输出前3个用户"><a href="#用户所有微博被转发的总数，输出前3个用户" class="headerlink" title="用户所有微博被转发的总数，输出前3个用户"></a>用户所有微博被转发的总数，输出前3个用户</h2><p>使用json_tuple提取多个字段</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">select b.id,<span class="built_in">sum</span>(b.cnt) as bsum </span><br><span class="line">from </span><br><span class="line">(select </span><br><span class="line">json_tuple(a.j,<span class="string">&#x27;userId&#x27;</span>,<span class="string">&#x27;reportCount&#x27;</span>) as (<span class="built_in">id</span>,cnt) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-1) as j from weibo) a) </span><br><span class="line">b </span><br><span class="line">group by b.id</span><br><span class="line">order by bsum desc</span><br><span class="line"><span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/3500581b3fc0441eba9f08671732b73f.png" alt="在这里插入图片描述"></p><h2 id="被转发次数最多的前3条微博，输出用户id"><a href="#被转发次数最多的前3条微博，输出用户id" class="headerlink" title="被转发次数最多的前3条微博，输出用户id"></a>被转发次数最多的前3条微博，输出用户id</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">select </span><br><span class="line">get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>) as <span class="built_in">id</span>,</span><br><span class="line">cast(get_json_object(a.j,<span class="string">&#x27;$.reportCount&#x27;</span>) as INT) as cnt </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-1) as j from weibo) a </span><br><span class="line">order by cnt desc </span><br><span class="line"><span class="built_in">limit</span> 3;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/b5267acdcb4d405c8941d4976cd0798b.png" alt="在这里插入图片描述"></p><h2 id="每个用户发布的微博总数，存储到临时表"><a href="#每个用户发布的微博总数，存储到临时表" class="headerlink" title="每个用户发布的微博总数，存储到临时表"></a>每个用户发布的微博总数，存储到临时表</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table weibo_uid_wbcnt( </span><br><span class="line">userid string, wbcnt int ) </span><br><span class="line">row format delimited </span><br><span class="line">fields terminated by <span class="string">&#x27;\t&#x27;</span>; </span><br><span class="line">insert overwrite table weibo_uid_wbcnt select get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>),count(1) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line">group by get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>); </span><br><span class="line"></span><br><span class="line">select * from weibo_uid_wbcnt <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/60bc41496fe548b4a1adb1c687e809a0.png" alt="在这里插入图片描述"></p><h2 id="统计带图片的微博数"><a href="#统计带图片的微博数" class="headerlink" title="统计带图片的微博数"></a>统计带图片的微博数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select count(1) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line"><span class="built_in">where</span> get_json_object(a.j,<span class="string">&#x27;$.pic_list&#x27;</span>) like <span class="string">&#x27;%http%&#x27;</span>; </span><br></pre></td></tr></table></figure><h2 id="统计使用iphone发微博的独立用户数"><a href="#统计使用iphone发微博的独立用户数" class="headerlink" title="统计使用iphone发微博的独立用户数"></a>统计使用iphone发微博的独立用户数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>)) </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line"><span class="built_in">where</span> lower(get_json_object(a.j,<span class="string">&#x27;$.source&#x27;</span>)) like <span class="string">&#x27;%iphone%&#x27;</span>;</span><br></pre></td></tr></table></figure><h2 id="微博中评论次数小于1000的用户id和数据来源，放入视图"><a href="#微博中评论次数小于1000的用户id和数据来源，放入视图" class="headerlink" title="微博中评论次数小于1000的用户id和数据来源，放入视图"></a>微博中评论次数小于1000的用户id和数据来源，放入视图</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">create view weibo_view as </span><br><span class="line">select get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>) as <span class="built_in">id</span>,get_json_object(a.j,<span class="string">&#x27;$.source&#x27;</span>) as <span class="built_in">source</span> </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line"><span class="built_in">where</span> get_json_object(a.j,<span class="string">&#x27;$.commentCount&#x27;</span>)&lt;1000; </span><br><span class="line">select * from weibo_view <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure><h2 id="统计上条视图中数据来源“ipad客户端”的用户数目"><a href="#统计上条视图中数据来源“ipad客户端”的用户数目" class="headerlink" title="统计上条视图中数据来源“ipad客户端”的用户数目"></a>统计上条视图中数据来源“ipad客户端”的用户数目</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select count(distinct <span class="built_in">id</span>) as cnt from weibo_view <span class="built_in">where</span> <span class="built_in">source</span>=<span class="string">&#x27;iPad客户端&#x27;</span>;</span><br></pre></td></tr></table></figure><h2 id="将微博的点赞数和转发数求和，降序，取前10条。"><a href="#将微博的点赞数和转发数求和，降序，取前10条。" class="headerlink" title="将微博的点赞数和转发数求和，降序，取前10条。"></a>将微博的点赞数和转发数求和，降序，取前10条。</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">public class DemoTest1 extends UDF &#123; </span><br><span class="line">public Integer evaluate(Integer num1,Integer num2)&#123; </span><br><span class="line">try&#123; </span><br><span class="line"><span class="built_in">return</span> num1+num2; </span><br><span class="line">&#125;catch (Exception e)&#123; </span><br><span class="line"><span class="built_in">return</span> null; &#125; </span><br><span class="line">&#125; </span><br><span class="line">&#125; </span><br><span class="line">create temporary <span class="keyword">function</span> wb as <span class="string">&#x27;DemoTest1&#x27;</span>;</span><br><span class="line"></span><br><span class="line">select wb(cast(get_json_object(a.j,<span class="string">&#x27;$.praiseCount&#x27;</span>) as int),cast(get_json_object(a.j,<span class="string">&#x27;$.reportCount&#x27;</span>) as int)) as cnt </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a </span><br><span class="line">order by cnt desc <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure><h2 id="⽤户微博内容中出现iphone关键词的最⼤次数"><a href="#⽤户微博内容中出现iphone关键词的最⼤次数" class="headerlink" title="⽤户微博内容中出现iphone关键词的最⼤次数"></a>⽤户微博内容中出现iphone关键词的最⼤次数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">public class DemoTest2 extends UDF &#123; </span><br><span class="line">public int evaluate(String content,String word)&#123; </span><br><span class="line">int count = 0; </span><br><span class="line"><span class="keyword">if</span>(content != null&amp;&amp;content.length()&gt;0)&#123; </span><br><span class="line">String[] array = content.split(word); </span><br><span class="line">count = array.length-1; </span><br><span class="line">&#125; </span><br><span class="line"><span class="built_in">return</span> count; </span><br><span class="line">&#125; </span><br><span class="line">&#125;</span><br><span class="line">create temporary <span class="keyword">function</span> wcount as <span class="string">&#x27;DemoTest2&#x27;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">select b.id,max(b.cnt) as cn </span><br><span class="line">from </span><br><span class="line">(select get_json_object(a.j,<span class="string">&#x27;$.userId&#x27;</span>) as <span class="built_in">id</span>,wcount(get_json_object(a.j,<span class="string">&#x27;$.content&#x27;</span>),<span class="string">&#x27;iphone&#x27;</span>) as cnt </span><br><span class="line">from </span><br><span class="line">(select substring(json,2,length(json)-2) as j from weibo) a) b </span><br><span class="line">group by b.id </span><br><span class="line">order by cn desc <span class="built_in">limit</span> 10;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Hive新浪微博日志查询分析</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="hive sql UDF" scheme="https://s-luping.github.io/luping/tags/hive-sql-UDF/"/>
    
  </entry>
  
  <entry>
    <title>Hive学习记录</title>
    <link href="https://s-luping.github.io/luping/2021/10/04/Hive%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://s-luping.github.io/luping/2021/10/04/Hive%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2021-10-03T22:19:17.000Z</published>
    <updated>2022-05-25T16:12:01.350Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装配置"><a href="#安装配置" class="headerlink" title="安装配置"></a>安装配置</h1><p>1.下载 2.解压 3.重命名 4.添加环境变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/proflie</span><br><span class="line"><span class="built_in">export</span> HIVE_HOME=/export/servers/hive-2.3.8</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HIVE_HOME</span>/bin</span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><p>修改配置文件<br>cp hive-env.sh.template hive-env.sh<br><img src="https://img-blog.csdnimg.cn/db764ab0fdf64f6d889b9969b71d90fd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="Hive-Metastore配置"><a href="#Hive-Metastore配置" class="headerlink" title="Hive Metastore配置"></a>Hive Metastore配置</h2><p>将自带的derby数据库替换为mysql数据库 可使多用户连接</p><p>参考文章 <a href="">https://my.oschina.net/u/4292373/blog/3497563</a></p><h2 id="新增hive-site-xml文件"><a href="#新增hive-site-xml文件" class="headerlink" title="新增hive-site.xml文件"></a>新增hive-site.xml文件</h2><p><strong>记坑</strong><br>hive-default.xml.template 的开头就写明了 WARNING!!!对该文件的任何更改都将被Hive忽略<br>其实hive-site.xml是用户定义的配置文件，hive在启动的时候会读取两个文件一个是hive-default.xml.template 还有一个就是hive-site.xml<br>在复制的hive-site.xml里保存你写的配置项，然后将其他的删掉，hive-site.xml只能写你自己的配置项，其他删掉</p><p>原文链接：<a href="">https://blog.csdn.net/qq_43506520&#x2F;article&#x2F;details&#x2F;83346463</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cp</span> hive-default.xml.template hive-site.xml </span><br><span class="line">vi hive-site.xml  在hive-site.xml</span><br><span class="line"><span class="comment">#文件只保存如下配置</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的URL 01--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop01:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的Driver 02--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.cj.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的username 03--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- jdbc连接的password 04--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--元数据访问地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://hadoop01:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- Hive默认在HDFS的工作目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.sql.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>metastore.catalog.default<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--鉴权设置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.enable.doAs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.users.in.admin.role<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authorization.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.security.authenticator.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--配置远程访问hive beeline --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.0.0.0<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.client.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.client.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root123<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 校验在metastore中存储的信息的版本和hive的jar包中的版本一致性--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--高可用集群设置设置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.support.dynamic.service.discovery<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.zookeeper.namespace<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hiveserver2_zk<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:2181,hadoop02:2181,hadoop03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.zookeeper.client.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--调试日志--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.show.job.failure.debug.info<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--允许单条插入--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.support.concurrency<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.enforce.bucketing<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.dynamic.partition.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>nonstrict<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.txn.manager<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.compactor.initiator.on<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.compactor.worker.threads<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.in.test<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>JDBC驱动</strong></p><p>把连接Mysql的JDBC驱动包复制到Hive的lib目录下 </p><p>下载地址：<a href="">https://dev.mysql.com/downloads/connector/j</a></p><p>驱动包名为：mysql-connector-java-5.1.48-bin.jar</p><p><strong>初始化数据库</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure><p>若失败 错误类型和参考如下</p><p><a href="https://blog.csdn.net/lsr40/article/details/78026125">https://blog.csdn.net/lsr40/article/details/78026125</a></p><p><a href="https://blog.csdn.net/brotherdong90/article/details/49661731/">https://blog.csdn.net/brotherdong90/article/details/49661731/</a></p><p><strong>开启metastore</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> hive --service metastore &amp; <span class="comment">#开启元数据服务 默认9083</span></span><br></pre></td></tr></table></figure><p><strong>开启hiveserver2</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> hiveserver2 &amp;</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置远程访问hive的用户密码--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.client.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.client.password<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root123<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>启动出现临时文件夹位置未定义问题</strong></p><p>解决方案参考如下文章：<br><a href="https://www.cnblogs.com/qxyy/articles/5247933.html">https://www.cnblogs.com/qxyy/articles/5247933.html</a></p><p>查看系统存储的hive运行日志</p><p>在.&#x2F;conf&#x2F;hive-log4j2.properties文件中记录系统日志位置，默认&#x2F;tmp&#x2F;user&#x2F;hive.log</p><h1 id="数据定义"><a href="#数据定义" class="headerlink" title="数据定义"></a>数据定义</h1><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#创建</span><br><span class="line">create database  if not exists emp；</span><br><span class="line">#查看</span><br><span class="line">show databases；</span><br><span class="line">#描述</span><br><span class="line">describe formatted emp；</span><br><span class="line">#使用</span><br><span class="line">use emp；</span><br><span class="line">#修改</span><br><span class="line">alter database set dbproperty;</span><br></pre></td></tr></table></figure><h2 id="数据表"><a href="#数据表" class="headerlink" title="数据表"></a>数据表</h2><p>创建普通内部表</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table employee(eid int,ename string,egender tinyint,esalary float);</span><br></pre></td></tr></table></figure><p>创建外部表</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create external table emp(eid int,ename string,egender tinyint,esalary float);</span><br></pre></td></tr></table></figure><p>内部表外部表转换</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table table_name set tablepropertiles(‘external’=’true’|false);</span><br></pre></td></tr></table></figure><h2 id="分隔符"><a href="#分隔符" class="headerlink" title="分隔符"></a>分隔符</h2><p><strong>列分隔符</strong> <strong>行分隔符</strong> <strong>Array分隔符</strong> <strong>Map分隔符</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">create table sales_info_new(</span><br><span class="line">sku_id string comment &#x27;商品id&#x27;,</span><br><span class="line">sku_name string comment &#x27;商品名称&#x27;,</span><br><span class="line">state_map map&lt;string,string&gt; comment &#x27;商品状态信息&#x27;,</span><br><span class="line">id_array array&lt;string&gt; comment &#x27;商品相关id列表&#x27;</span><br><span class="line">)</span><br><span class="line">partitioned by(</span><br><span class="line">dt string comment &#x27;年-月-日&#x27;</span><br><span class="line">)</span><br><span class="line">row format delimited</span><br><span class="line">  fields terminated by &#x27;|&#x27;</span><br><span class="line">  collection items terminated by &#x27;,&#x27;</span><br><span class="line">  map keys terminated by &#x27;:&#x27;;      </span><br></pre></td></tr></table></figure><p>导入数据样例</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">123|华为Mate10|id:1111,token:2222,user_name:zhangsan1|1235,345</span><br><span class="line">456|华为Mate30|id:1113,token:2224,user_name:zhangsan3|89,635</span><br><span class="line">789|小米5|id:1114,token:2225,user_name:zhangsan4|452,63</span><br><span class="line">1235|小米6|id:1115,token:2226,user_name:zhangsan5|785,36</span><br><span class="line">4562|OPPO Findx|id:1116,token:2227,user_name:zhangsan6|7875,3563</span><br></pre></td></tr></table></figure><p><strong>文本：不用写双引号，花括号，程序会自动添加双引号和花括号！ 加了会出错</strong></p><h2 id="分区分桶"><a href="#分区分桶" class="headerlink" title="分区分桶"></a>分区分桶</h2><h3 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h3><p>为什么分：</p><p>使用分区技术，避免hive全表扫描，提升查询效率</p><p>如何分：</p><p>整个表的数据在存储时划分到多个子目录，从而在查询时可以指定查询条件（子目录以分区变量的值来命名）eg:year&#x3D;‘2018’</p><p>分区需注意什么：</p><p>PARTIONED BY(colName dataType)<br>hive的分区字段使用的是表外字段。而mysql使用的是表内字段。</p><p>1、hive的分区名区分大小写 不能使用中文</p><p>2、hive的分区本质是在表目录下面创建目录，但是该分区字段是一个伪列，不真实存在于数据中</p><p>3、一张表可以有一个或者多个分区，分区下面也可以有一个或者多个分区</p><p><strong>导入分区</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data local inpath &#x27;/usr/local/xxx&#x27; into table part1 partition(country=&#x27;China&#x27;); #要指定分区</span><br></pre></td></tr></table></figure><p><strong>#修改分区的存储路径：(hdfs的路径必须是全路径)</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table part1 partition(country=&#x27;Vietnam&#x27;) set location ‘hdfs://hadoop01:9000/user/hive/warehouse/brz.db/part1/country=Vietnam’</span><br></pre></td></tr></table></figure><p><strong>二级分区</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists part2(</span><br><span class="line">uid int,</span><br><span class="line">uname string,</span><br><span class="line">uage int</span><br><span class="line">)</span><br><span class="line">PARTITIONED BY (year string,month string)</span><br><span class="line">row format delimited </span><br><span class="line">fields terminated by &#x27;,&#x27;;</span><br><span class="line"># 导入多分区</span><br><span class="line">load data local inpath &#x27;/usr/local/xxx&#x27; into table part1 partition(year=&#x27;2018&#x27;,month=&#x27;09&#x27;); </span><br></pre></td></tr></table></figure><p><strong>增加分区</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">alter table part1 add partition(country=&#x27;india&#x27;) partition(country=&#x27;korea&#x27;) partition(country=&#x27;America&#x27;)</span><br></pre></td></tr></table></figure><p><strong>动态分区</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#动态分区的属性：</span><br><span class="line">set hive.exec.dynamic.partition=true;//(true/false)</span><br><span class="line">set hive.exec.dynamic.partition.mode=strict;//(strict/nonstrict) #至少有一个静态的值</span><br><span class="line">set hive.exec.dynamic.partitions=1000;//(分区最大数)</span><br><span class="line">set hive.exec.max.dynamic.partitions.pernode=100</span><br><span class="line">#创建动态分区表</span><br><span class="line">create table if not exists dt_part1(</span><br><span class="line">uid int,</span><br><span class="line">uname string,</span><br><span class="line">uage int</span><br><span class="line">)</span><br><span class="line">PARTITIONED BY (year string,month string)</span><br><span class="line">row format delimited </span><br><span class="line">fields terminated by &#x27;,&#x27;</span><br><span class="line">;</span><br><span class="line">#加载数据：（使用 insert into方式加载数据）</span><br><span class="line">insert into dy_part1 partition(year,month) select * from part_tmp ;</span><br></pre></td></tr></table></figure><h3 id="分桶"><a href="#分桶" class="headerlink" title="分桶"></a>分桶</h3><p>在分区下分桶，分桶使用表内字段</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法格式</span><br><span class="line">CREATE TABLE test</span><br><span class="line">(&lt;col_name&gt; &lt;data_type&gt; [, &lt;col_name&gt; &lt;data_type&gt; ...])]</span><br><span class="line">[PARTITIONED BY ...]</span><br><span class="line">CLUSTERED BY (&lt;col_name&gt;)</span><br><span class="line">[SORTED BY (&lt;col_name&gt; [ASC|DESC] [, &lt;col_name&gt; [ASC|DESC]...])]</span><br><span class="line">INTO &lt;num_buckets&gt; BUCKETS</span><br></pre></td></tr></table></figure><p>CLUSTERED BY (<col_name>) 以哪一列进行分桶 选择一列来分桶</p><p>SORTED BY (<col_name> [ASC|DESC] 对分桶内的数据进行排序</p><p>INTO <num_buckets> BUCKETS 分成几个桶</p><p>##列信息更改<br>修改名称</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Alter table emp change eid id string;</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/18affe192e0e401688465b35b82ee73c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>增加列<br><img src="https://img-blog.csdnimg.cn/5dc7baa42431479784722bda4f2cf314.png" alt="在这里插入图片描述"></p><h1 id="数据操作"><a href="#数据操作" class="headerlink" title="数据操作"></a>数据操作</h1><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#装载数据</span><br><span class="line">Load data to table inpath </span><br><span class="line">#插入数据</span><br><span class="line">Insert into table emp partition(year=2021,month=10) select id,name from ept;</span><br><span class="line">#导出数据</span><br><span class="line">#到hdfs</span><br><span class="line">Export table ept to ‘/hom/emp’;</span><br><span class="line">#Insert 导出</span><br><span class="line">Insert overwrite local directory ‘path’ select * from emp;</span><br><span class="line">#到本地</span><br><span class="line">Hfds dfs -get localpath</span><br><span class="line">#Hive shell 命令导出</span><br><span class="line">Hive -e ‘select * from emp;’ &gt; localpath</span><br><span class="line">#导入数据</span><br><span class="line">Import table emp  from path;</span><br><span class="line">#HQL查询</span><br><span class="line">Case when </span><br><span class="line">Select name,salary, case</span><br><span class="line">Wehn salary &lt;5000  then ‘low’</span><br><span class="line">When salary &gt;=5000 and salary &lt;7000 then ‘middle’</span><br><span class="line">Whne salary &gt;=7000 then salary &lt; 10000 then ‘high’</span><br><span class="line">Else ‘vary high’</span><br><span class="line">End as bracket from emp;</span><br></pre></td></tr></table></figure><p>##Like和rlike<br>使用Like运算符可以进行模糊查询，通配符”%”代表0个或多个字符，”_”代表1个字符。</p><p>RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。</p><h2 id="GROUP-BY"><a href="#GROUP-BY" class="headerlink" title="GROUP BY"></a>GROUP BY</h2><p>GROUP BY语句通常会和聚合函数一起使用，按照一个或者多个队列结果进行分组，然后对每个组执行聚合操作</p><h2 id="HAVING"><a href="#HAVING" class="headerlink" title="HAVING"></a>HAVING</h2><p>在 text 中增加 HAVING 子句原因是，WHERE 关键字无法与合计函数一起使用。</p><p>Having 与where不同</p><p>（1）where是对表中数据的筛选，having是对分组统计结果的筛选</p><p>（2）Where后不能写分组函数，而having后可以使用分组函数。</p><p>（3）Having只用于group by分组统计语句。</p><p>SELECT Customer,SUM(OrderPrice) FROM Orders<br>GROUP BY Customer<br>HAVING SUM(OrderPrice)&lt;2000</p><h2 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h2><h3 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h3><p>内连接（INNER JOIN）中，只有进⾏连接的两个表中都存在与连接条件相匹配的数据 时，记录才会被筛选出来</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.empno,a.ename,b.dname FROM emp a JOIN dept b ON a.deptno=b.deptno;</span><br></pre></td></tr></table></figure><h3 id="左连接"><a href="#左连接" class="headerlink" title="左连接"></a>左连接</h3><p>左外连接（LEFT OUTER JOIN）中，JOIN操作符左边表中符合WHERE⼦句的所有记 录将会出现在查询结果中。右边表中如果没有符合ON后⾯连接条件的记录时，从右边表 指定选择的列的值将会是NULL。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.empno,a.ename,b.dname FROM emp a LEFT OUTER JOIN dept b ON a.deptno==b.deptno;</span><br></pre></td></tr></table></figure><h3 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h3><p><img src="https://img-blog.csdnimg.cn/49006f6c6d35416db52f662fdaab8791.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/5f62acabaa6444549dee737ae1d72303.png" alt="在这里插入图片描述"></p><h3 id="多表连接"><a href="#多表连接" class="headerlink" title="多表连接"></a>多表连接</h3><p>连接 n个表，⾄少需要n-1个连接条件。例如：连接三个表，⾄少需要两个连接条件。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT a.ename,b.dname,c.zip FROM emp a JOIN dept b ON a.deptno=b.deptno JOIN location c ON b.loc=c.loc;</span><br></pre></td></tr></table></figure><p>注意：为什么不是表b和表c先进⾏连接操作呢？这是因为Hive总是按照从左到右的顺序 执⾏的。</p><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>###ORDER BY<br>ORDER BY⽤于对全局查询结果进⾏排序，也就是说会有⼀个所有的数据都通过⼀个 reducer进⾏处理的过程。 </p><h3 id="SORT-BY"><a href="#SORT-BY" class="headerlink" title="SORT BY"></a>SORT BY</h3><p>Hive增加了⼀个可供选择的⽅式，即SORT BY，其只会在每个reducer中对数据进⾏排 序，即执⾏⼀个局部排序过程。这会保证每个reducer的输出数据都是有序的（但并⾮ 全局有序）。<br>ORDER BY 和SORT BY的区别是当reducer的个数⼤于1时，两种操作的输出结果是不 同的，SORT BY是reducer内的局部排序。<br>###DISTRIBUTE BY和SORT BY<br>如果我们想对同⼀部⻔中的员⼯进⾏排序处理，那么我们可以使⽤DISTRIBUTE BY来保 证具有相同部⻔编号的员⼯被分到同⼀个reducer中去，然后使⽤SORT BY来按照我们 的期望对数据进⾏排序。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM emp DISTRIBUTE BY deptno SORT BY empno DESC;</span><br></pre></td></tr></table></figure><p>###CLUSTER BY<br>当distribute by和sorts by字段相同时，可以使⽤cluster by⽅式。 ⽤cluster b除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序 排序，不能指定排序规则为ASC或者DESC。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">select * from emp cluster by deptno; </span><br><span class="line">select * from emp distribute by deptno sort by deptno;</span><br></pre></td></tr></table></figure><h2 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h2><p>Hive会在适当的时候对数值型数据类型进⾏隐式类型转换，有些时候需要显示类型转换 时可以使⽤关键字cast。</p><p>显示类型转换函数的语法是: </p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cast(value AS TYPE)</span><br><span class="line">ALTER TABLE employees CHANGE COLUMN salary salary STRING; </span><br><span class="line">SELECT name,salary FROM employees WHERE cast(salary AS FLOAT) &lt; 100000.0;</span><br></pre></td></tr></table></figure><h2 id="空字段赋值"><a href="#空字段赋值" class="headerlink" title="空字段赋值"></a>空字段赋值</h2><p>NVL：给值为NULL的数据赋值，它的格式是NVL( string1, replace_with)。它的功能是如果 string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为 NULL ，则返回NULL。</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select nvl(comm,-1) from emp;</span><br></pre></td></tr></table></figure><h1 id="Hive-合并小文件"><a href="#Hive-合并小文件" class="headerlink" title="Hive 合并小文件"></a>Hive 合并小文件</h1><p>当hive中数据都是由小文件组成时，需要将这些小文件合并为一个大的文件</p><p>步骤</p><p>创建临时表</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table test like table1;</span><br></pre></td></tr></table></figure><p>在当前会话设置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> hive.exec.dynamic.partition=<span class="literal">true</span>;</span><br><span class="line"><span class="built_in">set</span> hive.exec.dynamic.partition.mode=nostrict;</span><br><span class="line"><span class="built_in">set</span> hive.exec.max.dynamic.partitions=100000;</span><br><span class="line"><span class="built_in">set</span> hive.merge.smallfiles.avgsize=128000000; <span class="comment">#128M</span></span><br><span class="line"><span class="built_in">set</span> hive.merge.size.per.task=128000000;</span><br></pre></td></tr></table></figure><p>将原表数据合并到临时表</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">insert overwrite table test select * from table;</span><br></pre></td></tr></table></figure><p>创建备份目录将原表数据放进备份目录，将临时表数据迁移至原表目录</p><h1 id="hive数据倾斜问题"><a href="#hive数据倾斜问题" class="headerlink" title="hive数据倾斜问题"></a>hive数据倾斜问题</h1><p>核心解决方案时多段聚合，将第一次聚合时每个key加上随机数，对数据打散，在进行二次聚合。</p><h2 id="group-by-distinct"><a href="#group-by-distinct" class="headerlink" title="group by distinct"></a>group by distinct</h2><p>进行group by 时形成 key val_list,当某些key重复数据较多时，就会产生数据倾斜</p><p>核心解决方案时多段聚合，将第一次聚合时每个key加上随机数，对数据打散，在进行二次聚合。</p><h2 id="join导致的"><a href="#join导致的" class="headerlink" title="join导致的"></a>join导致的</h2><p><img src="/luping/2021/10/04/Hive%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_2.png" alt="img_2.png"><br>找出on的字段重复</p><h1 id="hive-text-如何转换为mapreduce任务"><a href="#hive-text-如何转换为mapreduce任务" class="headerlink" title="hive text 如何转换为mapreduce任务"></a>hive text 如何转换为mapreduce任务</h1><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">select userid from user group by userid;</span><br></pre></td></tr></table></figure><p>group by 的字段组合作为 map任务输出key的值，同时作为reduce得输入，在map阶段之后的分区、排序时reduce将相同的key放到一起来处理<br><img src="/luping/2021/10/04/Hive%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_3.png" alt="img_3.png"></p>]]></content>
    
    
    <summary type="html">Hive学习记录</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="Hive" scheme="https://s-luping.github.io/luping/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>spark job的几种提交流程</title>
    <link href="https://s-luping.github.io/luping/2021/10/02/spark%20job%E7%9A%84%E5%87%A0%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/"/>
    <id>https://s-luping.github.io/luping/2021/10/02/spark%20job%E7%9A%84%E5%87%A0%E7%A7%8D%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B/</id>
    <published>2021-10-01T20:43:38.000Z</published>
    <updated>2022-03-13T12:27:25.080Z</updated>
    
    <content type="html"><![CDATA[<h1 id="standalone"><a href="#standalone" class="headerlink" title="standalone"></a>standalone</h1><p>集群启动后worker向master注册信息，<br>通过spark-submit提交任务时，在任务提交节点或Client启动driver，<br>在driver创建并初始化sparkContext对象，包含DAGScheduler和TaskScheduler，TaskScheduler与Master节点通讯申请注册Application，Master节点接收到Application的注册请求后，通过资源调度算法，在自己的集群的worker上启动Executor进程；启动的Executor也会反向注册到TaskScheduler上<br>DAGScheduler：负责把Spark作业转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；<br>所有task运行完成后，SparkContext向Master注销，释放资源；<br><img src="https://img-blog.csdnimg.cn/94cb178cb14e4ccf982cd5420e795086.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><a href="https://blog.csdn.net/bokzmm/article/details/79476409">参考文章</a></p><h1 id="spark-on-yarn"><a href="#spark-on-yarn" class="headerlink" title="spark on yarn"></a>spark on yarn</h1><p>配置<br>在client节点配置中spark-env.sh添加Hadoop_HOME的配置目录即可提交yarn 任务，具体步骤如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=<span class="variable">$HADOOP_HOME</span>/etc/hadoop</span><br></pre></td></tr></table></figure><p>注意client只需要有Spark的安装包即可提交任务，不需要其他配置（比如slaves）!!!</p><h2 id="client"><a href="#client" class="headerlink" title="client"></a>client</h2><p><img src="https://img-blog.csdnimg.cn/e6dd5707482141ec8537ab908cb4c20e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>SparkContext在Client创建并实例化<br>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler<br>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster<br>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源<br>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver<br>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task<br>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#当前目录</span></span><br><span class="line"><span class="comment">#/export/servers/spark-2.2.3/</span></span><br><span class="line">spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn --deploy-mode client \</span><br><span class="line">--conf spark.driver.host=192.168.88.12 \</span><br><span class="line">examples/jars/spark-examples_2.11-2.2.3.jar 10</span><br></pre></td></tr></table></figure><p>–class 是全限定类名<br>–master yarn 是指使用yarn管理<br>–deploy-mode <strong>client</strong>*<em>cluster</em>* client可以在提交任务的机器查看结果<br>cluster只能在yarn上看结果<br>–conf spark.driver.host&#x3D;192.168.88.12 driver监听的主机名或者IP地址。就是提交任务机器的地址，这用于和executors以及独立的master通信接收结果<br>examples&#x2F;jars&#x2F;spark-examples_2.11-2.2.3.jar 运行的jar包<br>当我们测试一个demo如下图会为spark任务自动机器CPU、内存等资源<br><img src="https://img-blog.csdnimg.cn/9bf8d66eaae74d81af5a8471050826ef.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="Cluster"><a href="#Cluster" class="headerlink" title="Cluster"></a>Cluster</h2><p>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler<br>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster<br>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源<br>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver<br>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task<br>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p><h2 id="Yarn-client和Yarn-cluster的区别："><a href="#Yarn-client和Yarn-cluster的区别：" class="headerlink" title="Yarn-client和Yarn-cluster的区别："></a>Yarn-client和Yarn-cluster的区别：</h2><p>yarn-cluster模式下，Dirver运行在ApplicationMaster中，负责申请资源并监控task运行状态和重试失败的task，当用户提交了作业之后就可以关掉client，作业会继续在yarn中运行；<br>yarn-client模式下，Dirver运行在本地客户端，client不能离开。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;standalone&quot;&gt;&lt;a href=&quot;#standalone&quot; class=&quot;headerlink&quot; title=&quot;standalone&quot;&gt;&lt;/a&gt;standalone&lt;/h1&gt;&lt;p&gt;集群启动后worker向master注册信息，&lt;br&gt;通过spark-sub</summary>
      
    
    
    
    
    <category term="spark" scheme="https://s-luping.github.io/luping/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark学习记录</title>
    <link href="https://s-luping.github.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://s-luping.github.io/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2021-09-14T03:38:03.000Z</published>
    <updated>2022-03-29T11:22:58.560Z</updated>
    
    <content type="html"><![CDATA[<h1 id="spark介绍"><a href="#spark介绍" class="headerlink" title="spark介绍"></a>spark介绍</h1><p>Spark是加州大学伯克利分校AMP实验室（Algorithms, Machines, and People Lab）开发的通用内存并行计算框架<br>Spark使用Scala语言进行实现，它是一种面向对象、函数式编程语言，能够像操作本地集合对象一样轻松地操作分布式数据集，具有以下特点:<br>1.运行速度快：Spark拥有DAG执行引擎，支持在内存中对数据进行迭代计算。官方提供的数据表明，如果数据由磁盘读取，速度是Hadoop MapReduce的10倍以上，如果数据从内存中读取，速度可以高达100多倍。<br>2.易用性好：Spark不仅支持Scala编写应用程序，而且支持Java和Python等语言进行编写，特别是Scala是一种高效、可拓展的语言，能够用简洁的代码处理较为复杂的处理工作。<br>3.通用性强：Spark生态圈即BDAS（伯克利数据分析栈）包含了Spark Core、Spark SQL、Spark Streaming、MLLib和GraphX等组件，这些组件分别处理Spark Core提供内存计算框架、SparkStreaming的实时处理应用、Spark SQL的即席查询、MLlib或MLbase的机器学习和GraphX的图处理。<br>4.随处运行：Spark具有很强的适应性，能够读取HDFS、Cassandra、HBase、S3和Techyon为持久层读写原生数据，能够以Mesos、YARN和自身携带的Standalone作为资源管理器调度job，来完成Spark应用程序的计算</p><h2 id="Spark与Hadoop差异"><a href="#Spark与Hadoop差异" class="headerlink" title="Spark与Hadoop差异"></a>Spark与Hadoop差异</h2><p>Spark是在借鉴了MapReduce之上发展而来的，继承了其分布式并行计算的优点并改进了MapReduce明显的缺陷，具体如下:<br>首先，Spark把中间数据放到内存中，迭代运算效率高。MapReduce中计算结果需要落地，保存到磁盘上，这样势必会影响整体速度，而Spark支持DAG图的分布式并行计算的编程框架，减少了迭代过程中数据的落地，提高了处理效率。<br>其次，Spark容错性高。Spark引进了弹性分布式数据集RDD (Resilient Distributed Dataset) 的抽象，它是分布在一组节点中的只读对象集合，这些集合是弹性的，如果数据集一部分丢失，则可以根据“血统”（即充许基于数据衍生过程）对它们进行重建。另外在RDD计算时可以通过CheckPoint来实现容错，而CheckPoint有两种方式：CheckPoint Data，和Logging The Updates，用户可以控制采用哪种方式来实现容错。<br>最后，Spark更加通用。不像Hadoop只提供了Map和Reduce两种操作，Spark提供的数据集操作类型有很多种，大致分为：Transformations和Actions两大类。Transformations包括Map、Filter、FlatMap、Sample、GroupByKey、ReduceByKey、Union、Join、Cogroup、MapValues、Sort和PartionBy等多种操作类型，同时还提供Count, Actions包括Collect、Reduce、Lookup和Save等操作。另外各个处理节点之间的通信模型不再像Hadoop只有Shuffle一种模式，用户可以命名、物化，控制中间结果的存储、分区等。<br><a href="https://blog.csdn.net/kxiaozhuk/article/details/82699175">原文链接</a></p><h1 id="spark安装"><a href="#spark安装" class="headerlink" title="spark安装"></a>spark安装</h1><h2 id="下载解压"><a href="#下载解压" class="headerlink" title="下载解压"></a>下载解压</h2><p>下载安装包 解压到本地软件安装目录<br><a href="https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8.tgz">spark-2.4.8.tgz</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers</span><br><span class="line">wget https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8.tgz</span><br><span class="line">tar xvf spark-2.4.8.tgz .</span><br></pre></td></tr></table></figure><h2 id="添加系统环境变量"><a href="#添加系统环境变量" class="headerlink" title="添加系统环境变量"></a>添加系统环境变量</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/export/servers/spark-2.4.8</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h2 id="spark-shell"><a href="#spark-shell" class="headerlink" title="spark-shell"></a>spark-shell</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell</span><br></pre></td></tr></table></figure><p><img src="/luping/img_6.png" alt="img_5.png"></p><h1 id="spark任务提交执行"><a href="#spark任务提交执行" class="headerlink" title="spark任务提交执行"></a>spark任务提交执行</h1><h2 id="standalone-spark自主管理的集群模式"><a href="#standalone-spark自主管理的集群模式" class="headerlink" title="standalone spark自主管理的集群模式"></a>standalone spark自主管理的集群模式</h2><p><strong>要配置spark安装目录下的slaves文件</strong>添加本地注意域名映射<br><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_7.png" alt="img_7.png"><br>通过spark-submit提交任务时，在任务提交节点或Client启动driver，<br>在driver创建并初始化sparkContext对象包含DAGScheduler和TaskScheduler，<br>与master通信申请资源，master指派worker为其启动executor<br>生成job阶段，遇到行动算子生成一个job<br>DAGScheduler负责把Sparkjob转换成Stage的DAG（Directed Acyclic Graph有向无环图），根据宽窄依赖切分Stage，然后把Stage封装成TaskSet的形式发送个TaskScheduler；<br>TaskScheduler与Master节点通讯申请注册Application，Master节点接收到Application的注册请求后，通过资源调度算法，在自己的集群的worker上启动Executor进程；启动的Executor也会反向注册到TaskScheduler上<br>所有task运行完成后，SparkContext向Master注销，释放资源；<br>Stage阶段划分<br>根据宽依赖窄依赖划分阶段，判断宽依赖和窄依赖的依据是是否进行shuffle操作，不需要shuffle的窄依赖分到一个阶段中间的RDD转换操作无需落地，而宽依赖需要shuffle的过程数据需要落地磁盘</p><h2 id="spark-on-yarn-提交到hadoop的yarn集群执行"><a href="#spark-on-yarn-提交到hadoop的yarn集群执行" class="headerlink" title="spark on yarn 提交到hadoop的yarn集群执行"></a>spark on yarn 提交到hadoop的yarn集群执行</h2><p><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_8.png" alt="img_8.png"><br>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler<br>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster<br>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源<br>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver<br>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task<br>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p><h1 id="spark的模块"><a href="#spark的模块" class="headerlink" title="spark的模块"></a>spark的模块</h1><h2 id="spark-Core"><a href="#spark-Core" class="headerlink" title="spark Core"></a>spark Core</h2><p>###RDD<br>Spark提供的主要抽象是弹性分布式数据集(RDD),它是跨集群节点分区的元素集合,可以并行操作.<br>RDD特点:</p><ul><li>1.它是在集群节点上的不可变的、已分区的集合对象;</li><li>2.通过并行转换的方式来创建(如 Map、 filter、join 等);</li><li>3.失败自动重建;</li><li>4.可以控制存储级别(内存、磁盘等)来进行重用;</li><li>5.必须是可序列化的;</li><li>6.是静态类型的(只读)。</li></ul><h3 id="RDD操作函数"><a href="#RDD操作函数" class="headerlink" title="RDD操作函数"></a>RDD操作函数</h3><p>RDD的操作函数主要分为2种类型行动算子(Transformation)和转换算子(Action).<br>可以对RDD进行函数操作,当你对一个RDD进行了操作,那么结果将会是一个新的RDD<br><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_9.png" alt="img_9.png"><br>Transformation操作不是马上提交Spark集群执行,Spark在遇到 Transformation操作时只会记录需要这样的操作,并不会去执行,需要等到有Action 操作的时候才会真正启动计算过程进行计算.<br>针对每个 Action,Spark 会生成一个Job, 从数据的创建开始,经过 Transformation, 结尾是 Action 操作.<br>这些操作对应形成一个有向无环图(DAG),形成 DAG 的先决条件是最后的函数操作是一个Action.</p><h3 id="DAG-stage-划分依据"><a href="#DAG-stage-划分依据" class="headerlink" title="DAG stage 划分依据"></a>DAG stage 划分依据</h3><p><img src="/luping/2021/09/14/spark%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/img_10.png" alt="img_10.png"><br>spark dagscheduler将任务划分stage,shuffle是划分DAG中stage 的标识,同时影响Spark执行速度的关键步骤.<br>RDD 的 Transformation 函数中,又分为窄依赖(narrow dependency)和宽依赖(wide dependency)的操作.<br>窄依赖跟宽依赖的区别是是否发生 shuffle(洗牌) 操作.宽依赖会发生 shuffle 操作.<br>窄依赖是子 RDD的各个分片(partition)不依赖于其他分片,能够独立计算得到结果,<br>宽依赖指子 RDD 的各个分片会依赖于父RDD 的多个分片,所以会造成父 RDD 的各个分片在集群中重新分片</p><h3 id="shuffle优化"><a href="#shuffle优化" class="headerlink" title="shuffle优化"></a>shuffle优化</h3><p>shuffle涉及网络传输和磁盘io,非常消耗资源 因此需要对shuffle优化<br><strong>一是如果可以避免shuffle则不选择涉及shuffle的算子</strong><br>rdd.groupByKey().mapValues(_ .sum) 与 rdd.reduceByKey(_ + _) 执行的结果是一样的，但是前者需要把全部的数据通过网络传递一遍，而后者只需要根据每个 key 局部的 partition 累积结果，在 shuffle 的之后把局部的累积值相加后得到结果.<br><strong>缓存机制 cache persist</strong><br>Spark中对于一个RDD执行多次算子(函数操作)的默认原理是这样的:每次你对一个RDD执行一个算子操作时，都会重新从源头处计算一遍，计算出那个RDD来，然后再对这个RDD执行你的算子操作。这种方式的性能是很差的。<br>对于这种情况,可对多次使用的RDD进行持久化。<br>cache 是使用的默认缓存选项,一般默认为Memoryonly(内存中缓存),<br>persist 则可以在缓存的时候选择任意一种缓存类型.事实上,cache内部调用的是默认的persist.persist可选择的方式很多缓存到磁盘或是内存磁盘组合缓存等</p><h1 id="spark常用算子"><a href="#spark常用算子" class="headerlink" title="spark常用算子"></a>spark常用算子</h1><h2 id="转换算子-Transformations"><a href="#转换算子-Transformations" class="headerlink" title="转换算子(Transformations)"></a>转换算子(Transformations)</h2><table><thead><tr><th>Transformations</th><th>Description</th></tr></thead><tbody><tr><td>map(func)</td><td>通过函数func传递源的每个元素，返回一个新的分布式数据集。</td></tr><tr><td>filter(func)</td><td>过滤数据，通过选择func返回true的源元素返回一个新的数据集。</td></tr><tr><td>flatMap(func)</td><td>与map类似，但是每个输入项都可以映射到0个或更多的输出项(因此func应该返回一个Seq而不是单个项)。展平 多个集合 汇总成一个集合</td></tr><tr><td>mapPartitions(func)</td><td>与map类似，但在RDD的每个分区(块)上分别运行，因此在类型为T的RDD上运行时，func必须是Iterator &#x3D;&gt; Iterator</td></tr><tr><td>mapPartitionsWithIndex(func)</td><td>与mapPartitions类似，但也为func提供了一个表示分区索引的整数值，因此func必须是类型(Int, Iterator) &#x3D;&gt; Iterator时，类型为T的RDD。</td></tr><tr><td>sample(withReplacement, fraction, seed)</td><td>使用给定的随机数生成器种子，对数据的一小部分进行抽样，无论是否进行替换。</td></tr><tr><td>union(otherDataset)</td><td>合并，返回一个新数据集，其中包含源数据集中的元素和参数的并集。</td></tr><tr><td>intersection(otherDataset)</td><td>交集，返回一个新的RDD，其中包含源数据集中的元素和参数的交集。</td></tr><tr><td>distinct([numPartitions]))</td><td>去重，返回包含源数据集的不同元素的新数据集。</td></tr><tr><td>groupByKey([numPartitions])</td><td>当对一个(K, V)对的数据集调用时，返回一个(K，可迭代)对的数据集。注意:如果您要对每个键进行分组以执行聚合(比如求和或平均)，那么使用reduceByKey或aggregateByKey将产生更好的性能。注意:默认情况下，输出中的并行级别取决于父RDD的分区数量。您可以传递一个可选的numPartitions参数来设置不同数量的任务。</td></tr><tr><td>reduceByKey(func, [numPartitions])</td><td>在（K，V）对的数据集上调用时，返回一个（K，V）对的数据集，其中每个键的值使用给定的reduce函数func进行聚合，该函数的类型必须是（V，V）&#x3D;&gt;V。与groupByKey一样，reduce任务的数量可以通过可选的第二个参数进行配置。</td></tr><tr><td>aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions])</td><td>当对一个(K, V)对的数据集调用时，返回一个(K, U)对的数据集，其中每个键的值使用给定的combine函数和一个中立的“零”值进行聚合。允许不同于输入值类型的聚合值类型，同时避免不必要的分配。与groupByKey类似，reduce任务的数量可以通过第二个可选参数进行配置。</td></tr><tr><td>sortByKey([ascending], [numPartitions])</td><td>当对一个(K, V)对的数据集(K, V)调用时，K实现有序，返回一个(K, V)对的数据集，按键序升序或降序排序，如布尔升序参数中指定的那样。</td></tr><tr><td>join(otherDataset, [numPartitions])</td><td>当对类型(K, V)和(K, W)的数据集调用时，返回一个(K， (V, W))对的数据集，其中包含每个键的所有元素对。通过leftOuterJoin、right touterjoin和fullOuterJoin来支持外部连接。</td></tr><tr><td>cogroup(otherDataset, [numPartitions])</td><td>当对类型(K, V)和(K, W)的数据集调用时，返回一个元组(K， (Iterable， Iterable))的数据集。这个操作也称为groupWith。</td></tr><tr><td>cartesian(otherDataset)</td><td>当对T和U类型的数据集调用时，返回一个(T, U)对的数据集(所有元素对)。</td></tr><tr><td>pipe(command, [envVars])</td><td>通过shell命令(例如Perl或bash脚本)管道传输RDD的每个分区。RDD元素被写入到进程的stdin中，并以字符串的RDD形式返回到它的stdout中的行输出。</td></tr><tr><td>coalesce(numPartitions)</td><td>将RDD中的分区数减少到numPartitions。用于筛选大型数据集后更有效地运行操作。</td></tr><tr><td>repartition(numPartitions)</td><td>随机重组RDD中的数据，创建更多或更少的分区，并在这些分区之间进行平衡。这总是在网络上对所有数据进行无序处理。</td></tr><tr><td>repartitionAndSortWithinPartitions(partitioner)</td><td>根据给定的分区器重新分区RDD，并在每个结果分区中按关键字对记录进行排序。这比在每个分区内调用重新分区然后进行排序更有效，因为它可以将排序向下推到无序处理机制中。</td></tr></tbody></table><h2 id="行动算子-Actions"><a href="#行动算子-Actions" class="headerlink" title="行动算子(Actions)"></a>行动算子(Actions)</h2><p>行动算子从功能上来说作为一个触发器，会触发提交整个作业并开始执行。从代码上来说，它与转换算子的最大不同之处在于：转换算子返回的还是 RDD，行动算子返回的是非 RDD 类型的值，如整数，或者根本没有返回值。</p><table><thead><tr><th>Actions</th><th>Description</th></tr></thead><tbody><tr><td>reduce(func)</td><td>使用函数func（接受两个参数并返回一个）聚合数据集的元素。函数应该是可交换的和相联的，从而可以并行计算</td></tr><tr><td>collect()</td><td>在驱动程序中将数据集的所有元素作为数组返回。这通常在过滤器或其他返回足够小的数据子集的操作之后有用。</td></tr><tr><td>count()</td><td>返回数据集中元素的数量。</td></tr><tr><td>first()</td><td>返回数据集的第一个元素(类似于take(1))。</td></tr><tr><td>take(n)</td><td>返回一个包含数据集前n个元素的数组。</td></tr><tr><td>takeSample(withReplacement, num, [seed])</td><td>返回数据集num元素的随机样本数组，可选地预先指定随机数生成器种子，是否进行替换。</td></tr><tr><td>takeOrdered(n, [ordering])</td><td>使用自然顺序或自定义比较器返回RDD的前n个元素。</td></tr><tr><td>saveAsTextFile(path)</td><td>将数据集的元素作为文本文件(或一组文本文件)写入本地文件系统、HDFS或任何其他hadoop支持的文件系统的给定目录中。Spark将对每个元素调用toString，将其转换为文件中的一行文本。</td></tr><tr><td>saveAsSequenceFile(path)(Java and Scala)</td><td>在本地文件系统、HDFS或任何其他Hadoop支持的文件系统的给定路径中，将数据集的元素作为Hadoop序列文件编写。这在实现Hadoop可写接口的键值对RDDs上可用。在Scala中，它还可以用于隐式转换为可写的类型(Spark包括基本类型的转换，如Int、Double、String等)。</td></tr><tr><td>saveAsObjectFile(path)(Java and Scala)</td><td>使用Java序列化以简单的格式编写数据集的元素，然后可以使用SparkContext.objectFile()加载这些元素。</td></tr><tr><td>countByKey()</td><td>只在类型(K, V)的RDDs上可用。返回一个(K, Int)对的hashmap，并记录每个键的计数。</td></tr><tr><td>foreach(func)</td><td>对数据集的每个元素运行函数func。这通常是为了避免副作用，如更新累加器或与外部存储系统交互。注意：在foreach（）之外修改除累加器以外的变量可能会导致未定义的行为。有关更多详细信息，请参见理解闭包。</td></tr></tbody></table><h2 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h2><p>缓存是迭代算法和快速交互式使用的关键工具。‎第一次在操作中计算它时，它将保存在节点上的内存中。Spark的缓存是容错的 - 如果RDD的任何分区丢失，它将使用最初创建它的转换自动重新计算。‎persist() cache()</p><table><thead><tr><th align="left">Storage Level</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left">MEMORY_ONLY</td><td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they’re needed. This is the default level.</td></tr><tr><td align="left">MEMORY_AND_DISK</td><td align="left">Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don’t fit on disk, and read them from there when they’re needed.</td></tr><tr><td align="left">MEMORY_ONLY_SER (Java and Scala)</td><td align="left">Store RDD as <em>serialized</em> Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a <a href="http://spark.incubator.apache.org/docs/2.4.5/tuning.html">fast serializer</a>, but more CPU-intensive to read.</td></tr><tr><td align="left">MEMORY_AND_DISK_SER (Java and Scala)</td><td align="left">Similar to MEMORY_ONLY_SER, but spill partitions that don’t fit in memory to disk instead of recomputing them on the fly each time they’re needed.</td></tr><tr><td align="left">DISK_ONLY</td><td align="left">Store the RDD partitions only on disk.</td></tr><tr><td align="left">MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td><td align="left">Same as the levels above, but replicate each partition on two cluster nodes.</td></tr><tr><td align="left">OFF_HEAP (experimental)</td><td align="left">Similar to MEMORY_ONLY_SER, but store the data in <a href="http://spark.incubator.apache.org/docs/2.4.5/configuration.html#memory-management">off-heap memory</a>. This requires off-heap memory to be enabled.</td></tr></tbody></table><p>unpersist()</p><h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val broadcastVar = sc.broadcast(Array(1, 2, 3))</span><br><span class="line">broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0)</span><br><span class="line"></span><br><span class="line">scala&gt; broadcastVar.value</span><br><span class="line">res0: Array[Int] = Array(1, 2, 3)</span><br></pre></td></tr></table></figure><h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val accum = sc.longAccumulator(&quot;My Accumulator&quot;)</span><br><span class="line">accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum.add(x))</span><br><span class="line">10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s</span><br><span class="line"></span><br><span class="line">scala&gt; accum.value</span><br><span class="line">res2: Long = 10</span><br></pre></td></tr></table></figure><h1 id="spark-SQL"><a href="#spark-SQL" class="headerlink" title="spark SQL"></a>spark SQL</h1><p>SparkSession中所有功能的入口点是SparkSession类.使用:SparkSessionSparkSession.builder()创建SparkSession对象</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">  .builder()</span><br><span class="line">  .appName(<span class="string">&quot;Spark SQL example&quot;</span>)</span><br><span class="line">  .config(<span class="string">&quot;spark.some.config.option&quot;</span>, <span class="string">&quot;some-value&quot;</span>)</span><br><span class="line">  .getOrCreate()</span><br></pre></td></tr></table></figure><p>创建dataframes</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">&quot;examples/src/main/resources/people.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// | age|   name|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br><span class="line"><span class="comment">// |null|Michael|</span></span><br><span class="line"><span class="comment">// |  30|   Andy|</span></span><br><span class="line"><span class="comment">// |  19| Justin|</span></span><br><span class="line"><span class="comment">// +----+-------+</span></span><br></pre></td></tr></table></figure><h2 id="UDF和UDAF"><a href="#UDF和UDAF" class="headerlink" title="UDF和UDAF"></a>UDF和UDAF</h2><p>UDF用户自定义非聚合函数</p><p>UDAF用户自定义聚合操作函数</p><h2 id="DataSources"><a href="#DataSources" class="headerlink" title="DataSources"></a>DataSources</h2><h3 id="load-x2F-save函数"><a href="#load-x2F-save函数" class="headerlink" title="load &#x2F;save函数"></a>load &#x2F;save函数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val usersDF = spark.read.load(&quot;examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">usersDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;namesAndFavColors.parquet&quot;)</span><br></pre></td></tr></table></figure><p>format</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">val peopleDFCsv = spark.read.format(&quot;csv&quot;)</span><br><span class="line">  .option(&quot;sep&quot;, &quot;;&quot;)</span><br><span class="line">  .option(&quot;inferSchema&quot;, &quot;true&quot;)</span><br><span class="line">  .option(&quot;header&quot;, &quot;true&quot;)</span><br><span class="line">  .load(&quot;examples/src/main/resources/people.csv&quot;)</span><br><span class="line">  </span><br><span class="line">  usersDF.write.format(&quot;orc&quot;)</span><br><span class="line">  .option(&quot;orc.bloom.filter.columns&quot;, &quot;favorite_color&quot;)</span><br><span class="line">  .option(&quot;orc.dictionary.key.threshold&quot;, &quot;1.0&quot;)</span><br><span class="line">  .save(&quot;users_with_options.orc&quot;)</span><br></pre></td></tr></table></figure><h3 id="Save-Modes"><a href="#Save-Modes" class="headerlink" title="Save Modes"></a>Save Modes</h3><table><thead><tr><th align="left">Scala&#x2F;Java</th><th align="left">Any Language</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>SaveMode.ErrorIfExists</code> (default)</td><td align="left">erroror errorifexists(default)</td><td align="left">When saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.</td></tr><tr><td align="left"><code>SaveMode.Append</code></td><td align="left">append</td><td align="left">When saving a DataFrame to a data source, if data&#x2F;table already exists, contents of the DataFrame are expected to be appended to existing data.</td></tr><tr><td align="left">SaveMode.Overwrite</td><td align="left">overwrite</td><td align="left">Overwrite mode means that when saving a DataFrame to a data source, if data&#x2F;table already exists, existing data is expected to be overwritten by the contents of the DataFrame.</td></tr><tr><td align="left">SaveMode.Ignore</td><td align="left">ignore</td><td align="left">Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected not to save the contents of the DataFrame and not to change the existing data. This is similar to a <code>CREATE TABLE IF NOT EXISTS</code> in SQL.</td></tr></tbody></table><h3 id="JSON-file"><a href="#JSON-file" class="headerlink" title="JSON file"></a>JSON file</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">// Primitive types (Int, String, etc) and Product types (case classes) encoders are</span><br><span class="line">// supported by importing this when creating a Dataset.</span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">// A JSON dataset is pointed to by path.</span><br><span class="line">// The path can be either a single text file or a directory storing text files</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val peopleDF = spark.read.json(path)</span><br><span class="line"></span><br><span class="line">// The inferred schema can be visualized using the printSchema() method</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: long (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// Creates a temporary view using the DataFrame</span><br><span class="line">peopleDF.createOrReplaceTempView(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL statements can be run by using the sql methods provided by spark</span><br><span class="line">val teenagerNamesDF = spark.sql(&quot;SELECT name FROM people WHERE age BETWEEN 13 AND 19&quot;)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line">// +------+</span><br><span class="line">// |  name|</span><br><span class="line">// +------+</span><br><span class="line">// |Justin|</span><br><span class="line">// +------+</span><br><span class="line"></span><br><span class="line">// Alternatively, a DataFrame can be created for a JSON dataset represented by</span><br><span class="line">// a Dataset[String] storing one JSON object per string</span><br><span class="line">val otherPeopleDataset = spark.createDataset(</span><br><span class="line">  &quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val otherPeople = spark.read.json(otherPeopleDataset)</span><br><span class="line">otherPeople.show()</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |        address|name|</span><br><span class="line">// +---------------+----+</span><br><span class="line">// |[Columbus,Ohio]| Yin|</span><br><span class="line">// +---------------+----+</span><br></pre></td></tr></table></figure><h3 id="hive-表"><a href="#hive-表" class="headerlink" title="hive 表"></a>hive 表</h3><p>通过将 中的 （对于安全配置）和（对于 HDFS 配置）文件，可以完成 Hive 的配置。‎<code>hive-site.xml``core-site.xml``hdfs-site.xml``conf/</code></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line">import java.io.File</span><br><span class="line"></span><br><span class="line">import org.apache.spark.sql.&#123;Row, SaveMode, SparkSession&#125;</span><br><span class="line"></span><br><span class="line">case class Record(key: Int, value: String)</span><br><span class="line"></span><br><span class="line">// warehouseLocation points to the default location for managed databases and tables</span><br><span class="line">val warehouseLocation = new File(&quot;spark-warehouse&quot;).getAbsolutePath</span><br><span class="line"></span><br><span class="line">val spark = SparkSession</span><br><span class="line">  .builder()</span><br><span class="line">  .appName(&quot;Spark Hive Example&quot;)</span><br><span class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation)</span><br><span class="line">  .enableHiveSupport()</span><br><span class="line">  .getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line">import spark.sql</span><br><span class="line"></span><br><span class="line">sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive&quot;)</span><br><span class="line">sql(&quot;LOAD DATA LOCAL INPATH &#x27;examples/src/main/resources/kv1.txt&#x27; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// Queries are expressed in HiveQL</span><br><span class="line">sql(&quot;SELECT * FROM src&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Aggregation queries are also supported.</span><br><span class="line">sql(&quot;SELECT COUNT(*) FROM src&quot;).show()</span><br><span class="line">// +--------+</span><br><span class="line">// |count(1)|</span><br><span class="line">// +--------+</span><br><span class="line">// |    500 |</span><br><span class="line">// +--------+</span><br><span class="line"></span><br><span class="line">// The results of SQL queries are themselves DataFrames and support all normal functions.</span><br><span class="line">val sqlDF = sql(&quot;SELECT key, value FROM src WHERE key &lt; 10 ORDER BY key&quot;)</span><br><span class="line"></span><br><span class="line">// The items in DataFrames are of type Row, which allows you to access each column by ordinal.</span><br><span class="line">val stringsDS = sqlDF.map &#123;</span><br><span class="line">  case Row(key: Int, value: String) =&gt; s&quot;Key: $key, Value: $value&quot;</span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line">// +--------------------+</span><br><span class="line">// |               value|</span><br><span class="line">// +--------------------+</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// |Key: 0, Value: val_0|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// You can also use DataFrames to create temporary views within a SparkSession.</span><br><span class="line">val recordsDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i, s&quot;val_$i&quot;)))</span><br><span class="line">recordsDF.createOrReplaceTempView(&quot;records&quot;)</span><br><span class="line"></span><br><span class="line">// Queries can then join DataFrame data with data stored in Hive.</span><br><span class="line">sql(&quot;SELECT * FROM records r JOIN src s ON r.key = s.key&quot;).show()</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |key| value|key| value|</span><br><span class="line">// +---+------+---+------+</span><br><span class="line">// |  2| val_2|  2| val_2|</span><br><span class="line">// |  4| val_4|  4| val_4|</span><br><span class="line">// |  5| val_5|  5| val_5|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Create a Hive managed Parquet table, with HQL syntax instead of the Spark SQL native syntax</span><br><span class="line">// `USING hive`</span><br><span class="line">sql(&quot;CREATE TABLE hive_records(key int, value string) STORED AS PARQUET&quot;)</span><br><span class="line">// Save DataFrame to the Hive managed table</span><br><span class="line">val df = spark.table(&quot;src&quot;)</span><br><span class="line">df.write.mode(SaveMode.Overwrite).saveAsTable(&quot;hive_records&quot;)</span><br><span class="line">// After insertion, the Hive managed table has data now</span><br><span class="line">sql(&quot;SELECT * FROM hive_records&quot;).show()</span><br><span class="line">// +---+-------+</span><br><span class="line">// |key|  value|</span><br><span class="line">// +---+-------+</span><br><span class="line">// |238|val_238|</span><br><span class="line">// | 86| val_86|</span><br><span class="line">// |311|val_311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">// Prepare a Parquet data directory</span><br><span class="line">val dataDir = &quot;/tmp/parquet_data&quot;</span><br><span class="line">spark.range(10).write.parquet(dataDir)</span><br><span class="line">// Create a Hive external Parquet table</span><br><span class="line">sql(s&quot;CREATE EXTERNAL TABLE hive_bigints(id bigint) STORED AS PARQUET LOCATION &#x27;$dataDir&#x27;&quot;)</span><br><span class="line">// The Hive external table should already have data</span><br><span class="line">sql(&quot;SELECT * FROM hive_bigints&quot;).show()</span><br><span class="line">// +---+</span><br><span class="line">// | id|</span><br><span class="line">// +---+</span><br><span class="line">// |  0|</span><br><span class="line">// |  1|</span><br><span class="line">// |  2|</span><br><span class="line">// ... Order may vary, as spark processes the partitions in parallel.</span><br><span class="line"></span><br><span class="line">// Turn on flag for Hive Dynamic Partitioning</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition&quot;, &quot;true&quot;)</span><br><span class="line">spark.sqlContext.setConf(&quot;hive.exec.dynamic.partition.mode&quot;, &quot;nonstrict&quot;)</span><br><span class="line">// Create a Hive partitioned table using DataFrame API</span><br><span class="line">df.write.partitionBy(&quot;key&quot;).format(&quot;hive&quot;).saveAsTable(&quot;hive_part_tbl&quot;)</span><br><span class="line">// Partitioned column `key` will be moved to the end of the schema.</span><br><span class="line">sql(&quot;SELECT * FROM hive_part_tbl&quot;).show()</span><br><span class="line">// +-------+---+</span><br><span class="line">// |  value|key|</span><br><span class="line">// +-------+---+</span><br><span class="line">// |val_238|238|</span><br><span class="line">// | val_86| 86|</span><br><span class="line">// |val_311|311|</span><br><span class="line">// ...</span><br><span class="line"></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><h3 id="JDBC-TO-OTHER-Databases"><a href="#JDBC-TO-OTHER-Databases" class="headerlink" title="JDBC TO OTHER Databases"></a>JDBC TO OTHER Databases</h3><table><thead><tr><th align="left">Property Name</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>url</code></td><td align="left">The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., <code>jdbc:postgresql://localhost/test?user=fred&amp;password=secret</code></td></tr><tr><td align="left"><code>dbtable</code></td><td align="left">The JDBC table that should be read from or written into. Note that when using it in the read path anything that is valid in a <code>FROM</code> clause of a SQL query can be used. For example, instead of a full table you could also use a subquery in parentheses. It is not allowed to specify <code>dbtable</code> and <code>query</code> options at the same time.</td></tr><tr><td align="left"><code>query</code></td><td align="left">A query that will be used to read data into Spark. The specified query will be parenthesized and used as a subquery in the <code>FROM</code> clause. Spark will also assign an alias to the subquery clause. As an example, spark will issue a query of the following form to the JDBC Source.  <code>SELECT &lt;columns&gt; FROM (&lt;user_specified_query&gt;) spark_gen_alias</code>  Below are couple of restrictions while using this option. It is not allowed to specify <code>dbtable</code> and <code>query</code> options at the same time.It is not allowed to specify <code>query</code> and <code>partitionColumn</code> options at the same time. When specifying <code>partitionColumn</code> option is required, the subquery can be specified using <code>dbtable</code> option instead and partition columns can be qualified using the subquery alias provided as part of <code>dbtable</code>. Example: <code>spark.read.format(&quot;jdbc&quot;).option(&quot;url&quot;, jdbcUrl).option(&quot;query&quot;, &quot;select c1, c2 from t1&quot;).load()</code></td></tr><tr><td align="left"><code>driver</code></td><td align="left">The class name of the JDBC driver to use to connect to this URL.</td></tr><tr><td align="left"><code>partitionColumn, lowerBound, upperBound</code></td><td align="left">These options must all be specified if any of them is specified. In addition, <code>numPartitions</code> must be specified. They describe how to partition the table when reading in parallel from multiple workers. <code>partitionColumn</code> must be a numeric, date, or timestamp column from the table in question. Notice that <code>lowerBound</code> and <code>upperBound</code> are just used to decide the partition stride, not for filtering the rows in table. So all rows in the table will be partitioned and returned. This option applies only to reading.</td></tr><tr><td align="left"><code>numPartitions</code></td><td align="left">The maximum number of partitions that can be used for parallelism in table reading and writing. This also determines the maximum number of concurrent JDBC connections. If the number of partitions to write exceeds this limit, we decrease it to this limit by calling <code>coalesce(numPartitions)</code> before writing.</td></tr><tr><td align="left"><code>queryTimeout</code></td><td align="left">The number of seconds the driver will wait for a Statement object to execute to the given number of seconds. Zero means there is no limit. In the write path, this option depends on how JDBC drivers implement the API <code>setQueryTimeout</code>, e.g., the h2 JDBC driver checks the timeout of each query instead of an entire JDBC batch. It defaults to <code>0</code>.</td></tr><tr><td align="left"><code>fetchsize</code></td><td align="left">The JDBC fetch size, which determines how many rows to fetch per round trip. This can help performance on JDBC drivers which default to low fetch size (eg. Oracle with 10 rows). This option applies only to reading.</td></tr><tr><td align="left"><code>batchsize</code></td><td align="left">The JDBC batch size, which determines how many rows to insert per round trip. This can help performance on JDBC drivers. This option applies only to writing. It defaults to <code>1000</code>.</td></tr><tr><td align="left"><code>isolationLevel</code></td><td align="left">The transaction isolation level, which applies to current connection. It can be one of <code>NONE</code>, <code>READ_COMMITTED</code>, <code>READ_UNCOMMITTED</code>, <code>REPEATABLE_READ</code>, or <code>SERIALIZABLE</code>, corresponding to standard transaction isolation levels defined by JDBC’s Connection object, with default of <code>READ_UNCOMMITTED</code>. This option applies only to writing. Please refer the documentation in <code>java.sql.Connection</code>.</td></tr><tr><td align="left"><code>sessionInitStatement</code></td><td align="left">After each database session is opened to the remote DB and before starting to read data, this option executes a custom SQL statement (or a PL&#x2F;SQL block). Use this to implement session initialization code. Example: <code>option(&quot;sessionInitStatement&quot;, &quot;&quot;&quot;BEGIN execute immediate &#39;alter session set &quot;_serial_direct_read&quot;=true&#39;; END;&quot;&quot;&quot;)</code></td></tr><tr><td align="left"><code>truncate</code></td><td align="left">This is a JDBC writer related option. When <code>SaveMode.Overwrite</code> is enabled, this option causes Spark to truncate an existing table instead of dropping and recreating it. This can be more efficient, and prevents the table metadata (e.g., indices) from being removed. However, it will not work in some cases, such as when the new data has a different schema. It defaults to <code>false</code>. This option applies only to writing.</td></tr><tr><td align="left"><code>cascadeTruncate</code></td><td align="left">This is a JDBC writer related option. If enabled and supported by the JDBC database (PostgreSQL and Oracle at the moment), this options allows execution of a <code>TRUNCATE TABLE t CASCADE</code> (in the case of PostgreSQL a <code>TRUNCATE TABLE ONLY t CASCADE</code> is executed to prevent inadvertently truncating descendant tables). This will affect other tables, and thus should be used with care. This option applies only to writing. It defaults to the default cascading truncate behaviour of the JDBC database in question, specified in the <code>isCascadeTruncate</code> in each JDBCDialect.</td></tr><tr><td align="left"><code>createTableOptions</code></td><td align="left">This is a JDBC writer related option. If specified, this option allows setting of database-specific table and partition options when creating a table (e.g., <code>CREATE TABLE t (name string) ENGINE=InnoDB.</code>). This option applies only to writing.</td></tr><tr><td align="left"><code>createTableColumnTypes</code></td><td align="left">The database column data types to use instead of the defaults, when creating the table. Data type information should be specified in the same format as CREATE TABLE columns syntax (e.g: <code>&quot;name CHAR(64), comments VARCHAR(1024)&quot;)</code>. The specified types should be valid spark sql data types. This option applies only to writing.</td></tr><tr><td align="left"><code>customSchema</code></td><td align="left">The custom schema to use for reading data from JDBC connectors. For example, <code>&quot;id DECIMAL(38, 0), name STRING&quot;</code>. You can also specify partial fields, and the others use the default type mapping. For example, <code>&quot;id DECIMAL(38, 0)&quot;</code>. The column names should be identical to the corresponding column names of JDBC table. Users can specify the corresponding data types of Spark SQL instead of using the defaults. This option applies only to reading.</td></tr><tr><td align="left"><code>pushDownPredicate</code></td><td align="left">The option to enable or disable predicate push-down into the JDBC data source. The default value is true, in which case Spark will push down filters to the JDBC data source as much as possible. Otherwise, if set to false, no filter will be pushed down to the JDBC data source and thus all filters will be handled by Spark. Predicate push-down is usually turned off when the predicate filtering is performed faster by Spark than by the JDBC data source.</td></tr></tbody></table><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">// Note: JDBC loading and saving can be achieved via either the load/save or jdbc methods</span><br><span class="line">// Loading data from a JDBC source</span><br><span class="line">val jdbcDF = spark.read</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">val connectionProperties = new Properties()</span><br><span class="line">connectionProperties.put(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">connectionProperties.put(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">val jdbcDF2 = spark.read</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line">// Specifying the custom data types of the read schema</span><br><span class="line">connectionProperties.put(&quot;customSchema&quot;, &quot;id DECIMAL(38, 0), name STRING&quot;)</span><br><span class="line">val jdbcDF3 = spark.read</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Saving data to a JDBC source</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .format(&quot;jdbc&quot;)</span><br><span class="line">  .option(&quot;url&quot;, &quot;jdbc:postgresql:dbserver&quot;)</span><br><span class="line">  .option(&quot;dbtable&quot;, &quot;schema.tablename&quot;)</span><br><span class="line">  .option(&quot;user&quot;, &quot;username&quot;)</span><br><span class="line">  .option(&quot;password&quot;, &quot;password&quot;)</span><br><span class="line">  .save()</span><br><span class="line"></span><br><span class="line">jdbcDF2.write</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br><span class="line"></span><br><span class="line">// Specifying create table column data types on write</span><br><span class="line">jdbcDF.write</span><br><span class="line">  .option(&quot;createTableColumnTypes&quot;, &quot;name CHAR(64), comments VARCHAR(1024)&quot;)</span><br><span class="line">  .jdbc(&quot;jdbc:postgresql:dbserver&quot;, &quot;schema.tablename&quot;, connectionProperties)</span><br></pre></td></tr></table></figure><h2 id="性能调优"><a href="#性能调优" class="headerlink" title="性能调优"></a>性能调优</h2><h2 id="缓存内存中的数据‎"><a href="#缓存内存中的数据‎" class="headerlink" title="缓存内存中的数据‎"></a>缓存内存中的数据‎</h2><p>‎Spark SQL 可以通过调用 或 使用 内存中列式格式 来缓存表。然后，Spark SQL 将仅扫描所需的列，并将自动调整压缩，以最大程度地减少内存使用量和 GC 压力。您可以调用以从内存中删除该表。‎</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.catalog.cacheTable(&quot;tableName&quot;)</span><br><span class="line">dataFrame.cache()</span><br><span class="line">spark.catalog.uncacheTable(&quot;tableName&quot;)</span><br></pre></td></tr></table></figure><h2 id="‎其他配置选项‎"><a href="#‎其他配置选项‎" class="headerlink" title="‎其他配置选项‎"></a>‎其他配置选项‎</h2><p>‎以下选项还可用于优化查询执行的性能。这些选项可能会在将来的版本中弃用，因为会自动执行更多优化。‎</p><table><thead><tr><th align="left">Property Name</th><th align="left">Default</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><code>spark.sql.files.maxPartitionBytes</code></td><td align="left">134217728 (128 MB)</td><td align="left">The maximum number of bytes to pack into a single partition when reading files.</td></tr><tr><td align="left"><code>spark.sql.files.openCostInBytes</code></td><td align="left">4194304 (4 MB)</td><td align="left">The estimated cost to open a file, measured by the number of bytes could be scanned in the same time. This is used when putting multiple files into a partition. It is better to over-estimated, then the partitions with small files will be faster than partitions with bigger files (which is scheduled first).</td></tr><tr><td align="left"><code>spark.sql.broadcastTimeout</code></td><td align="left">300</td><td align="left">Timeout in seconds for the broadcast wait time in broadcast joins</td></tr><tr><td align="left"><code>spark.sql.autoBroadcastJoinThreshold</code></td><td align="left">10485760 (10 MB)</td><td align="left">Configures the maximum size in bytes for a table that will be broadcast to all worker nodes when performing a join. By setting this value to -1 broadcasting can be disabled. Note that currently statistics are only supported for Hive Metastore tables where the command <code>ANALYZE TABLE &lt;tableName&gt; COMPUTE STATISTICS noscan</code> has been run.</td></tr><tr><td align="left"><code>spark.sql.shuffle.partitions</code></td><td align="left">200</td><td align="left">Configures the number of partitions to use when shuffling data for joins or aggregations.</td></tr></tbody></table><h2 id="SQL-查询的使用广播变量"><a href="#SQL-查询的使用广播变量" class="headerlink" title="SQL 查询的使用广播变量"></a>SQL 查询的使用广播变量</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import org.apache.spark.sql.functions.broadcast</span><br><span class="line">broadcast(spark.table(&quot;src&quot;)).join(spark.table(&quot;records&quot;), &quot;key&quot;).show()</span><br></pre></td></tr></table></figure><h2 id="与-Apache-Hive-的兼容性"><a href="#与-Apache-Hive-的兼容性" class="headerlink" title="与 Apache Hive 的兼容性"></a>与 Apache Hive 的兼容性</h2><h3 id="支持的配置单元功能"><a href="#支持的配置单元功能" class="headerlink" title="支持的配置单元功能"></a>支持的配置单元功能</h3><p>Spark SQL 支持绝大多数 Hive 功能，例如：</p><ul><li>Hive 查询语句，包括：<ul><li><code>SELECT</code></li><li><code>GROUP BY</code></li><li><code>ORDER BY</code></li><li><code>CLUSTER BY</code></li><li><code>SORT BY</code></li></ul></li><li>所有 Hive 运算符，包括：<ul><li>关系运算符 （， ， ， ， ， ， ， ， 等）<code>=``⇔``==``&lt;&gt;``&lt;``&gt;``&gt;=``&lt;=</code></li><li>算术运算符（、、、、等）<code>+``-``*``/``%</code></li><li>逻辑运算符（、、、、等）<code>AND``&amp;&amp;``OR``||</code></li><li>复杂类型构造函数</li><li>数学函数（、、、等）<code>sign``ln``cos</code></li><li>字符串函数（、、、等）<code>instr``length``printf</code></li></ul></li><li>用户定义函数 （UDF）</li><li>用户定义的聚合函数 （UDAF）</li><li>用户定义的序列化格式 （SerDes）</li><li>窗口函数</li><li>加入<ul><li><code>JOIN</code></li><li><code>&#123;LEFT|RIGHT|FULL&#125; OUTER JOIN</code></li><li><code>LEFT SEMI JOIN</code></li><li><code>CROSS JOIN</code></li></ul></li><li>工会</li><li>子查询<ul><li><code>SELECT col FROM ( SELECT a + b AS col from t1) t2</code></li></ul></li><li>采样</li><li>解释</li><li>分区表，包括动态分区插入</li><li>视图</li><li>所有 Hive DDL 函数，包括：<ul><li><code>CREATE TABLE</code></li><li><code>CREATE TABLE AS SELECT</code></li><li><code>ALTER TABLE</code></li></ul></li><li>大多数 Hive 数据类型，包括：<ul><li><code>TINYINT</code></li><li><code>SMALLINT</code></li><li><code>INT</code></li><li><code>BIGINT</code></li><li><code>BOOLEAN</code></li><li><code>FLOAT</code></li><li><code>DOUBLE</code></li><li><code>STRING</code></li><li><code>BINARY</code></li><li><code>TIMESTAMP</code></li><li><code>DATE</code></li><li><code>ARRAY&lt;&gt;</code></li><li><code>MAP&lt;&gt;</code></li><li><code>STRUCT&lt;&gt;</code></li></ul></li></ul><h2 id="structure-treaming"><a href="#structure-treaming" class="headerlink" title="structure treaming"></a>structure treaming</h2><p><strong>example</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">// Create DataFrame representing the stream of input lines from connection to localhost:9999</span><br><span class="line">val lines = spark.readStream</span><br><span class="line">  .format(&quot;socket&quot;)</span><br><span class="line">  .option(&quot;host&quot;, &quot;localhost&quot;)</span><br><span class="line">  .option(&quot;port&quot;, 9999)</span><br><span class="line">  .load()</span><br><span class="line"></span><br><span class="line">// Split the lines into words</span><br><span class="line">val words = lines.as[String].flatMap(_.split(&quot; &quot;))</span><br><span class="line"></span><br><span class="line">// Generate running word count</span><br><span class="line">val wordCounts = words.groupBy(&quot;value&quot;).count()</span><br><span class="line"></span><br><span class="line">// Start running the query that prints the running counts to the console</span><br><span class="line">val query = wordCounts.writeStream</span><br><span class="line">  .outputMode(&quot;complete&quot;)</span><br><span class="line">  .format(&quot;console&quot;)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line">query.awaitTermination()</span><br></pre></td></tr></table></figure><p>数据源</p><table><thead><tr><th align="left"></th><th align="left"></th><th align="left"></th><th align="left"></th></tr></thead><tbody><tr><td align="left">Source</td><td align="left">Options</td><td align="left">Fault-tolerant</td><td align="left">Notes</td></tr><tr><td align="left"><strong>File source</strong></td><td align="left"><code>path</code>: path to the input directory, and common to all file formats. <code>maxFilesPerTrigger</code>: maximum number of new files to be considered in every trigger (default: no max) <code>latestFirst</code>: whether to process the latest new files first, useful when there is a large backlog of files (default: false) <code>fileNameOnly</code>: whether to check new files based on only the filename instead of on the full path (default: false). With this set to <code>true</code>, the following files would be considered as the same file, because their filenames, “dataset.txt”, are the same: “file:&#x2F;&#x2F;&#x2F;dataset.txt” “s3:&#x2F;&#x2F;a&#x2F;dataset.txt” “s3n:&#x2F;&#x2F;a&#x2F;b&#x2F;dataset.txt” “s3a:&#x2F;&#x2F;a&#x2F;b&#x2F;c&#x2F;dataset.txt” <code>maxFileAge</code>: Maximum age of a file that can be found in this directory, before it is ignored. For the first batch all files will be considered valid. If <code>latestFirst</code> is set to <code>true</code> and <code>maxFilesPerTrigger</code> is set, then this parameter will be ignored, because old files that are valid, and should be processed, may be ignored. The max age is specified with respect to the timestamp of the latest file, and not the timestamp of the current system.(default: 1 week) <code>cleanSource</code>: option to clean up completed files after processing. Available options are “archive”, “delete”, “off”. If the option is not provided, the default value is “off”. When “archive” is provided, additional option <code>sourceArchiveDir</code> must be provided as well. The value of “sourceArchiveDir” must not match with source pattern in depth (the number of directories from the root directory), where the depth is minimum of depth on both paths. This will ensure archived files are never included as new source files. For example, suppose you provide ‘&#x2F;hello?&#x2F;spark&#x2F;<em>‘ as source pattern, ‘&#x2F;hello1&#x2F;spark&#x2F;archive&#x2F;dir’ cannot be used as the value of “sourceArchiveDir”, as ‘&#x2F;hello?&#x2F;spark&#x2F;</em>‘ and ‘&#x2F;hello1&#x2F;spark&#x2F;archive’ will be matched. ‘&#x2F;hello1&#x2F;spark’ cannot be also used as the value of “sourceArchiveDir”, as ‘&#x2F;hello?&#x2F;spark’ and ‘&#x2F;hello1&#x2F;spark’ will be matched. ‘&#x2F;archived&#x2F;here’ would be OK as it doesn’t match. Spark will move source files respecting their own path. For example, if the path of source file is <code>/a/b/dataset.txt</code> and the path of archive directory is <code>/archived/here</code>, file will be moved to <code>/archived/here/a/b/dataset.txt</code>. NOTE: Both archiving (via moving) or deleting completed files will introduce overhead (slow down, even if it’s happening in separate thread) in each micro-batch, so you need to understand the cost for each operation in your file system before enabling this option. On the other hand, enabling this option will reduce the cost to list source files which can be an expensive operation. Number of threads used in completed file cleaner can be configured with<code>spark.sql.streaming.fileSource.cleaner.numThreads</code> (default: 1). NOTE 2: The source path should not be used from multiple sources or queries when enabling this option. Similarly, you must ensure the source path doesn’t match to any files in output directory of file stream sink. NOTE 3: Both delete and move actions are best effort. Failing to delete or move files will not fail the streaming query. Spark may not clean up some source files in some circumstances - e.g. the application doesn’t shut down gracefully, too many files are queued to clean up.  For file-format-specific options, see the related methods in <code>DataStreamReader</code> (<a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/sql/streaming/DataStreamReader.html">Scala</a>&#x2F;<a href="http://spark.incubator.apache.org/docs/3.1.2/api/java/org/apache/spark/sql/streaming/DataStreamReader.html">Java</a>&#x2F;<a href="http://spark.incubator.apache.org/docs/3.1.2/api/python/pyspark.sql.html#pyspark.sql.streaming.DataStreamReader">Python</a>&#x2F;<a href="http://spark.incubator.apache.org/docs/3.1.2/api/R/read.stream.html">R</a>). E.g. for “parquet” format options see <code>DataStreamReader.parquet()</code>.  In addition, there are session configurations that affect certain file-formats. See the <a href="http://spark.incubator.apache.org/docs/3.1.2/sql-programming-guide.html">SQL Programming Guide</a> for more details. E.g., for “parquet”, see <a href="http://spark.incubator.apache.org/docs/3.1.2/sql-data-sources-parquet.html#configuration">Parquet configuration</a> section.</td><td align="left">Yes</td><td align="left">Supports glob paths, but does not support multiple comma-separated paths&#x2F;globs.</td></tr><tr><td align="left"><strong>Socket Source</strong></td><td align="left"><code>host</code>: host to connect to, must be specified <code>port</code>: port to connect to, must be specified</td><td align="left">No</td><td align="left"></td></tr><tr><td align="left"><strong>Rate Source</strong></td><td align="left"><code>rowsPerSecond</code> (e.g. 100, default: 1): How many rows should be generated per second.  <code>rampUpTime</code> (e.g. 5s, default: 0s): How long to ramp up before the generating speed becomes <code>rowsPerSecond</code>. Using finer granularities than seconds will be truncated to integer seconds.  <code>numPartitions</code> (e.g. 10, default: Spark’s default parallelism): The partition number for the generated rows.  The source will try its best to reach <code>rowsPerSecond</code>, but the query may be resource constrained, and <code>numPartitions</code> can be tweaked to help reach the desired speed.</td><td align="left">Yes</td><td align="left"></td></tr><tr><td align="left"><strong>Kafka Source</strong></td><td align="left">See the <a href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a>.</td><td align="left">Yes</td><td align="left"></td></tr></tbody></table><h3 id="Output-Sinks"><a href="#Output-Sinks" class="headerlink" title="Output Sinks"></a>Output Sinks</h3><p>There are a few types of built-in output sinks.</p><ul><li><strong>File sink</strong> - Stores the output to a directory.</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;parquet&quot;</span>)        <span class="comment">// can be &quot;orc&quot;, &quot;json&quot;, &quot;csv&quot;, etc.</span></span><br><span class="line">    .option(<span class="string">&quot;path&quot;</span>, <span class="string">&quot;path/to/destination/dir&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><ul><li><strong>Kafka sink</strong> - Stores the output to one or more topics in Kafka.</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;kafka&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;kafka.bootstrap.servers&quot;</span>, <span class="string">&quot;host1:port1,host2:port2&quot;</span>)</span><br><span class="line">    .option(<span class="string">&quot;topic&quot;</span>, <span class="string">&quot;updates&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><ul><li><strong>Foreach sink</strong> - Runs arbitrary computation on the records in the output. See later in the section for more details.</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><ul><li><strong>Console sink (for debugging)</strong> - Prints the output to the console&#x2F;stdout every time there is a trigger. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory after every trigger.</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;console&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><ul><li><strong>Memory sink (for debugging)</strong> - The output is stored in memory as an in-memory table. Both, Append and Complete output modes, are supported. This should be used for debugging purposes on low data volumes as the entire output is collected and stored in the driver’s memory. Hence, use it with caution.</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">&quot;memory&quot;</span>)</span><br><span class="line">    .queryName(<span class="string">&quot;tableName&quot;</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><p>Some sinks are not fault-tolerant because they do not guarantee persistence of the output and are meant for debugging purposes only. See the earlier section on <a href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#fault-tolerance-semantics">fault-tolerance semantics</a>. Here are the details of all the sinks in Spark.</p><table><thead><tr><th align="left">Sink</th><th align="left">Supported Output Modes</th><th align="left">Options</th><th align="left">Fault-tolerant</th><th align="left">Notes</th></tr></thead><tbody><tr><td align="left"><strong>File Sink</strong></td><td align="left">Append</td><td align="left"><code>path</code>: path to the output directory, must be specified. <code>retention</code>: time to live (TTL) for output files. Output files which batches were committed older than TTL will be eventually excluded in metadata log. This means reader queries which read the sink’s output directory may not process them. You can provide the value as string format of the time. (like “12h”, “7d”, etc.) By default it’s disabled.  For file-format-specific options, see the related methods in DataFrameWriter (<a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/sql/DataFrameWriter.html">Scala</a>&#x2F;<a href="http://spark.incubator.apache.org/docs/3.1.2/api/java/org/apache/spark/sql/DataFrameWriter.html">Java</a>&#x2F;<a href="http://spark.incubator.apache.org/docs/3.1.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameWriter">Python</a>&#x2F;<a href="http://spark.incubator.apache.org/docs/3.1.2/api/R/write.stream.html">R</a>). E.g. for “parquet” format options see <code>DataFrameWriter.parquet()</code></td><td align="left">Yes (exactly-once)</td><td align="left">Supports writes to partitioned tables. Partitioning by time may be useful.</td></tr><tr><td align="left"><strong>Kafka Sink</strong></td><td align="left">Append, Update, Complete</td><td align="left">See the <a href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a></td><td align="left">Yes (at-least-once)</td><td align="left">More details in the <a href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-kafka-integration.html">Kafka Integration Guide</a></td></tr><tr><td align="left"><strong>Foreach Sink</strong></td><td align="left">Append, Update, Complete</td><td align="left">None</td><td align="left">Yes (at-least-once)</td><td align="left">More details in the <a href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch">next section</a></td></tr><tr><td align="left"><strong>ForeachBatch Sink</strong></td><td align="left">Append, Update, Complete</td><td align="left">None</td><td align="left">Depends on the implementation</td><td align="left">More details in the <a href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#using-foreach-and-foreachbatch">next section</a></td></tr><tr><td align="left"><strong>Console Sink</strong></td><td align="left">Append, Update, Complete</td><td align="left"><code>numRows</code>: Number of rows to print every trigger (default: 20) <code>truncate</code>: Whether to truncate the output if too long (default: true)</td><td align="left">No</td><td align="left"></td></tr><tr><td align="left"><strong>Memory Sink</strong></td><td align="left">Append, Complete</td><td align="left">None</td><td align="left">No. But in Complete Mode, restarted query will recreate the full table.</td><td align="left">Table name is the query name.</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td><td align="left"></td><td align="left"></td></tr></tbody></table><h3 id="Triggers-触发器"><a href="#Triggers-触发器" class="headerlink" title="Triggers 触发器"></a>Triggers 触发器</h3><table><thead><tr><th><strong>Fixed interval micro-batches</strong></th><th>‎查询将以微批处理模式执行，其中微批处理将按用户指定的时间间隔启动。‎‎如果前一个微批处理在间隔内完成，则引擎将等到间隔结束，然后再启动下一个微批次。‎‎如果前一个微批处理花费的时间超过完成间隔的时间（即，如果错过了间隔边界），则下一个微批处理将在前一个微批处理完成后立即启动（即，它不会等待下一个间隔边界）。‎‎如果没有新数据可用，则不会启动任何微批处理。‎</th></tr></thead><tbody><tr><td><strong>One-time micro-batch</strong></td><td>‎查询将仅执行‎<strong>‎一个‎</strong>‎微批处理来处理所有可用数据，然后自行停止。这在您希望定期启动群集、处理自上一个周期以来可用的所有内容，然后关闭群集的情况下非常有用。在某些情况下，这可能会节省大量成本。‎</td></tr><tr><td><strong>Continuous with fixed checkpoint interval</strong> <em>(experimental)</em></td><td>‎查询将在新的低延迟连续处理模式下执行。在下面的‎<a href="http://spark.incubator.apache.org/docs/3.1.2/structured-streaming-programming-guide.html#continuous-processing">‎”连续处理”部分中‎</a>‎阅读有关此内容的更多信息。‎</td></tr></tbody></table><h1 id="sparkstreaming"><a href="#sparkstreaming" class="headerlink" title="sparkstreaming"></a>sparkstreaming</h1><p><img src="http://spark.incubator.apache.org/docs/3.1.2/img/streaming-arch.png" alt="火花流"></p><p>在内部，它的工作方式如下。Spark 流接收实时输入数据流并将数据划分为批次，然后由 Spark 引擎处理这些批处理，以批量生成最终的结果流。‎</p><p><img src="http://spark.incubator.apache.org/docs/3.1.2/img/streaming-flow.png" alt="火花流"></p><h2 id="A-Quick-Example"><a href="#A-Quick-Example" class="headerlink" title="A Quick Example"></a>A Quick Example</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._ <span class="comment">// not necessary since Spark 1.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Create a local StreamingContext with two working thread and batch interval of 1 second.</span></span><br><span class="line"><span class="comment">// The master requires 2 cores to prevent a starvation scenario.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[2]&quot;</span>).setAppName(<span class="string">&quot;NetworkWordCount&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"><span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line"><span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="comment">// Split each line into words</span></span><br><span class="line"><span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.<span class="type">StreamingContext</span>._ <span class="comment">// not necessary since Spark 1.3</span></span><br><span class="line"><span class="comment">// Count each word in each batch</span></span><br><span class="line"><span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">wordCounts.print()</span><br><span class="line">ssc.start()             <span class="comment">// Start the computation</span></span><br><span class="line">ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># TERMINAL 1: # Running Netcat $ nc -lk 9999 hello world</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TERMINAL 2: RUNNING NetworkWordCount</span></span><br><span class="line"></span><br><span class="line">$ ./bin/run-example streaming.NetworkWordCount localhost 9999</span><br><span class="line">...</span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 1357008430000 ms</span><br><span class="line">-------------------------------------------</span><br><span class="line">(hello,1)</span><br><span class="line">(world,1)</span><br></pre></td></tr></table></figure><h2 id="基本概念‎"><a href="#基本概念‎" class="headerlink" title="基本概念‎"></a>基本概念‎</h2><p>引入sparkstreaming</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>要从 Spark Streaming 核心 API 中不存在的 Kafka 和 Kinesis 等源引入数据，必须将相应的项目添加到依赖项中。例如，一些常见的如下。‎<code>spark-streaming-xyz_2.12</code></p><table><thead><tr><th align="left">Source</th><th align="left">Artifact</th></tr></thead><tbody><tr><td align="left">Kafka</td><td align="left">spark-streaming-kafka-0-10_2.12</td></tr><tr><td align="left">Kinesis</td><td align="left">spark-streaming-kinesis-asl_2.12 [Amazon Software License]</td></tr></tbody></table><h3 id="初始化spark上下文"><a href="#初始化spark上下文" class="headerlink" title="初始化spark上下文"></a>初始化spark上下文</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark._</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(appName).setMaster(master)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>‎‎还可以从现有对象创建对象。‎<code>StreamingContext``SparkContext</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.streaming._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> sc = ...                <span class="comment">// existing SparkContext</span></span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>‎定义上下文后，必须执行以下操作。‎</p><ol><li>‎通过创建输入 DStream 来定义输入源。‎</li><li>‎通过将转换和输出操作应用于 DStream 来定义流式计算。‎</li><li>‎开始接收数据并使用 进行处理。‎<code>streamingContext.start()</code></li><li>‎使用 等待停止处理（手动或由于任何错误）。‎<code>streamingContext.awaitTermination()</code></li><li>‎可以使用 手动停止处理。‎<code>streamingContext.stop()</code></li></ol><h5 id="‎要记住的要点：‎"><a href="#‎要记住的要点：‎" class="headerlink" title="‎要记住的要点：‎"></a>‎要记住的要点：‎</h5><ul><li>‎启动上下文后，无法设置或向其添加新的流式计算。‎</li><li>‎一旦上下文停止，就无法重新启动。‎</li><li>‎一个 JVM 中只能同时激活一个流式流上下文。‎</li><li>‎StreamingContext 上的 stop（） 也会停止 SparkContext。若要仅停止流式处理上下文，请将 called 的可选参数设置为 false。‎<code>stop()``stopSparkContext</code></li><li>‎SparkContext 可以重新用于创建多个 StreamingContext，只要在创建下一个 StreamingContext 之前停止（不停止 SparkContext）即可。‎</li></ul><h2 id="DStream"><a href="#DStream" class="headerlink" title="DStream"></a>DStream</h2><p><strong>‎DStream‎</strong>‎ 是 Spark 流提供的基本抽象。它表示连续的数据流，可以是从源接收的输入数据流，也可以是通过转换输入流生成的已处理数据流。在内部，DStream由一系列连续的RDD表示，这是Spark对不可变的分布式数据集的抽象（有关详细信息，请参阅‎<a href="http://spark.incubator.apache.org/docs/3.1.2/rdd-programming-guide.html#resilient-distributed-datasets-rdds">‎Spark编程指南‎</a>‎）。DStream 中的每个 RDD 都包含特定时间间隔的数据</p><p><img src="http://spark.incubator.apache.org/docs/3.1.2/img/streaming-dstream-ops.png" alt="Spark Streaming"></p><h3 id="文件流"><a href="#文件流" class="headerlink" title="文件流"></a>文件流</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory)</span><br><span class="line">streamingContext.textFileStream(dataDirectory)</span><br></pre></td></tr></table></figure><h4 id="‎如何监视目录‎"><a href="#‎如何监视目录‎" class="headerlink" title="‎如何监视目录‎"></a>‎如何监视目录‎</h4><p>‎Spark 流式处理将监视该目录并处理在该目录中创建的任何文件。‎<code>dataDirectory</code></p><ul><li>‎可以监视一个简单的目录，例如 。直接位于此类路径下的所有文件都将在被发现时进行处理。‎<code>&quot;hdfs://namenode:8040/logs/&quot;</code></li><li>‎可以提供 ‎<a href="http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html#tag_02_13_02">‎POSIX glob 模式‎</a>‎，例如 。在这里，DStream 将包含目录中与模式匹配的所有文件。也就是说：它是目录的模式，而不是目录中的文件的模式。‎<code>&quot;hdfs://namenode:8040/logs/2017/*&quot;</code></li><li>‎所有文件必须采用相同的数据格式。‎</li><li>‎文件根据其修改时间（而不是其创建时间）被视为时间段的一部分。‎</li><li>‎处理后，在当前窗口中对文件所做的更改将不会导致重新读取该文件。也就是说：‎<em>‎更新将被忽略‎</em>‎。‎</li><li>‎目录下的文件越多，扫描更改所需的时间就越长，即使没有修改任何文件也是如此。‎</li><li>‎如果使用通配符来标识目录（例如 ），则重命名整个目录以匹配路径会将该目录添加到受监视目录列表中。只有目录中修改时间在当前窗口内的文件才会包含在流中。‎<code>&quot;hdfs://namenode:8040/logs/2016-*&quot;</code></li><li>‎调用 ‎<a href="https://hadoop.apache.org/docs/current/api/org/apache/hadoop/fs/FileSystem.html#setTimes-org.apache.hadoop.fs.Path-long-long-"><code>‎FileSystem.setTimes（）‎</code></a>‎ 来修复时间戳是一种在以后的窗口中选取文件的方法，即使其内容没有更改。‎</li></ul><h4 id="‎文件写入时不要监控"><a href="#‎文件写入时不要监控" class="headerlink" title="‎文件写入时不要监控"></a>‎文件写入时不要监控</h4><p>‎”完整”文件系统（如 HDFS）倾向于在创建输出流后立即设置其文件的修改时间。当文件被打开时，甚至在数据完全写入之前，它也可能包含在 - 中，之后将忽略同一窗口中对文件的更新。也就是说：可能会错过更改，并且从流中省略数据。‎</p><p>‎要确保在窗口中选取更改，<strong>请将文件写入不受监视的目录</strong>，然后在输出流关闭后立即将其重命名为目标目录。如果重命名的文件在创建过程中出现在扫描的目标目录中，则将选取新数据。‎</p><h4 id="‎DStream-上的转换‎"><a href="#‎DStream-上的转换‎" class="headerlink" title="‎DStream 上的转换‎"></a>‎DStream 上的转换‎</h4><table><thead><tr><th align="left">Transformation</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>map</strong>(<em>func</em>)</td><td align="left">Return a new DStream by passing each element of the source DStream through a function <em>func</em>.</td></tr><tr><td align="left"><strong>flatMap</strong>(<em>func</em>)</td><td align="left">Similar to map, but each input item can be mapped to 0 or more output items.</td></tr><tr><td align="left"><strong>filter</strong>(<em>func</em>)</td><td align="left">Return a new DStream by selecting only the records of the source DStream on which <em>func</em> returns true.</td></tr><tr><td align="left"><strong>repartition</strong>(<em>numPartitions</em>)</td><td align="left">Changes the level of parallelism in this DStream by creating more or fewer partitions.</td></tr><tr><td align="left"><strong>union</strong>(<em>otherStream</em>)</td><td align="left">Return a new DStream that contains the union of the elements in the source DStream and <em>otherDStream</em>.</td></tr><tr><td align="left"><strong>count</strong>()</td><td align="left">Return a new DStream of single-element RDDs by counting the number of elements in each RDD of the source DStream.</td></tr><tr><td align="left"><strong>reduce</strong>(<em>func</em>)</td><td align="left">Return a new DStream of single-element RDDs by aggregating the elements in each RDD of the source DStream using a function <em>func</em> (which takes two arguments and returns one). The function should be associative and commutative so that it can be computed in parallel.</td></tr><tr><td align="left"><strong>countByValue</strong>()</td><td align="left">When called on a DStream of elements of type K, return a new DStream of (K, Long) pairs where the value of each key is its frequency in each RDD of the source DStream.</td></tr><tr><td align="left"><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td><td align="left">When called on a DStream of (K, V) pairs, return a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function. <strong>Note:</strong> By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property <code>spark.default.parallelism</code>) to do the grouping. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td></tr><tr><td align="left"><strong>join</strong>(<em>otherStream</em>, [<em>numTasks</em>])</td><td align="left">When called on two DStreams of (K, V) and (K, W) pairs, return a new DStream of (K, (V, W)) pairs with all pairs of elements for each key.</td></tr><tr><td align="left"><strong>cogroup</strong>(<em>otherStream</em>, [<em>numTasks</em>])</td><td align="left">When called on a DStream of (K, V) and (K, W) pairs, return a new DStream of (K, Seq[V], Seq[W]) tuples.</td></tr><tr><td align="left"><strong>transform</strong>(<em>func</em>)</td><td align="left">Return a new DStream by applying a RDD-to-RDD function to every RDD of the source DStream. This can be used to do arbitrary RDD operations on the DStream.</td></tr><tr><td align="left"><strong>updateStateByKey</strong>(<em>func</em>)</td><td align="left">Return a new “state” DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.</td></tr></tbody></table><h4 id="‎UpdateStateByKey-操作‎"><a href="#‎UpdateStateByKey-操作‎" class="headerlink" title="‎UpdateStateByKey 操作‎"></a>‎UpdateStateByKey 操作‎</h4><p>‎该操作允许您保持任意状态，同时不断使用新信息对其进行更新。要使用它，您必须执行两个步骤。‎<code>updateStateByKey</code></p><ol><li>‎定义状态 - 状态可以是任意数据类型。‎</li><li>‎定义状态更新函数 - 使用函数指定如何使用以前的状态和输入流中的新值来更新状态。‎</li></ol><p>‎在每个批次中，Spark 都会对所有现有密钥应用状态更新功能，无论它们是否在批处理中具有新数据。如果更新函数返回，则键值对将被淘汰。‎<code>None</code></p><h4 id="‎转换操作‎"><a href="#‎转换操作‎" class="headerlink" title="‎转换操作‎"></a>‎转换操作‎</h4><p>‎该操作（及其变体，如）允许在DStream上应用任意RDD到RDD函数。它可用于应用 DStream API 中未公开的任何 RDD 操作。例如，将数据流中的每个批与另一个数据集联接的功能不会直接在 DStream API 中公开。但是，您可以轻松地使用它来执行此操作。这带来了非常强大的可能性。例如，可以通过将输入数据流与预先计算的垃圾邮件信息（也可能是由Spark生成的）连接起来，然后基于它进行过滤来执行实时数据清理。‎</p><h4 id="‎窗口操作‎"><a href="#‎窗口操作‎" class="headerlink" title="‎窗口操作‎"></a>‎窗口操作‎</h4><p>‎Spark 流式处理还提供‎<em>‎窗口化计算‎</em>‎，允许您在滑动的数据窗口上应用转换。下图说明了此滑动窗口。‎</p><p>‎如图所示，每次窗口在源 DStream 上‎<em>‎滑动‎</em>‎时，落在窗口内的源 RDD 都会被组合并操作，以生成窗口 DStream 的 RDD。在此特定情况下，该操作应用于数据的最近 3 个时间单位，并按 2 个时间单位滑动。这表明任何窗口操作都需要指定两个参数。‎</p><ul><li><em>‎窗口长度‎</em>‎ - 窗口的持续时间（图中为 3）。‎</li><li><em>‎滑动间隔‎</em>‎ - 执行窗口操作的时间间隔（图中为 2）。‎</li></ul><p>‎这两个参数必须是源 DStream 的批处理间隔的倍数（图中为 1）。‎</p><p>‎让我们通过一个示例来说明窗口操作。假设，您希望通过每 10 秒生成一次数据的最后 30 秒的字数统计来扩展‎<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#a-quick-example">‎前面的示例‎</a>‎。为此，我们必须在数据的最后 30 秒内对 DStream 上应用该操作。这是使用 操作完成的。‎<code>reduceByKey``pairs``(word, 1)``reduceByKeyAndWindow</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Reduce last 30 seconds of data, every 10 seconds</span></span><br><span class="line"><span class="keyword">val</span> windowedWordCounts = pairs.reduceByKeyAndWindow((a:<span class="type">Int</span>,b:<span class="type">Int</span>) =&gt; (a + b), <span class="type">Seconds</span>(<span class="number">30</span>), <span class="type">Seconds</span>(<span class="number">10</span>))</span><br></pre></td></tr></table></figure><table><thead><tr><th align="left">Transformation</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>window</strong>(<em>windowLength</em>, <em>slideInterval</em>)</td><td align="left">Return a new DStream which is computed based on windowed batches of the source DStream.</td></tr><tr><td align="left"><strong>countByWindow</strong>(<em>windowLength</em>, <em>slideInterval</em>)</td><td align="left">Return a sliding window count of elements in the stream.</td></tr><tr><td align="left"><strong>reduceByWindow</strong>(<em>func</em>, <em>windowLength</em>, <em>slideInterval</em>)</td><td align="left">Return a new single-element stream, created by aggregating elements in the stream over a sliding interval using <em>func</em>. The function should be associative and commutative so that it can be computed correctly in parallel.</td></tr><tr><td align="left"><strong>reduceByKeyAndWindow</strong>(<em>func</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td><td align="left">When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function <em>func</em> over batches in a sliding window. <strong>Note:</strong> By default, this uses Spark’s default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property <code>spark.default.parallelism</code>) to do the grouping. You can pass an optional <code>numTasks</code> argument to set a different number of tasks.</td></tr><tr><td align="left"><strong>reduceByKeyAndWindow</strong>(<em>func</em>, <em>invFunc</em>, <em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td><td align="left">A more efficient version of the above <code>reduceByKeyAndWindow()</code> where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter <em>invFunc</em>). Like in <code>reduceByKeyAndWindow</code>, the number of reduce tasks is configurable through an optional argument. Note that <a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#checkpointing">checkpointing</a> must be enabled for using this operation.</td></tr><tr><td align="left"><strong>countByValueAndWindow</strong>(<em>windowLength</em>, <em>slideInterval</em>, [<em>numTasks</em>])</td><td align="left">When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in <code>reduceByKeyAndWindow</code>, the number of reduce tasks is configurable through an optional argument.</td></tr></tbody></table><h4 id="‎联接操作‎"><a href="#‎联接操作‎" class="headerlink" title="‎联接操作‎"></a>‎联接操作‎</h4><p>‎最后，值得强调的是，您可以在Spark Streaming中轻松执行不同类型的联接。‎</p><h5 id="‎流-流联接‎"><a href="#‎流-流联接‎" class="headerlink" title="‎流-流联接‎"></a>‎流-流联接‎</h5><p>‎流可以很容易地与其他流连接。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> stream1: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> stream2: <span class="type">DStream</span>[<span class="type">String</span>, <span class="type">String</span>] = ...</span><br><span class="line"><span class="keyword">val</span> joinedStream = stream1.join(stream2)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>‎在每个批处理间隔中，生成的 RDD 将与 生成的 RDD 合并。也可以在流的窗口上进行联接通常非常有用。这也很容易。‎</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> windowedStream1 = stream1.window(<span class="type">Seconds</span>(<span class="number">20</span>))</span><br><span class="line"><span class="keyword">val</span> windowedStream2 = stream2.window(<span class="type">Minutes</span>(<span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> joinedStream = windowedStream1.join(windowedStream2)</span><br></pre></td></tr></table></figure><h4 id="‎DStream-上的输出操作‎"><a href="#‎DStream-上的输出操作‎" class="headerlink" title="‎DStream 上的输出操作‎"></a>‎DStream 上的输出操作‎</h4><p>‎输出操作允许将 DStream 的数据推送到外部系统，如数据库或文件系统。由于输出操作实际上允许外部系统使用转换后的数据，因此它们会触发所有 DStream 转换的实际执行（类似于 RDD 的操作）。目前，定义了以下输出操作：‎</p><table><thead><tr><th align="left">Output Operation</th><th align="left">Meaning</th></tr></thead><tbody><tr><td align="left"><strong>print</strong>()</td><td align="left">Prints the first ten elements of every batch of data in a DStream on the driver node running the streaming application. This is useful for development and debugging. <strong>Python API</strong> This is called <strong>pprint()</strong> in the Python API.</td></tr><tr><td align="left"><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td align="left">Save this DStream’s contents as text files. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: <em>“prefix-TIME_IN_MS[.suffix]”</em>.</td></tr><tr><td align="left"><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td align="left">Save this DStream’s contents as <code>SequenceFiles</code> of serialized Java objects. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: <em>“prefix-TIME_IN_MS[.suffix]”</em>. <strong>Python API</strong> This is not available in the Python API.</td></tr><tr><td align="left"><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td align="left">Save this DStream’s contents as Hadoop files. The file name at each batch interval is generated based on <em>prefix</em> and <em>suffix</em>: <em>“prefix-TIME_IN_MS[.suffix]”</em>. <strong>Python API</strong> This is not available in the Python API.</td></tr><tr><td align="left"><strong>foreachRDD</strong>(<em>func</em>)</td><td align="left">The most generic output operator that applies a function, <em>func</em>, to each RDD generated from the stream. This function should push the data in each RDD to an external system, such as saving the RDD to files, or writing it over the network to a database. Note that the function <em>func</em> is executed in the driver process running the streaming application, and will usually have RDD actions in it that will force the computation of the streaming RDDs.</td></tr></tbody></table><h3 id="‎缓存-x2F-持久性‎"><a href="#‎缓存-x2F-持久性‎" class="headerlink" title="‎缓存&#x2F;持久性‎"></a>‎缓存&#x2F;持久性‎</h3><p>‎与RDD类似，DStreams还允许开发人员将流的数据保存在内存中。也就是说，在 DStream 上使用该方法将自动在内存中保留该 DStream 的每个 RDD。如果 DStream 中的数据将被多次计算（例如，对同一数据执行多个操作），这将非常有用。对于基于窗口的操作（如 和）和基于状态的操作（如 ），这是隐式正确的。因此，由基于窗口的操作生成的 DStream 会自动保留在内存中，而无需开发人员调用 。‎<code>persist()``reduceByWindow``reduceByKeyAndWindow``updateStateByKey``persist()</code></p><p>‎对于通过网络接收数据的输入流（如 Kafka、套接字等），默认持久性级别设置为将数据复制到两个节点以实现容错。‎</p><p>‎请注意，与 RDD 不同，DStreams 的默认持久性级别将数据序列化在内存中。‎<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#memory-tuning">‎这将在性能调整‎</a>‎部分中进一步讨论。有关不同持久性级别的详细信息，请参阅 ‎<a href="http://spark.incubator.apache.org/docs/3.1.2/rdd-programming-guide.html#rdd-persistence">‎Spark 编程指南‎</a>‎。‎</p><hr><h3 id="‎检查点‎"><a href="#‎检查点‎" class="headerlink" title="‎检查点‎"></a>‎检查点‎</h3><p>‎流式处理应用程序必须全天候运行，因此必须能够灵活应对与应用程序逻辑无关的故障（例如，系统故障、JVM 崩溃等）。为了实现这一点，Spark 流需要将足够的信息‎<em>‎检查点‎</em>‎到容错存储系统，以便它可以从故障中恢复。有两种类型的数据需要检查点。‎</p><ul><li><em>‎元数据检查点‎</em>‎ - 将定义流式计算的信息保存到容错存储（如 HDFS）中。这用于从运行流式处理应用程序驱动程序的节点的故障中恢复（稍后将详细讨论）。元数据包括：‎<ul><li><em>‎配置‎</em>‎ - 用于创建流式处理应用程序的配置。‎</li><li><em>‎DStream 操作‎</em>‎ - 定义流式处理应用程序的 DStream 操作集。‎</li><li><em>‎不完整批‎</em>‎处理 - 作业已排队但尚未完成的批处理。‎</li></ul></li><li><em>‎数据检查点‎</em>‎ - 将生成的RDD保存到可靠的存储中。这在某些跨多个批处理组合数据的‎<em>‎有状态‎</em>‎转换中是必需的。在此类转换中，生成的 RDD 依赖于先前批次的 RDD，这会导致依赖关系链的长度随时间不断增加。为了避免恢复时间的这种无限增加（与依赖关系链成正比），有状态转换的中间RDD定期‎<em>‎通过检查点‎</em>‎连接到可靠存储（例如HDFS）以切断依赖关系链。‎</li></ul><p>‎总而言之，元数据检查点主要用于从驱动程序故障中恢复，而数据或 RDD 检查点对于使用有状态转换时，即使对于基本功能也是必需的。‎</p><h3 id="‎何时启用检查点‎"><a href="#‎何时启用检查点‎" class="headerlink" title="‎何时启用检查点‎"></a>‎何时启用检查点‎</h3><p>‎必须为具有以下任一要求的应用程序启用检查点：‎</p><ul><li><em>‎有状态转换的使用‎</em>‎ - 如果在应用程序中使用或（具有反向函数），则必须提供检查点目录以允许定期 RDD 检查点。‎<code>updateStateByKey``reduceByKeyAndWindow</code></li><li><em>‎从运行应用程序的驱动程序的故障中恢复‎</em>‎ - 元数据检查点用于使用进度信息进行恢复。‎</li></ul><p>‎请注意，无需启用检查点即可运行没有上述有状态转换的简单流式处理应用程序。在这种情况下，从驱动程序故障中恢复也将是部分的（一些已接收但未处理的数据可能会丢失）。这通常是可以接受的，许多以这种方式运行 Spark 流式处理应用程序。对非Hadoop环境的支持预计将来会得到改善。‎</p><h3 id="‎如何配置检查点‎"><a href="#‎如何配置检查点‎" class="headerlink" title="‎如何配置检查点‎"></a>‎如何配置检查点‎</h3><p>‎可以通过在容错、可靠的文件系统（例如 HDFS、S3 等）中设置一个目录来启用检查点，检查点信息将保存到该文件系统中。这是通过使用 来完成的。这将允许您使用上述有状态转换。此外，如果要使应用程序从驱动程序故障中恢复，则应重写流式处理应用程序以具有以下行为。‎<code>streamingContext.checkpoint(checkpointDirectory)</code></p><ul><li>‎当程序首次启动时，它将创建一个新的StreamingContext，设置所有流，然后调用start（）。‎</li><li>‎当程序在失败后重新启动时，它将从检查点目录中的检查点数据重新创建流式处理上下文。‎</li></ul><h2 id="‎部署应用程序‎"><a href="#‎部署应用程序‎" class="headerlink" title="‎部署应用程序‎"></a>‎部署应用程序‎</h2><p>‎本节讨论部署 Spark 流式处理应用程序的步骤。‎</p><h2 id="‎要求‎"><a href="#‎要求‎" class="headerlink" title="‎要求‎"></a>‎要求‎</h2><p>‎若要运行 Spark 流式处理应用程序，需要具备以下条件。‎</p><ul><li><em>‎具有集群管理器的集群‎</em>‎ - 这是任何 Spark 应用程序的一般要求，在‎<a href="http://spark.incubator.apache.org/docs/3.1.2/cluster-overview.html">‎部署指南‎</a>‎中进行了详细讨论。‎</li><li><em>‎打包应用程序 JAR‎</em>‎ - 您必须将流式处理应用程序编译为 JAR。如果使用 ‎<a href="http://spark.incubator.apache.org/docs/3.1.2/submitting-applications.html"><code>‎spark-submit‎</code></a>‎ 启动应用程序，则无需在 JAR 中提供 Spark 和 Spark 流式处理。但是，如果您的应用程序使用‎<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#advanced-sources">‎高级源‎</a>‎（例如 Kafka），则必须将它们链接到的额外工件及其依赖项打包到用于部署应用程序的 JAR 中。例如，使用的应用程序必须在应用程序 JAR 中包含其所有可传递依赖项及其所有依赖项。‎<code>KafkaUtils``spark-streaming-kafka-0-10_2.12</code></li><li><em>‎为执行程序配置足够的内存‎</em>‎ - 由于接收的数据必须存储在内存中，因此必须为执行程序配置足够的内存来保存接收的数据。请注意，如果要执行 10 分钟的窗口操作，系统必须在内存中保留至少最后 10 分钟的数据。因此，应用程序的内存要求取决于其中使用的操作。‎</li><li><em>‎配置检查点‎</em>‎ - 如果流应用程序需要它，则必须将Hadoop API兼容容错存储中的目录（例如HDFS，S3等）配置为检查点目录和流应用程序，其编写方式是检查点信息可用于故障恢复。有关更多详细信息，请参阅‎<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#checkpointing">‎检查点‎</a>‎部分。‎</li><li><em>‎配置应用程序驱动程序的自动重新启动‎</em>‎ - 若要从驱动程序故障中自动恢复，用于运行流式处理应用程序的部署基础结构必须监视驱动程序进程，并在驱动程序失败时重新启动驱动程序。不同的‎<a href="http://spark.incubator.apache.org/docs/3.1.2/cluster-overview.html#cluster-manager-types">‎集群管理器‎</a>‎有不同的工具来实现这一点。‎<ul><li><em>‎Spark 独立‎</em>‎ - 可以提交 Spark 应用程序驱动程序以在 Spark 独立群集中运行（请参阅‎<a href="http://spark.incubator.apache.org/docs/3.1.2/spark-standalone.html#launching-spark-applications">‎群集部署模式‎</a>‎），即应用程序驱动程序本身在其中一个工作节点上运行。此外，可以指示独立集群管理器‎<em>‎监督‎</em>‎驱动程序，并在驱动程序由于非零退出代码或由于运行驱动程序的节点故障而失败时重新启动驱动程序。有关更多详细信息，请参阅 ‎<a href="http://spark.incubator.apache.org/docs/3.1.2/spark-standalone.html">‎Spark 独立指南‎</a>‎中的‎<em>‎群集模式‎</em>‎和‎<em>‎监督‎</em>‎。‎</li><li><em>‎YARN‎</em>‎ - Yarn 支持用于自动重新启动应用程序的类似机制。有关更多详细信息，请参阅 YARN 文档。‎</li><li><em>‎Mesos‎</em>‎ - ‎<a href="https://github.com/mesosphere/marathon">‎Marathon‎</a>‎ 已被用于通过 Mesos 实现这一目标。‎</li></ul></li><li><em>‎配置预写日志‎</em>‎ - 自 Spark 1.2 起，我们引入了‎<em>‎预写日志‎</em>‎以实现强大的容错保证。如果启用，则从接收方接收的所有数据都将写入配置检查点目录中的预写日志中。这可以防止驱动程序恢复时丢失数据，从而确保零数据丢失（在‎<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#fault-tolerance-semantics">‎容错语义‎</a>‎部分中进行了详细讨论）。可以通过将‎<a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">‎配置参数‎</a>‎设置为 来启用此功能。但是，这些更强的语义可能会以单个接收方的接收吞吐量为代价。这可以通过‎<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#level-of-parallelism-in-data-receiving">‎并行运行更多接收器‎</a>‎来纠正，以提高聚合吞吐量。此外，建议在启用预写日志时禁用 Spark 中接收的数据的复制，因为日志已存储在复制的存储系统中。这可以通过将输入流的存储级别设置为 来完成。使用 S3（或任何不支持刷新的文件系统）进行‎<em>‎预写日志‎</em>‎时，请记住启用 和 。有关更多详细信息‎<a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">‎，请参阅 Spark 流式处理配置‎</a>‎。请注意，启用 I&#x2F;O 加密时，Spark 不会对写入预写日志的数据进行加密。如果需要对预写日志数据进行加密，则应将其存储在本机支持加密的文件系统中。‎<code>spark.streaming.receiver.writeAheadLog.enable``true``StorageLevel.MEMORY_AND_DISK_SER``spark.streaming.driver.writeAheadLog.closeFileAfterWrite``spark.streaming.receiver.writeAheadLog.closeFileAfterWrite</code></li><li><em>‎设置最大接收速率‎</em>‎ - 如果群集资源不够大，使流式处理应用程序无法像接收数据一样快地处理数据，则可以通过设置记录&#x2F;秒的最大速率限制来限制接收器的速率。请参阅接收器和直接 Kafka 方法的‎<a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">‎配置参数‎</a>‎。在 Spark 1.5 中，我们引入了一个名为‎<em>‎”背压‎</em>‎”的功能，无需设置此速率限制，因为 Spark 流会自动计算出速率限制，并在处理条件发生变化时动态调整它们。可以通过将‎<a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">‎配置参数‎</a>‎设置为 来启用此背压。‎</li></ul><h2 id="升级应用程序代码"><a href="#升级应用程序代码" class="headerlink" title="升级应用程序代码"></a>升级应用程序代码</h2><p>如果正在运行的 Spark 流式处理应用程序需要使用新的应用程序代码进行升级，则有两种可能的机制。</p><ul><li>升级后的 Spark 流式处理应用程序将启动并与现有应用程序并行运行。一旦新一个（接收与旧数据相同的数据）已经预热并准备好进入黄金时段，旧一个就可以被关闭。请注意，对于支持将数据发送到两个目标（即早期和升级的应用程序）的数据源，可以执行此操作。</li><li>现有应用程序正常关闭（有关正常关闭选项，请参阅 <a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/streaming/StreamingContext.html"><code>StreamingContext.stop（...）</code></a> 或 <a href="http://spark.incubator.apache.org/docs/3.1.2/api/java/index.html?org/apache/spark/streaming/api/java/JavaStreamingContext.html"><code>JavaStreamingContext.stop（...），</code></a>这可确保在关闭之前完全处理已接收的数据。然后可以启动升级的应用程序，该应用程序将从先前应用程序中断的同一点开始处理。请注意，这只能使用支持源端缓冲的输入源（如 Kafka）来完成，因为在以前的应用程序关闭且升级后的应用程序尚未启动时，需要缓冲数据。并且无法从升级前代码的早期检查点信息重新启动。检查点信息实质上包含序列化的 Scala&#x2F;Java&#x2F;Python 对象，尝试使用新的、修改过的类反序列化对象可能会导致错误。在这种情况下，请使用不同的检查点目录启动升级后的应用，或删除以前的检查点目录。</li></ul><hr><h2 id="监控应用程序"><a href="#监控应用程序" class="headerlink" title="监控应用程序"></a>监控应用程序</h2><p>除了 Spark 的<a href="http://spark.incubator.apache.org/docs/3.1.2/monitoring.html">监控功能</a>之外，还有特定于 Spark Streaming 的其他功能。使用StreamingContext时，<a href="http://spark.incubator.apache.org/docs/3.1.2/monitoring.html#web-interfaces">Spark Web UI</a>会显示一个附加选项卡，其中显示有关正在运行的接收器（接收器是否处于活动状态，收到的记录数，接收器错误等）和已完成批处理（批处理时间，排队延迟等）的统计信息。这可用于监视流式处理应用程序的进度。<code>Streaming</code></p><p>Web UI 中的以下两个指标尤其重要：</p><ul><li><em>处理时间</em> - 处理每批数据的时间。</li><li><em>计划延迟</em> - 批处理在队列中等待处理先前批处理完成的时间。</li></ul><p>如果批处理时间始终大于批处理间隔和&#x2F;或排队延迟不断增加，则表明系统无法像生成批处理批处理时那样快速处理批处理，并且正在落后。在这种情况下，请考虑<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#reducing-the-batch-processing-times">减少</a>批处理时间。</p><p>Spark 流式处理程序的进度也可以使用 <a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/streaming/scheduler/StreamingListener.html">StreamingListener</a> 接口进行监视，该接口允许您获取接收方状态和处理时间。请注意，这是一个开发人员API，将来可能会对其进行改进（即，报告的更多信息）。</p><hr><hr><h2 id="性能调优-1"><a href="#性能调优-1" class="headerlink" title="性能调优"></a>性能调优</h2><p>从群集上的 Spark 流式处理应用程序获得最佳性能需要一些调整。本节介绍一些参数和配置，可以调整这些参数和配置以提高应用程序的性能。在高层次上，您需要考虑两件事：</p><ol><li>通过有效使用群集资源，缩短每批数据的处理时间。</li><li>设置正确的批大小，以便可以像接收数据一样快地处理数据批次（即，数据处理跟上数据引入的步伐）。</li></ol><h2 id="减少批处理时间"><a href="#减少批处理时间" class="headerlink" title="减少批处理时间"></a>减少批处理时间</h2><p>Spark 中可以进行许多优化，以最大限度地减少每个批次的处理时间。这些已在<a href="http://spark.incubator.apache.org/docs/3.1.2/tuning.html">《调优指南</a>》中详细讨论过。本节重点介绍一些最重要的问题。</p><h3 id="数据接收的并行度级别"><a href="#数据接收的并行度级别" class="headerlink" title="数据接收的并行度级别"></a>数据接收的并行度级别</h3><p>通过网络接收数据（如 Kafka、套接字等）需要反序列化数据并将其存储在 Spark 中。如果数据接收成为系统中的瓶颈，则考虑并行化数据接收。请注意，每个输入 DStream 都会创建一个接收单个数据流的接收器（在工作计算机上运行）。因此，可以通过创建多个输入 DStream 并将其配置为从源接收数据流的不同分区来实现接收多个数据流。例如，接收两个数据主题的单个 Kafka 输入 DStream 可以拆分为两个 Kafka 输入流，每个输入流仅接收一个主题。这将运行两个接收器，允许并行接收数据，从而提高整体吞吐量。这些多个 DStream 可以合并在一起以创建单个 DStream。然后，应用于单个输入 DStream 的转换可以应用于统一流。这按如下方式完成。</p><ul><li><a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#tab_scala_19"><strong>斯卡拉</strong></a></li><li><a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#tab_java_19"><strong>爪哇岛</strong></a></li><li><a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#tab_python_19"><strong>蟒</strong></a></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val numStreams = 5</span><br><span class="line">val kafkaStreams = (1 to numStreams).map &#123; i =&gt; KafkaUtils.createStream(...) &#125;</span><br><span class="line">val unifiedStream = streamingContext.union(kafkaStreams)</span><br><span class="line">unifiedStream.print()</span><br></pre></td></tr></table></figure><p>另一个应该考虑的参数是接收器的块间隔，它由<a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-streaming">配置参数</a>确定。对于大多数接收器，接收到的数据在存储在Spark的内存中之前被合并成数据块。每个批处理中的块数决定了将在类似地图的转换中用于处理接收到的数据的任务数。每个接收方每个批次的任务数约为（批次间隔&#x2F;块间隔）。例如，200 毫秒的块间隔将每 2 个批次创建 10 个任务。如果任务数太少（即少于每台计算机的核心数），则效率低下，因为不会使用所有可用核心来处理数据。若要增加给定批处理间隔的任务数，请减少块间隔。但是，建议的最小块间隔值约为 50 毫秒，低于此值，任务启动开销可能是一个问题。<code>spark.streaming.blockInterval</code></p><p>使用多个输入流&#x2F;接收器接收数据的替代方法是显式地对输入数据流进行重新分区（使用 ）。这会在进一步处理之前，将接收到的数据批次分布到群集中指定数量的计算机上。<code>inputStream.repartition(&lt;number of partitions&gt;)</code></p><p>有关直接流式传输，请参阅 <a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-kafka-0-10-integration.html">Spark Streaming + Kafka 集成指南</a></p><h3 id="数据处理中的并行度级别"><a href="#数据处理中的并行度级别" class="headerlink" title="数据处理中的并行度级别"></a>数据处理中的并行度级别</h3><p>如果在计算的任何阶段使用的并行任务数不够高，则群集资源可能未得到充分利用。例如，对于分布式 reduce 操作（如 和 ），并行任务的默认数目由<a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-properties">配置属性控制</a>。您可以将并行度级别作为参数传递（请参阅 <a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/streaming/dstream/PairDStreamFunctions.html"><code>PairDStreamFunctions</code></a> 文档），或设置<a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#spark-properties">配置属性</a>以更改默认值。<code>reduceByKey``reduceByKeyAndWindow``spark.default.parallelism``spark.default.parallelism</code></p><h3 id="数据序列化"><a href="#数据序列化" class="headerlink" title="数据序列化"></a>数据序列化</h3><p>通过调整序列化格式，可以减少数据序列化的开销。对于流式处理，有两种类型的数据正在序列化。</p><ul><li><strong>输入数据</strong>：默认情况下，通过接收器接收的输入数据存储在执行器的内存中<a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/storage/StorageLevel$.html">，StorageLevel.MEMORY_AND_DISK_SER_2</a>。也就是说，数据被序列化为字节以减少GC开销，并复制以容忍执行程序故障。此外，数据首先保留在内存中，并且仅当内存不足以保存流式计算所需的所有输入数据时才溢出到磁盘。这种序列化显然会产生开销 - 接收方必须反序列化接收的数据，并使用 Spark 的序列化格式对其进行重新序列化。</li><li><strong>流式处理操作生成的持久化 RDD：流</strong>式处理计算生成的 RDD 可以持久保存在内存中。例如，窗口操作将数据保留在内存中，因为它们将被多次处理。但是，与 Spark Core 默认的 <a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/storage/StorageLevel$.html">StorageLevel.MEMORY_ONLY</a> 不同，流式处理计算生成的持久化 RDD 默认使用<a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/storage/StorageLevel.html$">StorageLevel.MEMORY_ONLY_SER</a>（即序列化）进行持久化，以最大程度地减少 GC 开销。</li></ul><p>在这两种情况下，使用 Kryo 序列化都可以减少 CPU 和内存开销。有关更多详细信息<a href="http://spark.incubator.apache.org/docs/3.1.2/tuning.html#data-serialization">，请参阅 Spark 调优指南</a>。对于 Kryo，请考虑注册自定义类并禁用对象引用跟踪（请参阅<a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#compression-and-serialization">配置指南</a>中的 Kryo 相关配置）。</p><p>在需要为流式处理应用程序保留的数据量不大的特定情况下，将数据（两种类型）保留为反序列化对象可能会很可行，而不会产生过多的 GC 开销。例如，如果使用几秒钟的批处理间隔且没有窗口操作，则可以尝试通过相应地显式设置存储级别来禁用持久化数据中的序列化。这将减少由于序列化而导致的 CPU 开销，从而有可能在不产生太多 GC 开销的情况下提高性能。</p><h3 id="任务启动开销"><a href="#任务启动开销" class="headerlink" title="任务启动开销"></a>任务启动开销</h3><p>如果每秒启动的任务数很高（例如，每秒 50 个或更多），则向执行程序发送任务的开销可能很大，并且很难实现亚秒级延迟。可以通过以下更改来减少开销：</p><ul><li><strong>执行模式</strong>：在独立模式或粗粒度 Mesos 模式下运行 Spark 可比细粒度 Mesos 模式获得更好的任务启动时间。有关更多详细信息，请参阅在 <a href="http://spark.incubator.apache.org/docs/3.1.2/running-on-mesos.html">Mesos 上运行指南</a>。</li></ul><p>这些更改可以将批处理时间减少 100 毫秒，从而使亚秒级批大小可行。</p><hr><h2 id="设置正确的批处理间隔"><a href="#设置正确的批处理间隔" class="headerlink" title="设置正确的批处理间隔"></a>设置正确的批处理间隔</h2><p>为了使在群集上运行的 Spark 流式处理应用程序保持稳定，系统应该能够像接收数据一样快地处理数据。换句话说，批量数据的处理速度应与生成数据的速度一样快。通过<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#monitoring-applications">监视</a>流式处理 Web UI 中的处理时间（其中批处理时间应小于批处理间隔），可以找到应用程序是否属于这种情况。</p><p>根据流式计算的性质，使用的批处理间隔可能会对应用程序在一组固定的群集资源上可以维持的数据速率产生重大影响。例如，让我们考虑前面的 WordCountNetwork 示例。对于特定的数据速率，系统可能能够每2秒（即，2秒的批处理间隔）跟上报告字数，但不是每500毫秒。因此，需要设置批次间隔，以便可以维持生产中的预期数据速率。</p><p>为应用程序确定正确的批大小是使用保守的批处理间隔（例如，5-10 秒）和低数据速率对其进行测试。若要验证系统是否能够跟上数据速率，可以检查每个已处理批次所经历的端到端延迟值（在 Spark 驱动程序 log4j 日志中查找”总延迟”，或使用<a href="http://spark.incubator.apache.org/docs/3.1.2/api/scala/org/apache/spark/streaming/scheduler/StreamingListener.html">流式处理听信器</a>接口）。如果将延迟保持为与批大小相当，则系统是稳定的。否则，如果延迟不断增加，则意味着系统无法跟上，因此不稳定。一旦您了解了稳定的配置，就可以尝试提高数据速率和&#x2F;或减小批大小。请注意，由于临时数据速率增加而导致的延迟暂时增加可能很好，只要延迟减小到较低值（即小于批大小）。</p><hr><h2 id="内存调整"><a href="#内存调整" class="headerlink" title="内存调整"></a>内存调整</h2><p>优化<a href="http://spark.incubator.apache.org/docs/3.1.2/tuning.html#memory-tuning">指南</a>中已经详细讨论了调整 Spark 应用程序的内存使用情况和 GC 行为。强烈建议您阅读该书。在本节中，我们将专门讨论 Spark 流式处理应用程序上下文中的一些优化参数。</p><p>Spark 流式处理应用程序所需的群集内存量在很大程度上取决于所使用的转换类型。例如，如果要对过去 10 分钟的数据使用窗口操作，则集群应具有足够的内存，以便在内存中保存 10 分钟的数据。或者，如果您想与大量按键一起使用，那么必要的内存将很高。相反，如果要执行简单的映射-过滤器-存储操作，则必要的内存将不足。<code>updateStateByKey</code></p><p>通常，由于通过接收器接收的数据与StorageLevel.MEMORY_AND_DISK_SER_2一起存储，因此不适合内存的数据将溢出到磁盘。这可能会降低流式处理应用程序的性能，因此建议根据流式处理应用程序的要求提供足够的内存。最好尝试以小规模查看内存使用情况并进行相应的估计。</p><p>内存调整的另一个方面是垃圾回收。对于需要低延迟的流式处理应用程序，不希望 JVM 垃圾回收导致大量暂停。</p><p>有几个参数可以帮助您调整内存使用情况和 GC 开销：</p><ul><li><strong>DStream 的持久性级别</strong>：如前所述<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#data-serialization">，在数据序列化</a>部分中，输入数据和 RDD 默认保留为序列化字节。与反序列化持久性相比，这减少了内存使用量和 GC 开销。启用 Kryo 序列化可进一步减少序列化大小和内存使用量。通过压缩可以进一步减少内存使用量（参见 Spark 配置），但代价是 CPU 时间。<code>spark.rdd.compress</code></li><li><strong>清除旧数据</strong>：默认情况下，将自动清除 DStream 转换生成的所有输入数据和持久化 RDD。Spark 流式处理根据使用的转换来决定何时清除数据。例如，如果您使用的是 10 分钟的窗口操作，则 Spark 流式处理将保留最后 10 分钟左右的数据，并主动丢弃较旧的数据。通过设置 ， 数据可以保留更长的时间（例如，以交互方式查询较旧的数据）。<code>streamingContext.remember</code></li><li><strong>CMS 垃圾回收器</strong>：强烈建议使用并发标记和扫描 GC，以保持与 GC 相关的暂停始终保持在较低水平。尽管已知并发 GC 会降低系统的整体处理吞吐量，但仍建议使用 GC 来实现更一致的批处理时间。确保在驱动程序（使用 in）和执行程序（使用 <a href="http://spark.incubator.apache.org/docs/3.1.2/configuration.html#runtime-environment">Spark 配置</a>）上设置 CMS GC。<code>--driver-java-options``spark-submit``spark.executor.extraJavaOptions</code></li><li><strong>其他提示</strong>：要进一步减少 GC 开销，下面是一些其他提示供您尝试。<ul><li>使用存储级别保留 RDD。有关更多详细信息，请参阅 <a href="http://spark.incubator.apache.org/docs/3.1.2/rdd-programming-guide.html#rdd-persistence">Spark 编程指南</a>。<code>OFF_HEAP</code></li><li>使用更多堆大小较小的执行程序。这将减少每个 JVM 堆中的 GC 压力。</li></ul></li></ul><hr><h5 id="要记住的要点："><a href="#要记住的要点：" class="headerlink" title="要记住的要点："></a>要记住的要点：</h5><ul><li>DStream 与单个接收器相关联。为了实现读取并行性，需要创建多个接收器，即多个DStream。接收器在执行器内运行。它占据一个核心。确保在预订接收器插槽后有足够的内核进行处理，即 应考虑接收器插槽。接收方以轮循机制方式分配给执行程序。<code>spark.cores.max</code></li><li>从流源接收数据时，接收方会创建数据块。每隔一毫秒就会生成一个新的数据块。在 batchInterval 期间创建 N 个数据块，其中 N &#x3D; batchInterval&#x2F;blockInterval。这些块由当前执行器的 BlockManager 分发给其他执行器的块管理器。之后，在驱动程序上运行的网络输入跟踪器将被告知块位置以进行进一步处理。</li><li>在驱动程序上为批处理期间创建的块创建 RDD。在批处理期间生成的块是 RDD 的分区。每个分区都是 spark 中的一个任务。blockInterval&#x3D;&#x3D; batchinterval 意味着创建了一个分区，并且可能在本地进行处理。</li><li>块上的映射任务在执行器（一个接收块，另一个复制块的位置）中处理，该执行器具有块而不考虑块间隔，除非非本地调度启动。拥有更大的区块意味着更大的区块。较高的 值 会增加处理本地节点上的块的几率。需要在这两个参数之间找到平衡，以确保较大的块在本地处理。<code>spark.locality.wait</code></li><li>无需依赖 batchInterval 和 blockInterval，您可以通过调用 来定义分区数。这会随机重新排列 RDD 中的数据，以创建 n 个分区。是的，为了提高并行性。虽然是以洗牌为代价的。RDD 的处理由驾驶员的作业分拣员安排为作业。在给定的时间点，只有一个作业处于活动状态。因此，如果一个作业正在执行，则其他作业将排队。<code>inputDstream.repartition(n)</code></li><li>如果您有两个 dstream，则将形成两个 RDD，并且将创建两个作业，这些作业将一个接一个地安排。为避免这种情况，可以合并两个 dstream。这将确保为 d 流的两个 RDD 形成一个联合 RDD。然后，该工会RDD被视为单一工作。但是，RDD 的分区不受影响。</li><li>如果批处理时间超过批处理间隔，那么显然接收方的内存将开始填满，并最终引发异常（最有可能是BlockNotFoundException）。目前，无法暂停接收机。使用SparkConf配置，接收器的速率可以受到限制。<code>spark.streaming.receiver.maxRate</code></li></ul><hr><hr><h2 id="容错语义"><a href="#容错语义" class="headerlink" title="容错语义"></a>容错语义</h2><p>在本节中，我们将讨论 Spark 流式处理应用程序在发生故障时的行为。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了理解 Spark Streaming 提供的语义，让我们记住 Spark 的 RDD 的基本容错语义。</p><ol><li>RDD 是不可变的、确定性可重计算的分布式数据集。每个 RDD 都会记住用于创建容错输入数据集的确定性操作的沿袭。</li><li>如果RDD的任何分区由于工作线程节点故障而丢失，则可以使用操作沿袭从原始容错数据集重新计算该分区。</li><li>假设所有 RDD 转换都是确定性的，则无论 Spark 群集中发生故障，最终转换的 RDD 中的数据都将始终相同。</li></ol><p>Spark 对 HDFS 或 S3 等容错文件系统中的数据进行操作。因此，从容错数据生成的所有RDD也是容错的。但是，Spark 流的情况并非如此，因为在大多数情况下，数据是通过网络接收的（除非使用时）。为了对所有生成的 RDD 实现相同的容错属性，将在群集中工作线程节点中的多个 Spark 执行程序之间复制接收到的数据（默认复制因子为 2）。这导致系统中的两种数据在发生故障时需要恢复：<code>fileStream</code></p><ol><li><em>接收和复制的数据</em> - 此数据在单个工作线程节点发生故障后仍会继续存在，因为该数据的副本存在于其他节点之一上。</li><li><em>已接收但缓冲以进行复制的数据</em> - 由于不会复制此数据，因此恢复此数据的唯一方法是从源中再次获取它。</li></ol><p>此外，我们应该关注两种故障：</p><ol><li><em>工作节点故障</em> - 任何运行执行程序的工作线程节点都可能失败，并且这些节点上的所有内存中数据都将丢失。如果任何接收器在故障节点上运行，则其缓冲数据将丢失。</li><li><em>驱动程序节点故障</em> - 如果运行 Spark 流式处理应用程序的驱动程序节点失败，则显然 SparkContext 将丢失，并且所有执行程序及其内存中数据都将丢失。</li></ol><p>有了这些基本知识，让我们了解一下 Spark 流的容错语义。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>流系统的语义通常根据系统可以处理每条记录的次数来捕获。系统可以在所有可能的操作条件下提供三种类型的保证（尽管出现故障等）。</p><ol><li><em>最多一次</em>：每条记录将处理一次或根本不处理。</li><li><em>至少一次</em>：每条记录将被处理一次或多次。这比<em>最多一次</em>更强大，因为它确保不会丢失任何数据。但可能会有重复。</li><li><em>正好一次</em>：每条记录将只处理一次 - 不会丢失任何数据，也不会多次处理任何数据。这显然是三者中最有力的保证。</li></ol><h2 id="基本语义"><a href="#基本语义" class="headerlink" title="基本语义"></a>基本语义</h2><p>从广义上讲，在任何流处理系统中，处理数据都有三个步骤。</p><ol><li><em>接收数据</em>：使用接收器或其他方式从源接收数据。</li><li><em>转换数据</em>：使用 DStream 和 RDD 转换转换接收到的数据。</li><li><em>推出数据</em>：最终转换后的数据被推送到外部系统，如文件系统、数据库、仪表板等。</li></ol><p>如果流应用程序必须实现端到端的精确一次保证，那么每个步骤都必须提供一次完全相同的保证。也就是说，每个记录必须只接收一次，转换一次，然后推送到下游系统一次。让我们在 Spark 流式处理的上下文中了解这些步骤的语义。</p><ol><li><em>接收数据</em>：不同的输入源提供不同的保证。这将在下一小节中详细讨论。</li><li><em>转换数据</em>：由于RDD提供的保证，所有已收到的数据将只处理一<em>次</em>。即使出现故障，只要接收到的输入数据可访问，最终转换的RDD将始终具有相同的内容。</li><li><em>推出数据</em>：默认情况下，输出操作确保<em>至少一次</em>语义，因为它取决于输出操作的类型（幂等或不幂等）和下游系统的语义（是否支持事务）。但是用户可以实现自己的事务机制来实现<em>恰好一次</em>的语义。本节稍后将对此进行更详细的讨论。</li></ol><h2 id="接收数据的语义"><a href="#接收数据的语义" class="headerlink" title="接收数据的语义"></a>接收数据的语义</h2><p>不同的输入源提供不同的保证，从<em>至少一次</em>到<em>恰好一次</em>不等。阅读了解更多详情。</p><h3 id="使用文件"><a href="#使用文件" class="headerlink" title="使用文件"></a>使用文件</h3><p>如果所有输入数据都已存在于容错文件系统（如HDFS）中，则Spark Streaming始终可以从任何故障中恢复并处理所有数据。这给出了<em>恰好一次</em>的语义，这意味着无论什么失败，所有数据都将被精确地处理一次。</p><h3 id="使用基于接收器的源"><a href="#使用基于接收器的源" class="headerlink" title="使用基于接收器的源"></a>使用基于接收器的源</h3><p>对于基于接收器的输入源，容错语义取决于故障场景和接收器类型。如前所述<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#receiver-reliability">，有</a>两种类型的接收器：</p><ol><li><em>可靠接收器</em> - 这些接收器仅在确保已复制接收的数据后才确认可靠的来源。如果此类接收器发生故障，则源将不会收到缓冲（未复制）数据的确认。因此，如果接收方重新启动，源将重新发送数据，并且不会因故障而丢失任何数据。</li><li><em>不可靠的接收器</em> - 此类接收器<em>不</em>发送确认，因此当它们由于工作线程或驱动程序故障而失败时<em>可能会</em>丢失数据。</li></ol><p>根据所使用的接收器类型，我们实现以下语义。如果工作线程节点发生故障，则使用可靠的接收器不会丢失数据。对于不可靠的接收器，接收但未复制的数据可能会丢失。如果驱动程序节点发生故障，则除了这些损失之外，在内存中接收和复制的所有过去数据都将丢失。这将影响有状态转换的结果。</p><p>为了避免过去接收数据的这种丢失，Spark 1.2引入了<em>预写日志</em>，将接收到的数据保存到容错存储中。启用<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-programming-guide.html#deploying-applications">预写日志</a>和可靠的接收器后，数据丢失为零。在语义方面，它提供了至少一次的保证。</p><p>下表总结了失败下的语义：</p><table><thead><tr><th align="left">部署方案</th><th align="left">工作线程故障</th><th align="left">驱动程序故障</th></tr></thead><tbody><tr><td align="left"><em>Spark 1.1 或更早版本，</em>或 <em>Spark 1.2 或更高版本，不带预写日志</em></td><td align="left">使用不可靠的接收器丢失缓冲数据 使用可靠的接收器 实现零数据丢失 至少一次语义</td><td align="left">不可靠的接收器丢失的缓冲数据 所有接收器丢失的过去数据 未定义的语义</td></tr><tr><td align="left"><em>Spark 1.2 或更高版本，带有预写日志</em></td><td align="left">使用可靠的接收器 实现零数据丢失 至少一次语义</td><td align="left">通过可靠的接收器和文件 实现零数据丢失 至少一次语义</td></tr><tr><td align="left"></td><td align="left"></td><td align="left"></td></tr></tbody></table><h3 id="使用-Kafka-Direct-API"><a href="#使用-Kafka-Direct-API" class="headerlink" title="使用 Kafka Direct API"></a>使用 Kafka Direct API</h3><p>在 Spark 1.3 中，我们引入了一个新的 Kafka Direct API，它可以确保 Spark Streaming 只接收一次所有 Kafka 数据。除此之外，如果您实现精确一次的输出操作，则可以实现端到端的精确一次保证。<a href="http://spark.incubator.apache.org/docs/3.1.2/streaming-kafka-0-10-integration.html">《Kafka 集成指南</a>》进一步讨论了这种方法。</p><h2 id="输出操作的语义"><a href="#输出操作的语义" class="headerlink" title="输出操作的语义"></a>输出操作的语义</h2><p>输出操作（如 ）<em>至少具有一次</em>语义，也就是说，在工作线程发生故障时，转换后的数据可能会多次写入外部实体。虽然这对于使用操作保存到文件系统是可以接受的（因为文件只会被相同的数据覆盖），但可能需要额外的工作来实现恰好一次的语义。有两种方法。<code>foreachRDD``saveAs***Files</code></p><ul><li><em>幂等更新</em>：多次尝试始终写入相同的数据。例如，始终将相同的数据写入生成的文件。<code>saveAs***Files</code></li><li><em>事务性更新</em>：所有更新都是以事务方式进行的，因此更新恰好以原子方式进行一次。执行此操作的一种方法是以下方法。<ul><li>使用 RDD 的批处理时间（在 中可用）和分区索引来创建标识符。此标识符唯一标识流式处理应用程序中的 Blob 数据。<code>foreachRDD</code></li><li>使用标识符以事务方式（即，正好一次，原子方式）使用此 Blob 更新外部系统。也就是说，如果标识符尚未提交，请以原子方式提交分区数据和标识符。否则，如果已提交此操作，请跳过更新。</li></ul></li></ul>]]></content>
    
    
    <summary type="html">spark学习记录</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="spark" scheme="https://s-luping.github.io/luping/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习记录</title>
    <link href="https://s-luping.github.io/luping/2021/04/02/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <id>https://s-luping.github.io/luping/2021/04/02/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</id>
    <published>2021-04-02T04:15:14.000Z</published>
    <updated>2022-05-25T17:42:25.874Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop模块"><a href="#Hadoop模块" class="headerlink" title="Hadoop模块"></a>Hadoop模块</h1><p><strong>common 公共通用模块</strong> </p><p><strong>HDFS 文件存储</strong> </p><p><strong>YARN 资源管理</strong> </p><p><strong>MapReduce 计算框架</strong></p><h1 id="Hadoop集群安装部署"><a href="#Hadoop集群安装部署" class="headerlink" title="Hadoop集群安装部署"></a>Hadoop集群安装部署</h1><h2 id="虚拟机配置-linux网络配置"><a href="#虚拟机配置-linux网络配置" class="headerlink" title="虚拟机配置 linux网络配置"></a>虚拟机配置 linux网络配置</h2><p>1.修改主机名称 &#x2F;etc&#x2F;hostname<br>将克隆的2、3主机分别改名为hadoop02、hadoop03<br>2.主机名和ip映射配置 此处设置IP时注意</p><p><img src="https://img-blog.csdnimg.cn/f988601e2c4547d3ab810d33bd6230b6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>Host文件配置 三台虚拟机之间通信名称代替ip</p><p><img src="https://img-blog.csdnimg.cn/9292a3493936478692a62b93ea359383.png" alt="在这里插入图片描述"></p><p>2.网络参数配置 配置静态ip</p><p><img src="https://img-blog.csdnimg.cn/7ad6752ab0764ced83d872b97417331e.png" alt="在这里插入图片描述"></p><p>3.测试网卡配置</p><p>若修改vmware默认初始网段,出现无法ping通外网在上图虚拟网络编辑器还原默认配置,使用还原后的网段即可.</p><p><img src="https://img-blog.csdnimg.cn/3dc896681a3445719df76d9cc8ba164c.png" alt="在这里插入图片描述"></p><h2 id="SSH服务配置-免密登录"><a href="#SSH服务配置-免密登录" class="headerlink" title="SSH服务配置  免密登录"></a>SSH服务配置  免密登录</h2><p>1.生成私匙和公匙</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>将共匙加入authorized_keys文件， 复制公匙到自己以及hadoop02和hadoop03 实现免密登录，每一台都要将公匙复制到其他主机的authorized_keys文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id hadoop01</span><br><span class="line">ssh-copy-id hadoop02</span><br><span class="line">ssh-copy-id hadoop03</span><br></pre></td></tr></table></figure><h2 id="防火墙配置"><a href="#防火墙配置" class="headerlink" title="防火墙配置"></a>防火墙配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=9000/tcp --permanent</span><br><span class="line">firewall-cmd --zone=public --add-port=50075/tcp --permanent</span><br><span class="line">firewall-cmd --zone=public --add-port=8088/tcp --permanent</span><br></pre></td></tr></table></figure><p>hdfs 9000 50070 50010</p><p>yarn 8030 8031 8032 8088</p><p>journalnode 8485</p><p>zookeeper 2181 2888 3888 </p><p>放开端口需重载防火墙配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure><p>查看一下开放的端口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --list-ports</span><br></pre></td></tr></table></figure><p>常用的端口如下</p><p><img src="https://img-blog.csdnimg.cn/9cc4cbe16e244f6a980367b5a2632a3c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="JDK安装"><a href="#JDK安装" class="headerlink" title="JDK安装"></a>JDK安装</h2><p>运行hadoop需要先安装java环境，一般选择自己配置jdk，也可yum或apt安装</p><h2 id="Hadoop安装和集群配置"><a href="#Hadoop安装和集群配置" class="headerlink" title="Hadoop安装和集群配置"></a>Hadoop安装和集群配置</h2><p>1.将下载的hadoop-bin安装包上传至服务器</p><p>2.解压</p><h3 id="3-配置环境变量"><a href="#3-配置环境变量" class="headerlink" title="3.配置环境变量"></a>3.配置环境变量</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/servers/hadoop-2.9.2</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=<span class="variable">$HADOOP_HOME</span>/lib/*</span><br><span class="line"><span class="built_in">export</span> HADOOP_CLASSPATH=<span class="variable">$HADOOP_CLASSPATH</span>:<span class="variable">$HIVE_HOME</span>/lib/*</span><br></pre></td></tr></table></figure><p>需要注意的是配置环境变量的时候若有两个path 一定要记得两个都要在前面加$符号</p><p>4.验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop -version</span><br></pre></td></tr></table></figure><h3 id="主节点配置文件"><a href="#主节点配置文件" class="headerlink" title="主节点配置文件"></a>主节点配置文件</h3><p><strong>hadoop-env.sh</strong></p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/export/servers/jdk1.8.0</span><br></pre></td></tr></table></figure><p><strong>core-site.xml</strong><br><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/core-default.xml">官网core-default</a></p><p>高可用ha配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/export/data/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:2181,hadoop02:2181,hadoop03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--ipc超时重试次数和间隔--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.filter.initializers<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.security.AuthenticationFilterInitializer<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义用于HTTPweb控制台的身份验证。支持的值是:simple|kerberos|#AUTHENTICATION_HANDLER_CLASSNAME#--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.authentication.type<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>simple<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">    签名秘密文件，用于对身份验证令牌进行签名。</span></span><br><span class="line"><span class="comment">    对于集群中的每个服务，ResourceManager, NameNode, DataNode和NodeManager，应该使用不同的secret。</span></span><br><span class="line"><span class="comment">    这个文件应该只有运行守护进程的Unix用户可以读。</span></span><br><span class="line"><span class="comment">       --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.authentication.signature.secret.file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/hadoop-http-auth-user<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--禁止匿名用户访问--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.authentication.simple.anonymous.allowed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>hdfs-site.xml</strong><br><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html">官网hdfs-default</a></p><p>高可用ha配置 </p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--副本数量--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--元数据信息位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/export/data/hadoop/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--数据位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/export/data/hadoop/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--开启WEB-HDFS--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.webhdfs.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--ha集群名称--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--两台namenode名称--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--nn1的通信地址--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--RPC通信地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--http通信地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--nn2的通信地址--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--RPC通信地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--http通信地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--高可用集群通过共享数据做热备份--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定journal在本地磁盘的存放位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/export/data/hadoop/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--开启namenode失败自动切换--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--隔离机制自动切换时登录第二台namenode --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span></span><br><span class="line">            sshfence</span><br><span class="line">            shell(/bin/true)</span><br><span class="line">        <span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>30000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--journal连接配置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.qjournal.start-segment.timeout.ms<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>20000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.qjournal.select-input-streams.timeout.ms<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>20000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.qjournal.write-txns.timeout.ms<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>20000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>Mapred-site.xml</strong><br><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">官网mapred-site</a></p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--该参数限制输入文件数目，避免namenode出现元数据加载处理瓶颈。</span></span><br><span class="line"><span class="comment">     详细说明见“http://blog.csdn.net/fjssharpsword/article/details/70258251”</span></span><br><span class="line"><span class="comment">     --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.split.metainfo.maxsize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--map任务内存总大小 根据任务需要调整大小值--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4096<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx3072m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx6144m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>yarn-site.xml</strong><br><a href="http://hadoop.apache.org/docs/r2.6.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">yran常用参数参考</a><br>资源管理器负责配置调控CPU，内存，磁盘等分配和使用</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--开启resourcemanager高可用--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定cluster ID--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yrc<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop02<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--开启失败恢复--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- ZKRMStateStore配置 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定zookeeper集群地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop01:2181,hadoop02:2181,hadoop03:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志聚集功能使能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志保留时间设置7天 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">&lt;!--是否将对容器实施物理内存检测--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--是否将对容器实施虚拟内存检测--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--物理核心和虚拟核心比率--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--单个节点可用资源--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--表示该节点可使用的物理内存--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10240<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--表示该节点可使用的CPU核心数--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.cpu-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--scheduler给每个容器可分配资源--&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--表示每个container的最大物理内存--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--表示每个container的最大CPU核心数--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p><strong>slaves</strong><br><img src="https://img-blog.csdnimg.cn/1fa9f9b0100540bda5394c2b6f304878.png" alt="在这里插入图片描述"></p><p>配置分发</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/profile hadoop02:/etc</span><br><span class="line">scp /etc/profile hadoop03:/etc</span><br><span class="line"><span class="built_in">cd</span> /export/servers/hadoop-2.9.2/etc/hadoop</span><br><span class="line">scp ./* hadoop02:<span class="variable">$PWD</span></span><br><span class="line">scp ./* hadoop03:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure><h2 id="zookeeper安装并配置"><a href="#zookeeper安装并配置" class="headerlink" title="zookeeper安装并配置"></a>zookeeper安装并配置</h2><h3 id="1-下载并解压"><a href="#1-下载并解压" class="headerlink" title="1.下载并解压"></a>1.下载并解压</h3><h3 id="2-配置环境变量"><a href="#2-配置环境变量" class="headerlink" title="2.配置环境变量"></a>2.配置环境变量</h3><h3 id="3-配置zoo-cfg"><a href="#3-配置zoo-cfg" class="headerlink" title="3.配置zoo.cfg"></a>3.配置zoo.cfg</h3><p>进入安装目录下conf下新建zoo.cfg文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The number of milliseconds of each tick</span></span><br><span class="line">tickTime=2000</span><br><span class="line"><span class="comment"># The number of ticks that the initial </span></span><br><span class="line"><span class="comment"># synchronization phase can take</span></span><br><span class="line">initLimit=10</span><br><span class="line"><span class="comment"># The number of ticks that can pass between </span></span><br><span class="line"><span class="comment"># sending a request and getting an acknowledgement</span></span><br><span class="line">syncLimit=5</span><br><span class="line"><span class="comment"># the directory where the snapshot is stored.</span></span><br><span class="line"><span class="comment"># do not use /tmp for storage, /tmp here is just </span></span><br><span class="line"><span class="comment"># example sakes. </span></span><br><span class="line"><span class="comment"># myid 和数据文件所在文件夹 </span></span><br><span class="line">dataDir=/export/data/zookeeper</span><br><span class="line"><span class="comment"># the port at which the clients will connect</span></span><br><span class="line">clientPort=2181</span><br><span class="line"><span class="comment">#若与8088冲突需要更换此端口</span></span><br><span class="line">admin.serverPort=6888</span><br><span class="line"><span class="comment"># the maximum number of client connections.</span></span><br><span class="line"><span class="comment"># increase this if you need to handle more clients</span></span><br><span class="line">maxClientCnxns=60</span><br><span class="line">standaloneEnabled=<span class="literal">false</span></span><br><span class="line">admin.enableServer=<span class="literal">true</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Be sure to read the maintenance section of the </span></span><br><span class="line"><span class="comment"># administrator guide before turning on autopurge.</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># The number of snapshots to retain in dataDir</span></span><br><span class="line"><span class="comment">#autopurge.snapRetainCount=3</span></span><br><span class="line"><span class="comment"># Purge task interval in hours</span></span><br><span class="line"><span class="comment"># Set to &quot;0&quot; to disable auto purge feature</span></span><br><span class="line"><span class="comment">#autopurge.purgeInterval=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Metrics Providers</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># https://prometheus.io Metrics Exporter</span></span><br><span class="line"><span class="comment">#metricsProvider.className=org.apache.zookeeper.metrics.prometheus.PrometheusMetricsProvider</span></span><br><span class="line"><span class="comment">#metricsProvider.httpPort=7000</span></span><br><span class="line"><span class="comment">#metricsProvider.exportJvmInfo=true</span></span><br><span class="line">server.1=hadoop01:2888:3888</span><br><span class="line">server.2=hadoop01:2888:3888</span><br><span class="line">server.3=hadoop01:2888:3888</span><br><span class="line">4lw.commands.whitelist=*</span><br></pre></td></tr></table></figure><h3 id="添加myid文件"><a href="#添加myid文件" class="headerlink" title="添加myid文件"></a>添加myid文件</h3><p>进入zoo.cfg文件的dataDir目录新建myid文件 并输入id值</p><p>hadoop01输入值1，hadoop02输入值为2，hadoop03输入值为3</p><p><strong>当使用云服务器搭建集群时</strong><br>需注意本机地址使用内网ip，其他机器使用外网ip<br><img src="/luping/2021/04/02/Hadoop%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/zookeeper.png"></p><h2 id="Hadoop集群启动测试"><a href="#Hadoop集群启动测试" class="headerlink" title="Hadoop集群启动测试"></a>Hadoop集群启动测试</h2><p>1.启动各个节点的zookeeper服务 </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/f4f4ebf36370404482edfc9c4ef1a7fc.png" alt="在这里插入图片描述"></p><p>2.启动集群监控namenode的管理日志journalNode</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemons.sh start journalnode </span><br><span class="line"><span class="comment"># 说明：该命令不推荐使用，系统会提示使用新的命令</span></span><br><span class="line">hdfs –daemon start journalnode</span><br></pre></td></tr></table></figure><p>可以不用单独启动，在启动hadoop集群的时候会自动启动（如果配置了的journalnode情况）</p><p><img src="https://img-blog.csdnimg.cn/8e80e7cbcea643c3a465f566b7ed2899.png" alt="在这里插入图片描述"></p><p>3.在node-01上格式化namenode，并分发到node-02</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br><span class="line"><span class="comment"># 注意：只分发master和backupmaster </span></span><br><span class="line">scp –r /export/data/hadoop hadoop02:/export/data</span><br></pre></td></tr></table></figure><p>若初始化时出现下面错误</p><p><img src="https://img-blog.csdnimg.cn/50bb38befb3f48dbafd5470c3dbf320a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>错误原因：<br>我们在执行start-dfs.sh的时候，默认启动顺序是namenode&gt;datanode&gt;journalnode&gt;zkfc，如果journalnode和namenode不在一台机器启动的话，很容易因为网络延迟问题导致namenode无法连接journalnode，无法实现选举，最后导致刚刚启动的namenode会突然挂掉。虽然namenode启动时有重试机制等待journalnode的启动，但是由于重试次数限制，可能网络情况不好，导致重试次数用完了，也没有启动成功。</p><p>解决方法：</p><p>方法①：手动启动namenode，避免了网络延迟等待journalnode的步骤，一旦两个namenode连入journalnode，实现了选举，则不会出现失败情况。</p><p>方法②：先启动journalnode然后再运行start-dfs.sh。</p><p>方法③：把namenode对journalnode的容错次数或时间调成更大的值，保证能够对正常的启动延迟、网络延迟能容错。在hdfs-site.xml中修改ipc参数，namenode对journalnode检测的重试次数，默认为10次，每次1000ms，故网络情况差需要增加。具体修改信息为：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.max.retries<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">description</span>&gt;</span>Indicates the number of retries a client will make to establish</span><br><span class="line">      a server connection.</span><br><span class="line">     <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">name</span>&gt;</span>ipc.client.connect.retry.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Indicates the number of milliseconds a client will wait for</span><br><span class="line">  before retrying to establish a server connection.</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p><a href="https://blog.csdn.net/weixin_43482612/article/details/109556284">原文链接</a></p><p>4.在node-01上格式化ZKFC  这个命令必须自己敲出来不能复制</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc –formatZK</span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br></pre></td></tr></table></figure><p>结果 3台均正常启动dfs 和yarn<br><img src="https://img-blog.csdnimg.cn/19e1db8e52974713bc0b352402008be6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>如果有漏掉的机器没有启动 则可以用 在漏掉的机器上执行启动</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode </span><br><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line">yarn-daemon.sh start secondarymanager</span><br><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><h1 id="hadoop-job执行流程"><a href="#hadoop-job执行流程" class="headerlink" title="hadoop job执行流程"></a>hadoop job执行流程</h1><p><img src="https://img-blog.csdnimg.cn/5f68d2ac44e94ae8864dedd6ff6b4118.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>dataInput–&gt;split–&gt;Mapper–&gt;Combine–&gt;(产出临时数据–&gt;Partition–&gt;Sort–&gt;Reducer–&gt;最终数据。</p><h2 id="Mapper阶段"><a href="#Mapper阶段" class="headerlink" title="Mapper阶段"></a>Mapper阶段</h2><p>Mapper的数量由输入的大小和个数决定。在默认情况下，最终input占据了多少block，就应该启动多少个Mapper。500M的数据分成四个block（128M*4）就是4个mapper。</p><h2 id="分区-排序-溢写-文件合并"><a href="#分区-排序-溢写-文件合并" class="headerlink" title="分区 排序 溢写 文件合并"></a>分区 排序 溢写 文件合并</h2><p>partition默认分区  分区器是HashPartitioner  对numReduceTasks取模，模数相同分到同一分区，对key进行排序，当内存缓冲区达到阈值进行溢写到磁盘，产生的多个小文件将合并为一个大文件。分区对应reduce；<a href="https://blog.csdn.net/qq_35699475/article/details/75582072">参考文章</a><br><img src="https://img-blog.csdnimg.cn/842a74b920fd4ae8b1ea97dd47ead6b0.png" alt="在这里插入图片描述"></p><h2 id="Reduce"><a href="#Reduce" class="headerlink" title="Reduce"></a>Reduce</h2><p>Reducer将与一个key关联的一组中间数值集归为一个更小的数值集。它的数据来源可能是多个mapper的某个分区，过程就要进行shuffle，然后对收集到的key进行合并；<br>reduce的数量可以直接在程序设置job.setNumReduceTasks属性设置</p><h1 id="HDFS的Checkpoint"><a href="#HDFS的Checkpoint" class="headerlink" title="HDFS的Checkpoint"></a>HDFS的Checkpoint</h1><p>Checkpoint（检查点）：因为数据库系统或者像HDFS这样的分布式文件系统，对文件数据的修改不是直接写回到磁盘的，很多操作是先缓存到内存的Buffer中，当遇到一个检查点Checkpoint时，系统会强制将内存中的数据写回磁盘，当然此时才会记录日志，从而产生持久的修改状态。</p><p>在介绍Checkpoint之前，先来看看Namenode上面有些什么数据：</p><p><strong>edits</strong> HDFS操作的日志记录，没此对HDFS进行修改操作后，都会往edits中记录一条日志；</p><p><strong>fsimage</strong> HDFS中命名空间、数据块分布、文件属性等信息都存放在fsimage中；</p><p>edits是在每次修改HDFS时都会插入记录，那么fsimage则在整个HDFS运行期间不会产生变化，用HDFS官方文档的说法就是：NameNode merges fsimage and edits files only during start up。也就是说，只有在每次启动Namenode时，才会把edits中的操作增加到fsimage中，并且把edits清空。所以fsimage总是记录启动Namenode时的状态，而edits在每次启动时也是空的，它只记录本次启动后的操作日志。</p><h2 id="为什么需要checkpoint？"><a href="#为什么需要checkpoint？" class="headerlink" title="为什么需要checkpoint？"></a>为什么需要checkpoint？</h2><p>按照fsimage和edits的工作机制，在一次启动后，edits的文件可能会增长到很大，这样在下次启动Namenode时需要花费很长时间来恢复；<br>另一方面，如果在HDFS运行过程中发生Namenode的故障，那么edits中的记录就会丢失。所以，我们需要利用Checkpoint即使将修改操作持久化。</p><h2 id="checkpoint触发条件"><a href="#checkpoint触发条件" class="headerlink" title="checkpoint触发条件"></a>checkpoint触发条件</h2><p>在配置文件中的参数：</p><p>时间维度，默认一小时触发一次工作流程 dfs.namenode.checkpoint.period ：3600</p><p>次数维度，默认100万次触发一次工作流程 dfs.namenode.checkpoint.txns ： 1000000</p><p>大小维度，默认64M触发一次工作流程 fs.checkpoint.size：67108864。</p><p>也就说触发HDFS中Checkpoint的机制有三种，一是时间、次数和日志的大小</p><h2 id="checkpoint做了什么"><a href="#checkpoint做了什么" class="headerlink" title="checkpoint做了什么"></a>checkpoint做了什么</h2><p>Chekpoint主要干的事情是，将Namenode中的edits和fsimage文件拷贝到Second Namenode上，然后将edits中的操作与fsimage文件merge以后形成一个新的fsimage，这样不仅完成了对现有Namenode数据的备份，而且还产生了持久化操作的fsimage。</p><p>最后一步，Second Namenode需要把merge后的fsimage文件upload到Namenode上面，完成Namenode中fsimage的更新。<br>以上提到的文件都可以在hadoop系统的data目录下找到。<br><a href="https://blog.csdn.net/angelofmersy/article/details/40959039">原文</a></p><h3 id="shared-edits-dir-日志文件位置设置"><a href="#shared-edits-dir-日志文件位置设置" class="headerlink" title="shared.edits.dir 日志文件位置设置"></a>shared.edits.dir 日志文件位置设置</h3><p>当集群为高可用集群时standbynamenode会读取该目录下edits文件并与fsimage合并为新的fsimage</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--NAMENODE的元数据在journalnode的存放位置--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://hadoop01:8485;hadoop02:8485;hadoop03:8485/ns1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--指定journalnode在本地磁盘的存放位置--&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/export/data/hadoop/journaldata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="fsimage-fsimage文件位置"><a href="#fsimage-fsimage文件位置" class="headerlink" title="fsimage fsimage文件位置"></a>fsimage fsimage文件位置</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--元数据信息位置--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/export/data/hadoop/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="利用副本机制故障还原"><a href="#利用副本机制故障还原" class="headerlink" title="利用副本机制故障还原"></a>利用副本机制故障还原</h2><p>1.删掉Active NameNode的FSimage和Edits_Log模拟数据丢失<br>记录NN存储 和Edits_Log的路径</p><p>2.将Standby NameNode的FSimage和Edits_Log复制到NN的FSimage和Edits_Log对应的目录下</p><p>3.启动挂掉的NameNode<br><a href="https://blog.csdn.net/weixin_44704605/article/details/110946336">原文</a></p><h1 id="HDFS中的fsck命令"><a href="#HDFS中的fsck命令" class="headerlink" title="HDFS中的fsck命令"></a>HDFS中的fsck命令</h1><p>查看文件目录的健康信息</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /weblog</span><br></pre></td></tr></table></figure><p>查看文件中损坏的块 (-list-corruptfileblocks)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /weblog -list-corruptfileblocks</span><br></pre></td></tr></table></figure><h2 id="损坏文件的处理"><a href="#损坏文件的处理" class="headerlink" title="损坏文件的处理"></a>损坏文件的处理</h2><p>将损坏的文件移动至&#x2F;lost+found目录 (-move)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /user/hadoop-twq/cmd -move</span><br></pre></td></tr></table></figure><p>删除有损坏数据块的文件 (-delete)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /user/hadoop-twq/cmd -delete</span><br></pre></td></tr></table></figure><h2 id="打印文件的Block报告-blocks"><a href="#打印文件的Block报告-blocks" class="headerlink" title="打印文件的Block报告(-blocks)"></a>打印文件的Block报告(-blocks)</h2><p>执行下面的命令，可以查看一个指定文件的所有的Block详细信息，需要和-files一起使用：　</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /user/hadoop-twq/cmd/big_file.txt -files -blocks</span><br></pre></td></tr></table></figure><p>如果，我们在上面的命令再加上-locations的话，就是表示还需要打印每一个数据块的位置信息，如下：<br><img src="https://img-blog.csdnimg.cn/2cf4f6f42cd04325b61664e0d2ef924c.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h1 id="hdfs-haadmin-命令"><a href="#hdfs-haadmin-命令" class="headerlink" title="hdfs haadmin 命令"></a>hdfs haadmin 命令</h1><h2 id="transitionToActive"><a href="#transitionToActive" class="headerlink" title="-transitionToActive "></a>-transitionToActive <namenodeid></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line"><span class="comment">#nn1 -&gt; active</span></span><br><span class="line">hdfs haadmin -transitionToActive -forcemanual nn1</span><br><span class="line"><span class="comment">#nn2 -&gt; standby</span></span><br><span class="line">hdfs haadmin -transitionToStandby -forcemanual nn1</span><br></pre></td></tr></table></figure><h2 id="getServiceState"><a href="#getServiceState" class="headerlink" title="-getServiceState "></a>-getServiceState <serviceId></h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/bin/bash</span></span><br><span class="line">hdfs haadmin -getServiceState nn1</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">Hadoop学习记录</summary>
    
    
    
    <category term="bigdata" scheme="https://s-luping.github.io/luping/categories/bigdata/"/>
    
    
    <category term="Hadoop" scheme="https://s-luping.github.io/luping/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>练习从数据采集、分析到展示的过程</title>
    <link href="https://s-luping.github.io/luping/2021/03/26/%E7%BB%83%E4%B9%A0%E4%B8%80%E4%B8%AA%E4%BB%8E%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%88%86%E6%9E%90%E5%88%B0%E5%B1%95%E7%A4%BA%E7%9A%84%E8%BF%87%E7%A8%8B/"/>
    <id>https://s-luping.github.io/luping/2021/03/26/%E7%BB%83%E4%B9%A0%E4%B8%80%E4%B8%AA%E4%BB%8E%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%88%86%E6%9E%90%E5%88%B0%E5%B1%95%E7%A4%BA%E7%9A%84%E8%BF%87%E7%A8%8B/</id>
    <published>2021-03-26T15:50:52.000Z</published>
    <updated>2022-03-13T12:25:25.714Z</updated>
    
    <content type="html"><![CDATA[<h1 id="描述"><a href="#描述" class="headerlink" title="描述"></a>描述</h1><p>学了Python爬虫 文本分析 又看到学校上热搜 就写了个舆论监测的东西<br>#结果展示<br><strong>首页数据总览</strong><br><img src="https://img-blog.csdnimg.cn/20210313222235204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>热度排行榜TOP10</strong><br><img src="https://img-blog.csdnimg.cn/20210315233432230.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>趋势观察</strong><br><img src="https://img-blog.csdnimg.cn/20210313222449253.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20210313222511515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>词云展示</strong><br><img src="https://img-blog.csdnimg.cn/20210313222612656.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>lda话题分析</strong><br><img src="https://img-blog.csdnimg.cn/20210313222708167.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h1 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a>项目结构</h1><p><strong>数据采集、数据分析、数据展示</strong></p><p>信息来源都是面向公众的媒体平台，像微博、贴吧、知乎、微信这些，主要搜集关于某个主题文本信息。<br>爬下来的信息做了些初步的统计信息，和一些简单分析如上图。</p><h2 id="采集数据"><a href="#采集数据" class="headerlink" title="采集数据"></a>采集数据</h2><p>用python写的爬虫</p><h3 id="爬虫结构"><a href="#爬虫结构" class="headerlink" title="爬虫结构"></a>爬虫结构</h3><p><img src="https://img-blog.csdnimg.cn/20210313231506641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>用到的库</p><p><img src="https://img-blog.csdnimg.cn/20210313224218588.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>这几个平台的都有相应的反扒措施，但是我爬的都是大家都能看到的不违法，而且我用一台服务器每隔四个小时爬取一次，不会造成多大影响<br>于是我看了其他一些反反扒的文章抓到了数据</p><h2 id="模块介绍"><a href="#模块介绍" class="headerlink" title="模块介绍"></a>模块介绍</h2><p><strong>download模块</strong></p><p>通用下载模块，对网页内容下载</p><p><strong>parser模块</strong></p><p>请求到的信息格式微博、知乎是json格式，只需json.loads下来取某个key的values即可，微信、贴吧等是html网页源码格式，我使用的是BeautifulSoup库，soup.find_all()很顺手，使用lxml库的etree的xpath语法虽更简单，但是有时因为一个元素去改整个得到list很是麻烦</p><p><strong>dataoutput</strong></p><p>数据储存，我用的是mysql</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 数据统一格式</span><br><span class="line">mdata = <span class="punctuation">&#123;</span></span><br><span class="line">    &#x27;plantform&#x27;<span class="punctuation">:</span> &#x27;weibo&#x27;<span class="punctuation">,</span> </span><br><span class="line">    &#x27;mtimestamp&#x27;<span class="punctuation">:</span> create_time<span class="punctuation">,</span> </span><br><span class="line">    &#x27;hotnum&#x27;<span class="punctuation">:</span> hotnum<span class="punctuation">,</span> </span><br><span class="line">    &#x27;url&#x27;<span class="punctuation">:</span> url<span class="punctuation">,</span> </span><br><span class="line">    &#x27;title&#x27;<span class="punctuation">:</span> content<span class="punctuation">,</span></span><br><span class="line">    &#x27;comments&#x27;<span class="punctuation">:</span> comments</span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p><strong>spidermain</strong></p><p>调度模块，将前面三个模块综合起来</p><p><strong>爬虫这里遇到的问题：如何设计增量爬取，如何保证数据质量</strong></p><p><strong>解决方案</strong></p><p><strong>增量爬取</strong></p><p>每次爬取平台的信息前先查询数据库里最新的一条数据时间，作为参数只请求或保留该时间之后的数据即可</p><p><strong>解决数据质量问题</strong></p><p>我想要的是比如关于我们学校的人物、事件的讨论 就要对爬取的数据处理 比某些微博超话题下的微博内容就是些语气词 或是如 生气 开心这些分析价值不大的给过滤掉 一般小于三字的内容都可以过滤</p><p>时间储存统一使用时间戳，</p><p><strong>代码量</strong>：爬虫部分大概900行代码</p><p>本地调试完成，就可上传Linux服务器，设置定时任务，每天抓取，做为数据分析的基础</p><h2 id="数据处理分析"><a href="#数据处理分析" class="headerlink" title="数据处理分析"></a>数据处理分析</h2><p>爬虫写好并稳定运行后 就开始处理爬取到的数据 数据如下图</p><p><img src="https://img-blog.csdnimg.cn/2021031413532941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="原始数据"></p><p><strong>数据统计</strong> </p><p><strong>分组统计</strong>，</p><p>统计每个平台每天抓取的信息数量，web页面实时展示每个平台当天抓取的信息数量，定时每天凌晨将昨天统计数据写入mysql表，记录为每天各平台历史抓取信息数量，用于观察信息发布趋势变化</p><p><img src="https://img-blog.csdnimg.cn/2021031601040112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>词云</strong> </p><p>查询一段时间内的所有数据，将title、comment字段下的内容查询先使用分词工具jieba分词、去停用词等预处理，得到每条记录的keywords将这些记录丢进模型计算每个词TFIDF值做出词云，可观察这段时间讨论的内容的关键词。<br>其实这个跟下面的lda差不多</p><p><strong>情感分析</strong> </p><p>一是使用snownlp，但是结果很难让人满意。<br>二是使用情感词典(玻森情感词典)，如下，会有11万词的评分，使用使就是遍历该词典，结果得分一般是加和计算。</p><p><img src="https://img-blog.csdnimg.cn/20210314150320310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="珀森词典"><br>我对比后选择了后者。</p><p><strong>话题聚类</strong>  </p><p>将信息进行主题聚类，发现讨论的话题，<br>这里我使用的是lda主题模型，目标是找到每一篇文档的主题分布和每一个主题中词的分布，根据每个主题词的分布，我们加以概括这个主题，该主题下的文章内容我称之为一个讨论话题。<br>再处理文本之前先进行文本预处理，切词、去停用词，标记文档，参数调整，再训练模型即可，过程原理大家感兴趣可以找相关文档看。</p><p><strong>代码量</strong> </p><p>统计分析部分大概500行代码</p><h2 id="数据展示"><a href="#数据展示" class="headerlink" title="数据展示"></a>数据展示</h2><p>用Python flask 做的web，结合echarts生成一些图，更直观的查看信息发布趋势</p><p><img src="https://img-blog.csdnimg.cn/20210316011330473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><strong>代码量</strong> 该部分不算前端代码只有100行。</p><p><strong>总记代码量</strong> 不到2000行</p><p>每一部分的东西都是之前几个学期学的东西，零零散散组了起来。</p>]]></content>
    
    
    <summary type="html">练习从数据采集、分析到展示的过程</summary>
    
    
    
    <category term="python" scheme="https://s-luping.github.io/luping/categories/python/"/>
    
    
    <category term="python 爬虫 舆论监测" scheme="https://s-luping.github.io/luping/tags/python-%E7%88%AC%E8%99%AB-%E8%88%86%E8%AE%BA%E7%9B%91%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>东方财富股吧标题爬取分析</title>
    <link href="https://s-luping.github.io/luping/2021/03/10/%E4%B8%9C%E6%96%B9%E8%B4%A2%E5%AF%8C%E8%82%A1%E5%90%A7%E6%A0%87%E9%A2%98%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90/"/>
    <id>https://s-luping.github.io/luping/2021/03/10/%E4%B8%9C%E6%96%B9%E8%B4%A2%E5%AF%8C%E8%82%A1%E5%90%A7%E6%A0%87%E9%A2%98%E7%88%AC%E5%8F%96%E5%88%86%E6%9E%90/</id>
    <published>2021-03-10T01:46:21.000Z</published>
    <updated>2022-03-13T12:25:25.733Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20210326003708831.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2t1bjY2NjY2Ng==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>45个股吧，140万条数据库记录<br>日期从2018-03-01至2021-03-01共36个月的股吧帖子，<br>爬取股吧名称、阅读、评论、标题、作者和发帖时间，<br>并分析总体情绪</p><h2 id="亮点回顾"><a href="#亮点回顾" class="headerlink" title="亮点回顾"></a>亮点回顾</h2><p>时间问题<br>获取的时间未加年份，解决方法，观察发现发帖日期月份逐级递减，按获取顺序下一个时间月份在同一年内小于等于上一个月份，设一个变量m储存月份，始值设为12，与获取的最新月份new_m比较，若new_m&gt;m，使当前年份减一；再令m&#x3D;new_m。<br>数据去重问题<br>有时候爬取会因各种问题中断，当你再次续爬时数据会重复，于是我加了一个用于去重的myid<br>myid &#x3D; item[‘username’] + str(item[‘mdate’])[3:-4] + title[:100]<br>思想是，时间地点人物组合，即{<strong>谁</strong>}在{<strong>什么时间</strong>}{<strong>干了什么</strong>}地点没加，但也使每条记录内容保证唯一，大概率去重。<br>考虑过用每个news的url做主键去重，但是一下url是有重复的<br>创建的数据表语句如下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> info_guba</span><br><span class="line">(</span><br><span class="line">    id           <span class="type">int</span> auto_increment</span><br><span class="line">        <span class="keyword">primary</span> key,</span><br><span class="line">    myid         <span class="type">varchar</span>(<span class="number">255</span>) <span class="keyword">collate</span> utf8mb4_croatian_ci <span class="keyword">not</span> <span class="keyword">null</span>,</span><br><span class="line">    scans        <span class="type">int</span>(<span class="number">8</span>)                                   <span class="keyword">null</span>,</span><br><span class="line">    comments     <span class="type">int</span>(<span class="number">6</span>)                                   <span class="keyword">null</span>,</span><br><span class="line">    titles       text <span class="keyword">collate</span> utf8mb4_croatian_ci         <span class="keyword">null</span>,</span><br><span class="line">    usernames    <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">collate</span> utf8mb4_croatian_ci  <span class="keyword">null</span>,</span><br><span class="line">    mdates       <span class="type">int</span>(<span class="number">15</span>)                                  <span class="keyword">null</span>,</span><br><span class="line">    f_scores     <span class="type">float</span>(<span class="number">12</span>, <span class="number">10</span>) <span class="keyword">default</span> <span class="number">0.0000000000</span>       <span class="keyword">null</span>,</span><br><span class="line">    polarity     <span class="type">int</span>(<span class="number">1</span>)                                   <span class="keyword">null</span>,</span><br><span class="line">    company_name <span class="type">varchar</span>(<span class="number">80</span>) <span class="keyword">collate</span> utf8mb4_croatian_ci  <span class="keyword">null</span>,</span><br><span class="line">    industry     <span class="type">varchar</span>(<span class="number">80</span>) <span class="keyword">collate</span> utf8mb4_croatian_ci  <span class="keyword">null</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>跨年份取月份对应时间戳问题<br>2018-03至2021-03期间每个月份的情绪指数，要取每个月的一号和下个月的一号时间戳，使能完成区间取值，<br>我的方法<br>将38个月份数值储存在一个列表，遍历列表月份，如果月份没到倒数第二位，判断月份是否为12月，是则变量年份减一，拿日期转换时间戳得到时间段较大值，再判断，该月份的下一个月份是不是12月，是则年份减一，日期转换时间戳得到较小值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">months = [<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>,</span><br><span class="line">          <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, </span><br><span class="line">          <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, </span><br><span class="line">          <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line"><span class="keyword">for</span> num, m <span class="keyword">in</span> <span class="built_in">enumerate</span>(months):</span><br><span class="line">    <span class="keyword">if</span> num != <span class="number">36</span>:</span><br><span class="line">        <span class="keyword">if</span> m == <span class="number">12</span>:</span><br><span class="line">        year -= <span class="number">1</span></span><br><span class="line">    max_timestamp = self.date_to_timestamp(year, m)</span><br><span class="line">    <span class="keyword">if</span> months[num + <span class="number">1</span>] == <span class="number">12</span>:</span><br><span class="line">        year02 = year - <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        year02 = year</span><br><span class="line">    month = <span class="built_in">str</span>(year02) + <span class="string">&#x27;-&#x27;</span> + <span class="built_in">str</span>(months[num + <span class="number">1</span>])</span><br><span class="line">    min_timestamp = self.date_to_timestamp(year02, months[num + <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>爬虫部分代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /usr/bin/python </span></span><br><span class="line"><span class="comment"># --*-- coding:UTF-8 --*--</span></span><br><span class="line"><span class="comment"># Date 2021/3/18 16:35</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"><span class="keyword">from</span> snownlp <span class="keyword">import</span> SnowNLP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">guba</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, host, db, user, passwd</span>):</span><br><span class="line">        self.headers = &#123;<span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, &#x27;</span></span><br><span class="line">                                      <span class="string">&#x27;like Gecko) Chrome/89.0.4389.90 Safari/537.36 Edg/89.0.774.54&#x27;</span>&#125;</span><br><span class="line">        self.host = host</span><br><span class="line">        self.db = db</span><br><span class="line">        self.user = user</span><br><span class="line">        self.passwd = passwd</span><br><span class="line">        self.dataoutput = DataOutput()</span><br><span class="line">        self.ip_num = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取代理</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_new_ip</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">if</span> self.ip_num &lt;= <span class="number">1000</span>:</span><br><span class="line">            ip_port = requests.get(</span><br><span class="line">                <span class="string">&#x27;获取代理的api&#x27;</span>,</span><br><span class="line">                timeout=<span class="number">6</span>)</span><br><span class="line">            ip = ip_port.text.replace(<span class="string">&#x27;\r\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            proxyip = &#123;<span class="string">&quot;http&quot;</span>: <span class="string">&quot;http://&quot;</span> + ip,</span><br><span class="line">                       <span class="string">&quot;https&quot;</span>: <span class="string">&quot;https://&quot;</span> + ip&#125;</span><br><span class="line">            self.ip_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> proxyip</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 移除换行符</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rm_special_letters</span>(<span class="params">self, old_list</span>):</span><br><span class="line">        new_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> old_list:</span><br><span class="line">            i = i.replace(<span class="string">&#x27;\r\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            i = i.replace(<span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">            new_list.append(i)</span><br><span class="line">        <span class="keyword">return</span> new_list</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将日期格式转换为时间戳</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">date_to_timestamp</span>(<span class="params">self, year, timestr</span>):</span><br><span class="line">        mdate = <span class="built_in">str</span>(year) + <span class="string">&#x27;-&#x27;</span> + timestr</span><br><span class="line">        time_array = time.strptime(mdate, <span class="string">&quot;%Y-%m-%d&quot;</span>)</span><br><span class="line">        news_timestamp = time.mktime(time_array)</span><br><span class="line">        <span class="keyword">return</span> news_timestamp</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取每日热帖  阅读量 评论量 标题 用户 发帖时间</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dangu_pinglun</span>(<span class="params">self, url, company_name, industry</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param 所属板块:</span></span><br><span class="line"><span class="string">        :param 公司名称:</span></span><br><span class="line"><span class="string">        :type url: 股吧首页链接</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">global</span> mtimestamp</span><br><span class="line">        mtimestamp = time.time()</span><br><span class="line">        page = <span class="number">1</span></span><br><span class="line">        year = <span class="number">2021</span></span><br><span class="line">        latest_mounth = <span class="number">12</span></span><br><span class="line">        proxyip = self.get_new_ip()</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            datalist = []</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                <span class="keyword">if</span> page % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">                    <span class="comment">#每50页换一次代理ip 防止反扒</span></span><br><span class="line">                    <span class="built_in">print</span>(company_name, page, <span class="string">&quot;----&quot;</span> + <span class="built_in">str</span>(time.time()))</span><br><span class="line">                    proxyip = self.get_new_ip()</span><br><span class="line">                <span class="comment">#拼接url</span></span><br><span class="line">                murl = url + <span class="built_in">str</span>(page) + <span class="string">&#x27;.html&#x27;</span></span><br><span class="line">                resp = requests.get(murl, headers=self.headers, proxies=proxyip, timeout=<span class="number">10</span>)</span><br><span class="line">                <span class="comment"># print(resp.text)</span></span><br><span class="line">                htmltree = etree.HTML(resp.text)</span><br><span class="line">                yuedu_count = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l1 a1&quot;]/text()&#x27;</span>)</span><br><span class="line">                yuedu_count = self.rm_special_letters(yuedu_count)[<span class="number">1</span>:]</span><br><span class="line">                pinglun_count = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l2 a2&quot;]/text()&#x27;</span>)</span><br><span class="line">                pinglun_count = self.rm_special_letters(pinglun_count)[<span class="number">1</span>:]</span><br><span class="line">                title_list = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l3 a3&quot;]/a/@title&#x27;</span>)</span><br><span class="line">                username_list = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l4 a4&quot;]/a//text()&#x27;</span>)</span><br><span class="line">                last_time_list = htmltree.xpath(<span class="string">&#x27;//span[@class=&quot;l5 a5&quot;]/text()&#x27;</span>)[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 此处将评论列表保存到字典 交给dataoutput储存</span></span><br><span class="line">                <span class="keyword">for</span> num, p <span class="keyword">in</span> <span class="built_in">enumerate</span>(pinglun_count):</span><br><span class="line">                    <span class="comment"># 当阅读数量含有汉字时</span></span><br><span class="line">                    <span class="keyword">if</span> re.search(<span class="string">&#x27;[\u4e00-\u9fa5]&#x27;</span>, yuedu_count[num]):</span><br><span class="line">                        yuedu_count[num] = <span class="number">20000</span></span><br><span class="line">                    <span class="keyword">if</span> re.search(<span class="string">&#x27;[\u4e00-\u9fa5]&#x27;</span>, pinglun_count[num]):</span><br><span class="line">                        pinglun_count[num] = <span class="number">20000</span></span><br><span class="line">                    <span class="comment"># 截取时间具体提到天 去掉时分时间</span></span><br><span class="line">                    lastdate = last_time_list[num].split(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">                    <span class="comment">#发帖时间递减 ，当下层月份大于上边时年份减一</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="built_in">int</span>(lastdate.split(<span class="string">&#x27;-&#x27;</span>)[<span class="number">0</span>]) &gt; latest_mounth:</span><br><span class="line">                        year -= <span class="number">1</span></span><br><span class="line">                    mtimestamp = self.date_to_timestamp(year, lastdate)</span><br><span class="line">                    info_dict = &#123;<span class="string">&#x27;scan&#x27;</span>: yuedu_count[num],</span><br><span class="line">                                 <span class="string">&#x27;comment_num&#x27;</span>: pinglun_count[num],</span><br><span class="line">                                 <span class="string">&#x27;title&#x27;</span>: title_list[num],</span><br><span class="line">                                 <span class="string">&#x27;username&#x27;</span>: username_list[num],</span><br><span class="line">                                 <span class="string">&#x27;mdate&#x27;</span>: mtimestamp,</span><br><span class="line">                                 <span class="string">&#x27;company&#x27;</span>: company_name,</span><br><span class="line">                                 <span class="string">&#x27;industry&#x27;</span>: industry&#125;</span><br><span class="line">                    datalist.append(info_dict)</span><br><span class="line">                    latest_mounth = <span class="built_in">int</span>(lastdate.split(<span class="string">&#x27;-&#x27;</span>)[<span class="number">0</span>])</span><br><span class="line">                page += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># 将存库语句写到这里是为了个别字节数据储存终端而导致总程序终段</span></span><br><span class="line">                self.dataoutput.write_to_mysql(host=self.host, db=self.db, user=self.user, passwd=self.passwd,</span><br><span class="line">                                               datalist=datalist)</span><br><span class="line">                time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(industry, company_name, page, <span class="string">&quot;---&quot;</span> + <span class="built_in">str</span>(time.time()))</span><br><span class="line">                <span class="built_in">print</span>(<span class="built_in">str</span>(e))</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;HTTPConnectionPool&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                    proxyip = self.get_new_ip()</span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;index out of range&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                    page += <span class="number">1</span></span><br><span class="line">                <span class="keyword">elif</span> <span class="string">&#x27;day is out of range for month&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                    page += <span class="number">1</span></span><br><span class="line">            <span class="comment"># 此处判断总时间是否到达最大时间  即到2018年3月终止 爬取下一个</span></span><br><span class="line">            <span class="keyword">if</span> mtimestamp &lt;= <span class="number">1521475200</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;时间到&#x27;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataOutput</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.__tablename = <span class="string">&#x27;info_guba&#x27;</span></span><br><span class="line">        self.__tablekeys = <span class="string">&#x27;(myid,scans,comments,titles,usernames,mdates,f_scores,company_name,industry)&#x27;</span></span><br><span class="line"><span class="comment">#删除特殊字符 以防引起mysql异常</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rm_special_letter</span>(<span class="params">self, line</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> [<span class="string">&quot;\&#x27;&quot;</span>, <span class="string">&quot;\&quot;&quot;</span>, <span class="string">&quot;#&quot;</span>, <span class="string">&quot;\\&quot;</span>]:</span><br><span class="line">            line = line.replace(i, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> line</span><br><span class="line">        </span><br><span class="line"><span class="string">&quot;&quot;&quot;借助snownlp</span></span><br><span class="line"><span class="string">    分析news的情绪分为3级 0：积极  1：中立  2：消极&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feeling</span>(<span class="params">self, line</span>):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            res = SnowNLP(line)</span><br><span class="line">            f_score = res.sentiments</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            f_score = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> f_score</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__rm_stopwords</span>(<span class="params">self, wordlist</span>):</span><br><span class="line">        new_wordlist = []</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tool_files/stopwords.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> r:</span><br><span class="line">            stopwords = r.read()</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> wordlist:</span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">in</span> stopwords:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    new_wordlist.append(i)</span><br><span class="line">            <span class="keyword">return</span> new_wordlist</span><br><span class="line">            </span><br><span class="line"><span class="string">&quot;&quot;&quot;使用玻森情感词典 计算情绪指数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">feeling2</span>(<span class="params">self, line</span>):</span><br><span class="line">        path = <span class="string">&quot;tool_files/sentiment_score.txt&quot;</span></span><br><span class="line">        df = pd.read_table(path, sep=<span class="string">&quot; &quot;</span>, names=[<span class="string">&#x27;key&#x27;</span>, <span class="string">&#x27;score_snownlp&#x27;</span>])</span><br><span class="line">        key = df[<span class="string">&#x27;key&#x27;</span>].values.tolist()</span><br><span class="line">        score = df[<span class="string">&#x27;score_snownlp&#x27;</span>].values.tolist()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">getscore</span>(<span class="params">line</span>):</span><br><span class="line">            segs = jieba.lcut(line)  <span class="comment"># 分词</span></span><br><span class="line">            jieba.load_userdict(<span class="string">&#x27;tool_files/userdict.txt&#x27;</span>)</span><br><span class="line">            segs = self.__rm_stopwords(segs)</span><br><span class="line">            score_list = [score[key.index(x)] <span class="keyword">for</span> x <span class="keyword">in</span> segs <span class="keyword">if</span> (x <span class="keyword">in</span> key)]</span><br><span class="line">            <span class="comment"># 修改后的sentiment_score.txt 得分有的为字符串格式不能直接使用sum求和</span></span><br><span class="line">            <span class="comment"># print(score_list)</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(score_list) != <span class="number">0</span>:</span><br><span class="line">                sums = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> score_list:</span><br><span class="line">                    sums = sums + <span class="built_in">float</span>(i)</span><br><span class="line">                <span class="keyword">return</span> sums / <span class="built_in">len</span>(score_list)  <span class="comment"># 计算得分</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        last_score = getscore(line)</span><br><span class="line">        <span class="keyword">if</span> last_score == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">round</span>(last_score, <span class="number">5</span>)</span><br><span class="line"><span class="comment">#数据去重</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__mysql_data_rechecking</span>(<span class="params">self, item, ids_inmysql</span>):</span><br><span class="line">        id_inmysqls = [myid[<span class="number">0</span>] <span class="keyword">for</span> myid <span class="keyword">in</span> ids_inmysql]</span><br><span class="line">        title = self.rm_special_letter(item[<span class="string">&#x27;title&#x27;</span>])</span><br><span class="line">        myid = item[<span class="string">&#x27;username&#x27;</span>] + <span class="built_in">str</span>(item[<span class="string">&#x27;mdate&#x27;</span>])[<span class="number">3</span>:-<span class="number">4</span>] + title[:<span class="number">100</span>]</span><br><span class="line">        <span class="keyword">if</span> myid <span class="keyword">not</span> <span class="keyword">in</span> id_inmysqls:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;newrecord&#x27;</span>, title, myid</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">&#x27;数据已存在&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">write_to_mysql</span>(<span class="params">self, datalist, host, db, user, passwd</span>):</span><br><span class="line">        <span class="comment"># mysql连接初始化连接</span></span><br><span class="line">        db = pymysql.connect(host=host, user=user, password=passwd, database=db)</span><br><span class="line">        <span class="comment"># 使用 cursor() 方法创建一个游标对象cursor</span></span><br><span class="line">        cursor = db.cursor()</span><br><span class="line">        <span class="comment"># 查询表中 plantform title username 数据拼接字符串用于去重</span></span><br><span class="line">        quchong_sql = <span class="string">&#x27;SELECT myid FROM &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.__tablename)</span><br><span class="line">        cursor.execute(quchong_sql)</span><br><span class="line">        myids = cursor.fetchall()</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> datalist:</span><br><span class="line">            data = self.__mysql_data_rechecking(item, myids)</span><br><span class="line">            <span class="keyword">if</span> data[<span class="number">0</span>] == <span class="string">&#x27;newrecord&#x27;</span>:</span><br><span class="line">                title, myid = data[<span class="number">1</span>], data[<span class="number">2</span>]</span><br><span class="line">                <span class="comment"># feeling = self.feeling(title)</span></span><br><span class="line">                feeling = <span class="number">0</span></span><br><span class="line">                <span class="comment"># SQL插入语句</span></span><br><span class="line">                sql = <span class="string">&quot;INSERT INTO &#123;TABLENAME&#125;&#123;keys&#125;&quot;</span> \</span><br><span class="line">                      <span class="string">&quot;VALUES (&#x27;&#123;v0&#125;&#x27;,&#x27;&#123;v1&#125;&#x27;,&#x27;&#123;v2&#125;&#x27;,&#x27;&#123;v3&#125;&#x27;,&#x27;&#123;v4&#125;&#x27;,&#x27;&#123;v5&#125;&#x27;,&#x27;&#123;v6&#125;&#x27;,&#x27;&#123;v7&#125;&#x27;,&#x27;&#123;v8&#125;&#x27;)&quot;</span>.<span class="built_in">format</span> \</span><br><span class="line">                    (TABLENAME=self.__tablename,</span><br><span class="line">                     keys=self.__tablekeys,</span><br><span class="line">                     v0=myid,</span><br><span class="line">                     v1=item[<span class="string">&#x27;scan&#x27;</span>],</span><br><span class="line">                     v2=item[<span class="string">&#x27;comment_num&#x27;</span>],</span><br><span class="line">                     v3=title,</span><br><span class="line">                     v4=item[<span class="string">&#x27;username&#x27;</span>],</span><br><span class="line">                     v5=item[<span class="string">&#x27;mdate&#x27;</span>],</span><br><span class="line">                     v6=feeling,</span><br><span class="line">                     v7=item[<span class="string">&#x27;company&#x27;</span>],</span><br><span class="line">                     v8=item[<span class="string">&#x27;industry&#x27;</span>])</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="comment"># 执行sql语句</span></span><br><span class="line">                    cursor.execute(sql)</span><br><span class="line">                    <span class="comment"># 执行sql语句</span></span><br><span class="line">                    db.commit()</span><br><span class="line">                <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;PRIMARY&#x27;</span> <span class="keyword">in</span> <span class="built_in">str</span>(e):</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">&#x27;查重失败&#x27;</span>)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="built_in">print</span>(item)</span><br><span class="line">                        <span class="built_in">print</span>(<span class="built_in">str</span>(e) + <span class="string">&quot;---&quot;</span> + <span class="built_in">str</span>(time.time()))</span><br><span class="line">                        <span class="comment"># 发生错误时回滚</span></span><br><span class="line">                        db.rollback()</span><br><span class="line">                        <span class="comment"># raise e</span></span><br><span class="line">        <span class="comment"># 关闭数据库连接</span></span><br><span class="line">        db.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data01 = &#123;</span><br><span class="line">    <span class="string">&#x27;批发和零售业&#x27;</span>: [<span class="string">&#x27;大参林 603233&#x27;</span>, <span class="string">&#x27;广百股份 002187&#x27;</span>, <span class="string">&#x27;来伊份 603777&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;制造业&#x27;</span>: [<span class="string">&#x27;中国中车 601766&#x27;</span>, <span class="string">&#x27;永兴材料 002756&#x27;</span>, <span class="string">&#x27;海思科 002653&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;房地产业&#x27;</span>: [<span class="string">&#x27;格力地产 600185&#x27;</span>, <span class="string">&#x27;绿景控股 000502&#x27;</span>, <span class="string">&#x27;万科Ａ 000002&#x27;</span>],</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;租赁和商务服务业&#x27;</span>: [<span class="string">&#x27;深圳华强 000062&#x27;</span>, <span class="string">&#x27;渤海租赁 000415&#x27;</span>, <span class="string">&#x27;轻纺城 600790&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;采矿业&#x27;</span>: [<span class="string">&#x27;兴业矿业 000426&#x27;</span>, <span class="string">&#x27;冀中能源 000937&#x27;</span>, <span class="string">&#x27;中国石化 600028&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;交通运输、仓储和邮政业&#x27;</span>: [<span class="string">&#x27;中远海控 601919&#x27;</span>, <span class="string">&#x27;宜昌交运 002627&#x27;</span>, <span class="string">&#x27;大众交通 600611&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;信息传输、软件和信息技术服务业&#x27;</span>: [<span class="string">&#x27;恒生电子 600570&#x27;</span>, <span class="string">&#x27;中国联通 600050&#x27;</span>, <span class="string">&#x27;恒华科技 300365&#x27;</span>],</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;教育&#x27;</span>: [<span class="string">&#x27;好未来 ustal&#x27;</span>, <span class="string">&#x27;中公教育 002607&#x27;</span>, <span class="string">&#x27;紫光学大 000526&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;卫生和社会工作业&#x27;</span>: [<span class="string">&#x27;通策医疗 600763&#x27;</span>, <span class="string">&#x27;迪安诊断 300244&#x27;</span>, <span class="string">&#x27;爱尔眼科 300015&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;文化、体育和娱乐业&#x27;</span>: [<span class="string">&#x27;凤凰传媒 601928&#x27;</span>, <span class="string">&#x27;新华传媒 600825&#x27;</span>, <span class="string">&#x27;长江传媒 600757&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;金融业&#x27;</span>: [<span class="string">&#x27;民生银行 600016&#x27;</span>, <span class="string">&#x27;中国平安 601318&#x27;</span>, <span class="string">&#x27;国信证券 002736&#x27;</span>],</span><br><span class="line"></span><br><span class="line">    <span class="string">&#x27;建筑业&#x27;</span>: [<span class="string">&#x27;棕榈股份 002431&#x27;</span>, <span class="string">&#x27;上海建工 600170&#x27;</span>, <span class="string">&#x27;隧道股份 600820&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;电力、热力、燃气及水的生产和供应业&#x27;</span>: [<span class="string">&#x27;滨海能源 000695&#x27;</span>, <span class="string">&#x27;太阳能 000591&#x27;</span>, <span class="string">&#x27;上海电力 600021&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;水利、环境和公共设施管理业&#x27;</span>: [<span class="string">&#x27;远达环保 600292&#x27;</span>, <span class="string">&#x27;碧水源 300070&#x27;</span>, <span class="string">&#x27;启迪环境 000826&#x27;</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    gb = guba(host=<span class="string">&#x27;localhost&#x27;</span>, db=<span class="string">&#x27;guba&#x27;</span>, user=<span class="string">&#x27;root&#x27;</span>, passwd=<span class="string">&#x27;root&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data:</span><br><span class="line">        <span class="keyword">for</span> num, company <span class="keyword">in</span> <span class="built_in">enumerate</span>(data[item]):</span><br><span class="line">            stock_code = company.split(<span class="string">&#x27; &#x27;</span>)[<span class="number">1</span>]</span><br><span class="line">            name = company.split(<span class="string">&#x27; &#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">            url = <span class="string">&#x27;http://guba.eastmoney.com/list,&#x27;</span> + <span class="built_in">str</span>(stock_code) + <span class="string">&#x27;,f_&#x27;</span></span><br><span class="line">            gb.dangu_pinglun(url, name, item)</span><br></pre></td></tr></table></figure><p>情绪分析指标计算部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /usr/bin/python </span></span><br><span class="line"><span class="comment"># --*-- coding:UTF-8 --*--</span></span><br><span class="line"><span class="comment"># Date 2021/3/23 21:12</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pymysql</span><br><span class="line"><span class="keyword">from</span> dataoutput <span class="keyword">import</span> DataOutput</span><br><span class="line"></span><br><span class="line">d = DataOutput()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Analyse</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.host = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">        self.db = <span class="string">&#x27;guba&#x27;</span></span><br><span class="line">        self.user = <span class="string">&#x27;root&#x27;</span></span><br><span class="line">        self.passwd = <span class="string">&#x27;root&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 该部分计算每个title的情绪得分</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_score_and_polarity</span>(<span class="params">self</span>):</span><br><span class="line">        db = pymysql.connect(host=self.host, user=self.user, password=self.passwd, database=self.db)</span><br><span class="line">        cursor = db.cursor()</span><br><span class="line">        sql01 = <span class="string">&#x27;select titles,id from info_guba&#x27;</span></span><br><span class="line">        cursor.execute(sql01)</span><br><span class="line">        res = cursor.fetchall()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> res:</span><br><span class="line">            <span class="comment"># 使用snownlp计算情绪值</span></span><br><span class="line">            score_snownlp = d.feeling(i[<span class="number">0</span>])</span><br><span class="line">            <span class="comment"># 评出情感极性 将情绪得分&gt;0.6的评论当作积极评论，小于0.4的评论当作消极评论。</span></span><br><span class="line">            <span class="keyword">if</span> score_snownlp &lt;= <span class="number">0.6</span>:</span><br><span class="line">                <span class="keyword">if</span> score_snownlp &gt; <span class="number">0.4</span>:</span><br><span class="line">                    p = <span class="number">0</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    p = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                p = <span class="number">1</span></span><br><span class="line">            sql02 = <span class="string">&quot;update info_guba set f_scores=&#123;0&#125;,polarity=&#123;1&#125; where id=&#123;2&#125;&quot;</span>.<span class="built_in">format</span>(score_snownlp, p, i[<span class="number">1</span>])</span><br><span class="line">            cursor.execute(sql02)</span><br><span class="line">            db.commit()</span><br><span class="line">            <span class="built_in">print</span>(i[<span class="number">1</span>])</span><br><span class="line">        <span class="comment"># db.commit()</span></span><br><span class="line">        db.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算单股吧一天内所有帖子情绪值加和求平均</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_score_of_day</span>(<span class="params">self</span>):</span><br><span class="line">        db = pymysql.connect(host=self.host, user=self.user, password=self.passwd, database=self.db)</span><br><span class="line">        cursor = db.cursor()</span><br><span class="line">        sql03 = <span class="string">&quot;select company_name from info_guba group by company_name&quot;</span></span><br><span class="line">        cursor.execute(sql03)</span><br><span class="line">        com_names = cursor.fetchall()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> com_names:</span><br><span class="line">            <span class="comment"># 该语句查询每只股票散户每天发贴的情绪</span></span><br><span class="line">            sql04 = <span class="string">&quot;select round(sum(f_scores*log10(scans+comments))/count(f_scores),10),mdates from info_guba &quot;</span> \</span><br><span class="line">                    <span class="string">&quot;where company_name=&#x27;&#123;0&#125;&#x27; and usernames not like &#x27;%资讯%&#x27; group by &quot;</span> \</span><br><span class="line">                    <span class="string">&quot;mdates order by mdates&quot;</span>.<span class="built_in">format</span>(i[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">            cursor.execute(sql04)</span><br><span class="line">            res = cursor.fetchall()</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> res:</span><br><span class="line">                score = j[<span class="number">0</span>]</span><br><span class="line">                day = j[<span class="number">1</span>]</span><br><span class="line">                com_name = i[<span class="number">0</span>]</span><br><span class="line">                sql05 = <span class="string">&quot;insert into score_of_day(score,daytimestamp,company_name) &quot;</span> \</span><br><span class="line">                        <span class="string">&quot;values (&#123;0&#125;,&#123;1&#125;,&#x27;&#123;2&#125;&#x27;)&quot;</span>.<span class="built_in">format</span>(score, day, com_name)</span><br><span class="line">                <span class="comment"># print(sql03)</span></span><br><span class="line">                cursor.execute(sql05)</span><br><span class="line">                db.commit()</span><br><span class="line">        db.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将日期格式转换为时间戳</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">date_to_timestamp</span>(<span class="params">self, year, month</span>):</span><br><span class="line">        mdate = <span class="built_in">str</span>(year) + <span class="string">&#x27;-&#x27;</span> + <span class="built_in">str</span>(month)</span><br><span class="line">        time_array = time.strptime(mdate, <span class="string">&quot;%Y-%m&quot;</span>)</span><br><span class="line">        news_timestamp = time.mktime(time_array)</span><br><span class="line">        <span class="keyword">return</span> news_timestamp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">month_score_of_all</span>(<span class="params">self</span>):</span><br><span class="line">        db = pymysql.connect(host=self.host, user=self.user, password=self.passwd, database=self.db)</span><br><span class="line">        cursor = db.cursor()</span><br><span class="line">        year = <span class="number">2021</span></span><br><span class="line">        months = [<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">12</span>, <span class="number">11</span>, <span class="number">10</span>,</span><br><span class="line">                  <span class="number">9</span>, <span class="number">8</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">for</span> num, m <span class="keyword">in</span> <span class="built_in">enumerate</span>(months):</span><br><span class="line">            <span class="keyword">if</span> num != <span class="number">37</span>:</span><br><span class="line">                <span class="keyword">if</span> m == <span class="number">12</span>:</span><br><span class="line">                    year -= <span class="number">1</span></span><br><span class="line">                max_timestamp = self.date_to_timestamp(year, m)</span><br><span class="line">                <span class="keyword">if</span> months[num + <span class="number">1</span>] == <span class="number">12</span>:</span><br><span class="line">                    year02 = year - <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    year02 = year</span><br><span class="line">                month = <span class="built_in">str</span>(year02) + <span class="string">&#x27;-&#x27;</span> + <span class="built_in">str</span>(months[num + <span class="number">1</span>])</span><br><span class="line">                min_timestamp = self.date_to_timestamp(year02, months[num + <span class="number">1</span>])</span><br><span class="line">                sql06 = <span class="string">&quot;select round(sum(polarity*f_scores*log10(scans+comments)),10) from info_guba &quot;</span> \</span><br><span class="line">                        <span class="string">&quot;where usernames not like &#x27;%资讯%&#x27; &quot;</span> \</span><br><span class="line">                        <span class="string">&quot;and mdates&gt;&#123;0&#125; and mdates&lt;&#123;1&#125;&quot;</span>.<span class="built_in">format</span> \</span><br><span class="line">                    (min_timestamp, max_timestamp)</span><br><span class="line">                cursor.execute(sql06)</span><br><span class="line">                res = cursor.fetchall()</span><br><span class="line">                <span class="built_in">print</span>(month, res)</span><br><span class="line">                <span class="comment">#break</span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">东方财富股吧标题爬取分析</summary>
    
    
    
    <category term="python" scheme="https://s-luping.github.io/luping/categories/python/"/>
    
    
    <category term="python 爬虫" scheme="https://s-luping.github.io/luping/tags/python-%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>随机森林算法的Python实现</title>
    <link href="https://s-luping.github.io/luping/2021/03/02/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E7%9A%84Python%E5%AE%9E%E7%8E%B0/"/>
    <id>https://s-luping.github.io/luping/2021/03/02/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E7%AE%97%E6%B3%95%E7%9A%84Python%E5%AE%9E%E7%8E%B0/</id>
    <published>2021-03-01T20:43:38.000Z</published>
    <updated>2022-03-13T12:25:25.723Z</updated>
    
    <content type="html"><![CDATA[<p><strong>随机森林主要应用于回归和分类。<br>它几乎可以将任何数据填进去，下文使用鸢尾花数据进行分类和预测</strong><br><strong>环境</strong> python3.8<br><strong>数据集</strong> 鸢尾花数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dataset</span>(<span class="params">self</span>):</span><br><span class="line">    iris = load_iris()</span><br><span class="line">    feature = pd.DataFrame(data=iris.data, columns=iris.feature_names)</span><br><span class="line">    target = pd.DataFrame(data=<span class="built_in">map</span>(<span class="keyword">lambda</span> item: iris.target_names[item],</span><br><span class="line">                                   iris.target), columns=&#123;<span class="string">&#x27;target_names&#x27;</span>&#125;)</span><br><span class="line">    feature_train, feature_test, target_train, target_test = \</span><br><span class="line">        train_test_split(feature, target, test_size=<span class="number">0.3</span>)</span><br><span class="line">    <span class="keyword">return</span> feature_train, feature_test, target_train, target_test</span><br></pre></td></tr></table></figure><p>实验思路：<br>首先训练10个基分类器，每个基分类器为一个决策树；在预测时对每个基分类器投票结果进行统计倒排，选取票数最多的结果；其中‎每棵树的生长情况如下：‎<br>‎如果培训集中的案例数为 N，则随机取样 N 案例 - 但从原始数据中‎‎替换‎‎。此示例将是种植树的培训集。‎<br>‎如果有 M 输入变量，则指定一个数字 m&lt;&lt;M，以便在每个节点中随机从 M 中选择 m 变量，并且这些 m 上的最佳拆分用于拆分节点。在森林生长过程中，m 值保持不变。‎<br>‎每棵树都尽可能的生长。没有修剪。<br><a href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#workings">查看随机森林官网描述</a><br><strong>fit训练</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, feature=<span class="literal">None</span>, label=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    训练模型，请记得将模型保存至self.models</span></span><br><span class="line"><span class="string">    :param feature: 训练集数据，类型为ndarray</span></span><br><span class="line"><span class="string">    :param label: 训练集标签，类型为ndarray</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># ************* Begin ************#</span></span><br><span class="line">    n = <span class="built_in">len</span>(feature)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n_model):</span><br><span class="line">        <span class="comment"># 在训练集N随机选取n个样本  #frac=1 样本可重复取 （样本只包含特征数据）</span></span><br><span class="line">        randomSamples = feature.sample(n, replace=<span class="literal">True</span>, axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 在所有特征M随机选取m个特征 特征无重复 0.75表示选取4*0.75=3个特征</span></span><br><span class="line">        randomFeatures = randomSamples.sample(frac=<span class="number">0.75</span>, replace=<span class="literal">False</span>, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 标记该模型选取的特征</span></span><br><span class="line">        tags = self.connect(randomFeatures.columns.tolist())</span><br><span class="line">        <span class="comment"># 根据样本筛选出索引与之相同的lable即target_name</span></span><br><span class="line">        <span class="comment"># 使用loc标签索引获取</span></span><br><span class="line">        randomLable = label.loc[randomSamples.index.tolist(),:]</span><br><span class="line">        <span class="comment"># for i,j in zip(randomFeatures.index.tolist(),,randomLable.index.tolist()):</span></span><br><span class="line">        <span class="comment">#print(i,j)</span></span><br><span class="line">        model = DecisionTreeClassifier()</span><br><span class="line">        model = model.fit(randomFeatures, randomLable)</span><br><span class="line">        self.models.append(&#123;tags: model&#125;)</span><br><span class="line">    <span class="comment"># ************* End **************#</span></span><br></pre></td></tr></table></figure><p><strong>预测</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, features, target</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :param features: 测试集数据，类型为ndarray</span></span><br><span class="line"><span class="string">    :param target: 测试集实际lable，类型为ndarray</span></span><br><span class="line"><span class="string">    :return: 预测结果，类型为ndarray</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># ************* Begin ************#</span></span><br><span class="line">    result = []</span><br><span class="line">    vote = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> model <span class="keyword">in</span> self.models:</span><br><span class="line">        <span class="comment"># 获取模型使用的训练特征</span></span><br><span class="line">        modelFeatures = <span class="built_in">list</span>(model.keys())[<span class="number">0</span>].split(<span class="string">&#x27;000&#x27;</span>)[:-<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 提取出模型预测需要的标签</span></span><br><span class="line">        test_data = features[modelFeatures]</span><br><span class="line">        <span class="comment"># 基分类器进行预测</span></span><br><span class="line">        r = <span class="built_in">list</span>(model.values())[<span class="number">0</span>].predict(test_data)</span><br><span class="line">        vote.append(r)</span><br><span class="line">    <span class="comment"># 将数组转换为矩阵 10行45列</span></span><br><span class="line">    vote = np.array(vote)</span><br><span class="line">    <span class="comment"># print(vote.shape) # print(vote)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(features)):</span><br><span class="line">        <span class="comment"># 对每棵树的投票结果进行排序选取最大的</span></span><br><span class="line">        v = <span class="built_in">sorted</span>(Counter(vote[:, i]).items(),</span><br><span class="line">                   key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">       <span class="comment"># 查看投票情况和实际标签对比</span></span><br><span class="line">        <span class="built_in">print</span>(v, <span class="string">&quot;---&quot;</span>,<span class="built_in">list</span>(target)[i])</span><br><span class="line">        result.append(v[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line">    <span class="comment"># ************* End **************#</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">connect</span>(<span class="params">self, ls</span>):</span><br><span class="line">    s = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ls:</span><br><span class="line">        s += i + <span class="string">&#x27;000&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><p><strong>主函数</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    Bcf = BaggingClassifier()</span><br><span class="line">    featureAndTarget = Bcf.dataset()</span><br><span class="line">    Bcf.fit(featureAndTarget[<span class="number">0</span>],featureAndTarget[<span class="number">2</span>])</span><br><span class="line">    res = Bcf.predict(features=featureAndTarget[<span class="number">1</span>], target=featureAndTarget[<span class="number">3</span>][<span class="string">&#x27;target_names&#x27;</span>])</span><br><span class="line">    right = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> <span class="built_in">zip</span>(featureAndTarget[<span class="number">3</span>][<span class="string">&#x27;target_names&#x27;</span>], res):</span><br><span class="line">        <span class="keyword">if</span> i == j:</span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        <span class="comment">#print(i + &#x27;\t&#x27; + j)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;准确率为&#x27;</span> + <span class="built_in">str</span>(right / <span class="built_in">len</span>(res) * <span class="number">100</span>) + <span class="string">&quot;%&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://img-blog.csdnimg.cn/0d10e92317b34ce696252af9ca60df82.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5LiA5Yqg5YWt,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>可以看出准确率很高啦，可以调整特征数量m参数，准确率也会不同。</p>]]></content>
    
    
    <summary type="html">随机森林算法的Python实现</summary>
    
    
    
    <category term="python" scheme="https://s-luping.github.io/luping/categories/python/"/>
    
    
    <category term="python 随机森林" scheme="https://s-luping.github.io/luping/tags/python-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/"/>
    
  </entry>
  
</feed>
